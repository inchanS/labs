[{"categories":["Apps"],"content":"국립국어원 우리말샘 웹사이트의 검색어를 알프레드 호출창에서 자동완성으로 사용할 수 있는 워크플로우. 다운로드 링크 포함","date":"2024-10-17","objectID":"/posts/202410170921/","tags":["Apps","macOS","Alfred"],"title":"Alfred Workflow - 국립국어원 우리말샘 검색어 자동완성","uri":"/posts/202410170921/"},{"categories":["Apps"],"content":" 글 쓰다 간간이 우리말샘 웹사이트에서 신어나 용례를 찾아보는데, 요즘 빈도가 늘어나는 것 같아 기존 Alfred Workflow - 네이버 검색 워크플로우를 바탕으로 간단하게 만들었다. 우리말샘의 특징 개방성 신어, 방언, 전문 용어 등 다양한 분야의 한국어 자료 수록 일반 사용자가 편집에 참여 자유 저작권 정책에 기반을 둔 지식 기부로 수록 정보의 자유로운 활용 연계성 국내외 기관과의 정보 교류, 연계가 가능하도록 국제 표준 규격 준용 휴대용 기기, 인터넷 사이트 등에서 정보의 2차 활용성 강화 실용성 일반 대중이 필요로 하는 다양한 언어 정보 제공 사용자가 쉽게 사용할 수 있는 웹 기반 환경 적용 우리말샘 - 누리집 이용 안내 ","date":"2024-10-17","objectID":"/posts/202410170921/:0:0","tags":["Apps","macOS","Alfred"],"title":"Alfred Workflow - 국립국어원 우리말샘 검색어 자동완성","uri":"/posts/202410170921/"},{"categories":["Apps"],"content":"Description 국립국어원 우리말샘 웹사이트의 검색어를 알프레드 호출창에서 자동완성으로 사용할 수 있는 워크플로우 ","date":"2024-10-17","objectID":"/posts/202410170921/:0:1","tags":["Apps","macOS","Alfred"],"title":"Alfred Workflow - 국립국어원 우리말샘 검색어 자동완성","uri":"/posts/202410170921/"},{"categories":["Apps"],"content":"Install workflow 여기에서 최신 AlfKoreanSearch.alfredworkflow를 다운로드 받아서 실행한다. macOS 버전 12.3 이후 python3 설치 brew install python xcode-select --install 주의사항 Alfred 4.0 이상 지원 Python 2 사용 불가 ","date":"2024-10-17","objectID":"/posts/202410170921/:0:2","tags":["Apps","macOS","Alfred"],"title":"Alfred Workflow - 국립국어원 우리말샘 검색어 자동완성","uri":"/posts/202410170921/"},{"categories":["Apps"],"content":"Usage k ... : Naver Search(일반 네이버 검색)1 Cmd + Y : 검색결과를 미리 보기(웹브라우져 출력) 트리거가 되는 키워드k는 Alfred 설정-workflow-AlfKoreanSearch 에서 개인에 맞게 직접 수정할 수 있다. ↩︎ ","date":"2024-10-17","objectID":"/posts/202410170921/:0:3","tags":["Apps","macOS","Alfred"],"title":"Alfred Workflow - 국립국어원 우리말샘 검색어 자동완성","uri":"/posts/202410170921/"},{"categories":["Apps"],"content":"PopClip에서 DeepL과 Papago 번역 확장이 제대로 작동하지 않아, 비공식 확장을 만들어 배포. 다운로드 링크 포함.","date":"2024-10-12","objectID":"/posts/202410122044/","tags":["macOS","Apps"],"title":"PopClip 번역 확장 문제 해결 - DeepL \u0026 Papago 비공식 버전 배포","uri":"/posts/202410122044/"},{"categories":["Apps"],"content":" PopClip 앱이 맥 앱스토어에서 내려갔다. 이 사실을 늦게 알게된 바람에 약 1년간의 공백을 건너띄고 업데이트를 했더니 몇몇 Extension에서 문제들이 발생했다. 특히 가장 빈번하게 사용하던 번역 기능 확장들이 제대로 동작하지 않다보니 급한대로 만들었다. ","date":"2024-10-12","objectID":"/posts/202410122044/:0:0","tags":["macOS","Apps"],"title":"PopClip 번역 확장 문제 해결 - DeepL \u0026 Papago 비공식 버전 배포","uri":"/posts/202410122044/"},{"categories":["Apps"],"content":"PopClip Extension - deepL translation PopClip Extensions Directory의 리스트에 있는 DeepL Translate Extension이 제대로 작동하지 않는다. (2024-10-10 기준) 해당 확장은 deepL의 App, Website 둘 중 하나를 선택하여 사용할 수 있도록 JS,TS로 동작하게 되어있는데 일단 Website 선택시 해당 페이지로 이동만 할 뿐, 선택된 텍스트가 전달되지 못하는 문제가 있다. App으로 선택해보니 아예 앱이 열리지도 않았다. 현재 Extension이 제대로 동작하지 않는 현재 조건은 다음과 같다. macOS 15.0.1 PopClip ver 2024.5.2 ","date":"2024-10-12","objectID":"/posts/202410122044/:1:0","tags":["macOS","Apps"],"title":"PopClip 번역 확장 문제 해결 - DeepL \u0026 Papago 비공식 버전 배포","uri":"/posts/202410122044/"},{"categories":["Apps"],"content":"문제 Website 사용의 경우 전달 URL의 해시값과 관련된 API가 변경된듯 보인다. APP 사용의 경우 앱 호출이 안된다. ","date":"2024-10-12","objectID":"/posts/202410122044/:1:1","tags":["macOS","Apps"],"title":"PopClip 번역 확장 문제 해결 - DeepL \u0026 Papago 비공식 버전 배포","uri":"/posts/202410122044/"},{"categories":["Apps"],"content":"PopClip Official Extension과의 차이점 우선 website로만 연결 가능 웹사이트를 통한 기능은 그대로 작동할 수 있도록 url 해시 수정 APP 또는 Website를 고를 수 있는 불필요한 Option 제거(target Language 선택창은 그대로) 불필요한 APP 작동 코드 제거 대상언어의 기본값을 한국어로 변경 아래 링크에서 다운로드 받을 수 있다. ⬇️ DeepL 비공식 확장 다운로드 ","date":"2024-10-12","objectID":"/posts/202410122044/:1:2","tags":["macOS","Apps"],"title":"PopClip 번역 확장 문제 해결 - DeepL \u0026 Papago 비공식 버전 배포","uri":"/posts/202410122044/"},{"categories":["Apps"],"content":"PopClip Extension - Papago translation PopClip Extensions Directory에서 Papago 번역 기능을 하던 extension이 리스트에서 이 사라졌다.(2024-10-10 기준) 때문에 이전의 기능 그대로 작동할 수 있도록 다시 대충(?) 만들었다. 아래 링크에서 다운로드 받을 수 있다. ⬇️ Papago 비공식 확장 다운로드 Reference documentation PopClip Extensions Developer Reference ","date":"2024-10-12","objectID":"/posts/202410122044/:2:0","tags":["macOS","Apps"],"title":"PopClip 번역 확장 문제 해결 - DeepL \u0026 Papago 비공식 버전 배포","uri":"/posts/202410122044/"},{"categories":["Apps"],"content":"2024년 3월부로 더이상 맥 앱스토어에서는 PopClip을 구매할 수 없기에 기존 MacAppStore(MAS) 버전으로부터 StandAlone License 버전으로의 마이그레이션 방법을 소개한다.","date":"2024-10-10","objectID":"/posts/202410101108/","tags":["macOS","Apps"],"title":"PopClip 앱이 맥 앱스토어에서 내려갔다.","uri":"/posts/202410101108/"},{"categories":["Apps"],"content":" macOS에서 요긴하게 사용중인 앱 중 하나가 PopClip for Mac이다. 그런데 2024년 3월 20일자로 PopClip 앱이 MacAppStore(이하 MAS)에서 내려갔다는 글을 보게 되었다. 더이상 MAS에서는 구매할 수 없으며 업데이트 또한 받을 수 없다. 하지만 이미 구입한 경우, MAS의 구입목록에서는 여전히 다운로드 받을 수는 있다. 문제는 MAS에서의 마지막 버전은 2023.9라는 점이다. ","date":"2024-10-10","objectID":"/posts/202410101108/:0:0","tags":["macOS","Apps"],"title":"PopClip 앱이 맥 앱스토어에서 내려갔다.","uri":"/posts/202410101108/"},{"categories":["Apps"],"content":"PopClip 버전 정보 Mac App Store에서 더 이상 업데이트는 제공되지 않으며, 기존 구매자는 여전히 구입 목록에서 2023.9 버전을 다운로드할 수 있다. 현재 시점에서 최신 버전은 2024.5.2이다​.1 ","date":"2024-10-10","objectID":"/posts/202410101108/:1:0","tags":["macOS","Apps"],"title":"PopClip 앱이 맥 앱스토어에서 내려갔다.","uri":"/posts/202410101108/"},{"categories":["Apps"],"content":"기존 MAS 앱을 StandAlone Edition으로 마이그레이션하는 방법 PopClip을 완전히 종료한다. 새로운 PopClip의 최신버전을 download page에서 다운로드 하고 압축을 푼다. /Applications 폴더에 있는 MAS 버전의 PopClip 앱을 새로 다운로드 받은 앱으로 대치한다.2 다시 PopClip을 실행한다. 메뉴바에서 PopClip을 클릭하고 정보보기(‘i’ 모양의 아이콘)를 한 후, StandAlone License로 바뀌어있는지 확인한다. 위 순서를 잘 따랐다면 마지막에 다음과 같은 이미지를 확인 할 수 있다. Migrate from the Mac App Store (MAS) edition to the Standalone edition — PopClip ","date":"2024-10-10","objectID":"/posts/202410101108/:2:0","tags":["macOS","Apps"],"title":"PopClip 앱이 맥 앱스토어에서 내려갔다.","uri":"/posts/202410101108/"},{"categories":["Apps"],"content":"새로운 Mac에서의 설치 Question 그럼 기존 MAS 버전의 앱 구매자는 새로운 맥에서 설치할때마다 MAS의 구입목록에서 설치한 후, 다시 이 마이그레이션 과정을 반복해야하는건가? 앞으로는 iCloud를 통해 기존 구매 증명을 감지하여 Standalone 버전의 라이선스를 자동으로 활성화하는 방식이 도입되었다. 이는 MAS 버전을 설치한 후 마이그레이션해야 하는 번거로움을 줄이기 위해 추가된 기능이라고 한다. Installing in future on a new Mac You may be thinking that you have to mess about installing the MAS edition first, then the Standalone edition, whenever you install PopClip on a new Mac. Not so! In future, the Standalone edition will detect a proof your MAS purchase in iCloud and unlock itself. This assumes you have iCloud enabled on your Mac now, and you use the same iCloud account on the new Mac. ","date":"2024-10-10","objectID":"/posts/202410101108/:3:0","tags":["macOS","Apps"],"title":"PopClip 앱이 맥 앱스토어에서 내려갔다.","uri":"/posts/202410101108/"},{"categories":["Apps"],"content":"Standalone License 할인 기존 MAS 구매자에게 Standalone License를 50% 할인해주는 정책이 있다. 구매 증명을 이메일로 보내면 할인된 가격으로 라이선스를 구매할 수 있다​. Can I get a license key for the Standalone edition? License keys can be purchased on the buy page. I am currently offering MAS customers a 50% discount. Please email proof of your MAS purchase to my support email. PopClip Version History ↩︎ macOS 언어가 한국어라면 새로받은 앱을 응용프로그램 폴더에 끌어넣으면 된다. 이때 알림창이 뜨는데 대치를 선택하면 된다. ↩︎ ","date":"2024-10-10","objectID":"/posts/202410101108/:4:0","tags":["macOS","Apps"],"title":"PopClip 앱이 맥 앱스토어에서 내려갔다.","uri":"/posts/202410101108/"},{"categories":["Env"],"content":"재부팅 후 SSH 키가 자동으로 등록되지 않거나 암호를 계속해서 묻는 문제? macOS에서 SSH 키 관리와 관련된 AddKeysToAgent 및 UseKeychain 설정 문제를 해결하는 방법을 알아본다.","date":"2024-10-04","objectID":"/posts/202410042356/","tags":["macOS"],"title":"macOS에서 SSH 키 자동 등록 및 암호 관리 문제 해결 방법","uri":"/posts/202410042356/"},{"categories":["Env"],"content":" Failure AddKeysToAgent yes 설정을 했지만 재부팅 후 SSH 키가 자동으로 ssh agent에 등록되지 않는다? ssh key의 암호(passphrase) 설정후 UseKeychain yes 설정을 했는데 왜 자꾸 암호를 물을까? ","date":"2024-10-04","objectID":"/posts/202410042356/:0:0","tags":["macOS"],"title":"macOS에서 SSH 키 자동 등록 및 암호 관리 문제 해결 방법","uri":"/posts/202410042356/"},{"categories":["Env"],"content":"재부팅 후 SSH 키가 자동으로 ssh agent에 등록되지 않는다? macOS에서 SSH 클라이언트는 파일 이름이 다음과 같은 경우 기본 키파일명으로서 특별한 처리를 한다. id_rsa id_dsa id_ecdsa id_ed25519 등등… 이러한 기본 키 파일명을 사용할 때 AddKeysToAgent yes와 UseKeychain yes 옵션이 설정되어 있으면, SSH 클라이언트는 자동으로 해당 키를 SSH 에이전트에 추가하고 macOS 키체인에서 암호를 가져온다. 보면 알겠지만 기본 키 파일명은 바로 키 생성시 설정한 암호화 알고리즘 옵션명, 즉 id_알고리즘으로 이루어져있다. 따라서 AddKeysToAgent yes 옵션을 사용하고자 한다면 ssh key 파일명은 위에서 언급한대로 설정하여야 한다. 그럼에도 불구하고 파일명을 달리하고 싶다면 2가지 방법이 있다. 각자 다른 암호화 알고리즘으로 키를 생성하여 사용한다. 재부팅시 자동으로 파일을 등록할 수 있도록 shell 초기화 파일에서 SSH 키를 자동으로 로드하도록 한다. ","date":"2024-10-04","objectID":"/posts/202410042356/:1:0","tags":["macOS"],"title":"macOS에서 SSH 키 자동 등록 및 암호 관리 문제 해결 방법","uri":"/posts/202410042356/"},{"categories":["Env"],"content":"키체인에서 암호를 가져오지 못하고 매번 묻는다? 키체인에서 암호를 가져오지 못하는 이유는 크게 2가지 상황이 있을 수 있다. SSH key가 (자동으로) 등록되지 않았을 경우 UseKeychain yes 옵션이 제대로 작동하지 않는 경우 1번의 경우에는 다시 재부팅 후 SSH 키가 자동으로 ssh agent에 등록되지 않는다?를 참고한다. 2번의 경우에는 다음의 상황이 있을 수 있다. 해당 레포지토리가 있는 경로에서 git fetch, git push와 같은 명령을 했을 때, 암호를 묻는다. 암호를 묻는다는건 ssh key는 제대로 가져오고 있다는 뜻이다. 그런데 분명 UseKeychain yes 설정을 했는데 왜 계속해서 묻는걸까? ssh-add -l 위 명령으로 현재 ssh agent에 등록된 키를 살펴본다. 이 때 키가 보이지 않을 것이다. 이제 아래의 명령으로 ssh 접속 확인을 해본다. ssh -T git@github.com 이때 정상적으로 진행이 되어 확인이 된다면 이어서 git fetch 명령을 실행해보자. 아마 암호를 묻지 않고 정상적으로 추가 암호입력 작업 없이 명령이 작동할 것이다. 자 이제 다시 ssh agent에 등록된 키를 살펴본다. 아까는 보이지 않았던 키가 지금은 보일 것이다. 이 이유는 ssh 클라이언트와 에이전트의 작동방식 때문이다. 비대화형 환경에서의 키체인 연동 문제: git fetch와 같은 비대화형 명령에서는 UseKeychain yes 옵션이 기대한 대로 동작하지 않을 수 있다. AddKeysToAgent yes 옵션은 SSH 클라이언트가 키를 사용할 때만 동작하며, 자동으로 키를 로드하지 않는다. 키체인은 SSH 클라이언트가 아닌 ssh agent를 통해 암호(passphrase)를 가져온다. ssh -T git@github.com와 같은 명령은 대화형(interactive) 세션에서 동작하며 대화형 세션이 진행될 때, 에이전트에서는 config파일 설정에 따라 에이전트에 다시 키를 로드한다. 때문에 재부팅 후, 바로 ssh 비대화형 명령을 질행하면 SSH 클라이언트가 키체인에서 암호를 가져오지 못하기에 암호를 매번 묻게된다. 따라서 만약 한번이라도 대화형 세션을 진행했다면 키는 정상적으로 ssh agent에 로드되고, 이후 암호를 되묻는 일이 없다. ","date":"2024-10-04","objectID":"/posts/202410042356/:2:0","tags":["macOS"],"title":"macOS에서 SSH 키 자동 등록 및 암호 관리 문제 해결 방법","uri":"/posts/202410042356/"},{"categories":["Env"],"content":"해결방법 해결 방법은 크게 2가지를 소개한다. macOS의 launchd를 이용하여 재부팅 및 mac 사용자 로그인시 SSH 키 자동 로드하는 방법 터미널 최초 실행시 shell 초기화 파일에서 SSH 키 자동 로드 위 2가지 방법 중 하나만 사용하면 되며, 다음 설명 이후 각각의 상황에 맞게 판단하여 사용할 수 있다. ","date":"2024-10-04","objectID":"/posts/202410042356/:3:0","tags":["macOS"],"title":"macOS에서 SSH 키 자동 등록 및 암호 관리 문제 해결 방법","uri":"/posts/202410042356/"},{"categories":["Env"],"content":"1. macOS의 launchd를 이용하여 SSH 키 자동 로드 macOS의 launchd는 재부팅 및 mac 사용자 로그인시 최초로 한번 작동한다. 즉 재부팅을 하거나 또는 맥 시스템 설정 중 사용자 로그아웃 이후 로그인시 작동한다는 뜻인데, 바로 이 때 SSH 키를 자동로드 할 수 있도록 plist 파일을 하나 추가해주는 방법이다. 1. plist 파일 생성 ~/Library/LaunchAgents/ 디렉토리에 com.username.ssh-add.plist 파일을 생성한다. 주의 사항은 위 파일명에서 username 부분을 실제 사용자 이름으로 변경 실제 사용자 이름은 finder에서 홈 디렉토리를 나타낸다. 터미널에서 다음 명령어로 확인할 수 있다. whoami 이후 나타나는 이름을 그대로 username 부분으로 대치한다. 해당 파일의 내용은 다음을 복사하여 넣는다. \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003c!DOCTYPE plist PUBLIC \"-//Apple Computer//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"\u003e \u003cplist version=\"1.0\"\u003e \u003cdict\u003e \u003ckey\u003eLabel\u003c/key\u003e \u003cstring\u003ecom.username.ssh-add\u003c/string\u003e \u003ckey\u003eProgramArguments\u003c/key\u003e \u003carray\u003e \u003cstring\u003e/usr/bin/ssh-add\u003c/string\u003e \u003cstring\u003e--apple-use-keychain\u003c/string\u003e \u003cstring\u003e/Users/username/.ssh/id_ed25519\u003c/string\u003e \u003c/array\u003e \u003ckey\u003eRunAtLoad\u003c/key\u003e \u003ctrue/\u003e \u003ckey\u003eStandardErrorPath\u003c/key\u003e \u003cstring\u003e/tmp/ssh-add.err\u003c/string\u003e \u003ckey\u003eStandardOutPath\u003c/key\u003e \u003cstring\u003e/tmp/ssh-add.out\u003c/string\u003e \u003c/dict\u003e \u003c/plist\u003e 이 때, 위 코드에서 하이라이트 처리하여 보이는 7번, 12번 line의 ‘username’ 부분을 아까 찾은 홈 디렉토리명으로 동일하게 대치한다. 2. launchd 에이전트에 생성한 파일 로드 터미널에서 다음 명령을 실행하여 1. plist 파일 생성에서 생성한 파일을 로드시켜준다. (역시 username 부분은 홈 디렉토리 명으로 대치해준다.) launchctl load ~/Library/LaunchAgents/com.username.ssh-add.plist 끝, 이제 재부팅하고 테스트해본다. ","date":"2024-10-04","objectID":"/posts/202410042356/:3:1","tags":["macOS"],"title":"macOS에서 SSH 키 자동 등록 및 암호 관리 문제 해결 방법","uri":"/posts/202410042356/"},{"categories":["Env"],"content":"2. shell 초기화 파일에서 SSH 키 자동 로드 이 방법은 최초 터미널 실행시 로드되는 쉘 초기화 파일에서 SSH 키를 자동으로 로드시키는 방법이다. ~/.bash_profile 또는 ~/.zshrc 파일에 다음을 추가한다. ssh-add --apple-use-keychain ~/.ssh/id_ed25519 \u0026\u003e/dev/null 재부팅 후, 터미널 실행시 해당 명령이 실행되면서 이제 비대화형 세션(git fetch와 같은)을 처음 진행하더라도 암호를 되묻지 않는다. 이 방법은 SSH key 생성시 기본 파일명이 아니더라도 AddKeysToAgent yes 옵션이 마치 항상 실행하듯 해준다는 장점이 있다. Warning 이 방법은 터미널을 열 때만 실행되므로, GUI 환경이나 터미널을 열지 않고 git 명령을 실행할 때는 키가 로드되지 않는다. ","date":"2024-10-04","objectID":"/posts/202410042356/:3:2","tags":["macOS"],"title":"macOS에서 SSH 키 자동 등록 및 암호 관리 문제 해결 방법","uri":"/posts/202410042356/"},{"categories":["Env"],"content":"터미널에서 각 레포지토리별로 github 다계정 사용시 SSH키 생성 및 등록, 그리고 재부팅 이후에도 에이전트에 자동 등록하는 방법, SSH키 암호(passphrase) 사용시 macOS의 키체인을 이용해 암호입력 패스하는 방법 기술","date":"2024-10-04","objectID":"/posts/202410041532/","tags":["macOS"],"title":"GitHub 계정 다중 사용자를 위한 macOS SSH 설정 방법","uri":"/posts/202410041532/"},{"categories":["Env"],"content":" github를 이용하다보면 개인계정, 회사계정 등등 여러 계정을 사용하는 상황이 오곤 한다. 이 글은 macOS의 터미널에서 각 레포지토리별 다른 github 계정을 사용할 때, SSH를 이용하는 방법을 설명한다. ","date":"2024-10-04","objectID":"/posts/202410041532/:0:0","tags":["macOS"],"title":"GitHub 계정 다중 사용자를 위한 macOS SSH 설정 방법","uri":"/posts/202410041532/"},{"categories":["Env"],"content":"새 SSH 키 생성 및 ssh-agent에 추가하기 ","date":"2024-10-04","objectID":"/posts/202410041532/:1:0","tags":["macOS"],"title":"GitHub 계정 다중 사용자를 위한 macOS SSH 설정 방법","uri":"/posts/202410041532/"},{"categories":["Env"],"content":"1. SSH 키 생성 1. 키 생성 우선 터미널을 열고 다음을 입력한다. ssh-keygen -t ed25519 -C \"your_email@example.com\" -t : 이 옵션은 생성할 키의 유형을 지정하는데 사용한다. 일반적으로 어떤 암호화 알고리즘을 사용할지에 따른다. -C : 이 옵션은 키에 추가할 주석을 지정하는데 사용된다. 여러 키를 가지고 있을 경우, 키를 식별할 때 유용하며, 보통 이메일 주소나 설명을 넣는다. 만약 ed25519 방식을 지원하지 않는 레거시 시스템을 사용하는 경우 다음과 같이 rsa 방식을 사용할 수 있다. ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\" -b : 이 옵션은 키의 비트 길이(bit length)를 지정하는데 사용된다. RSA 키의 경우, 일반적으로 2048비트가 기본값이지만, 보안을 강화하기 위해 따로이 -b 4096명령을 추가하여 더 긴 키를 사용할 수 있다. 비트가 길어지면 키 생성 및 사용시 약간 더 많은 연산 자원을 필요로 한다. ssh-keygen -t 옵션 종류 rsa: 오래된 방식이지만 여전히 많이 사용됨. ssh-keygen -t rsa로 생성. ecdsa: ECC(Elliptic Curve Cryptography)를 기반으로 하여, 작은 키 크기로도 높은 보안을 제공함. ed25519: 가장 최신의 알고리즘 중 하나로, 빠르고 높은 보안을 제공함. 현대적인 서버에서 많이 권장됨. 2. 파일 저장위치 및 파일명 선택 1. 키 생성같이 실행을 했다면 다음과 같은 문구가 뜬다. \u003e Generating public/private ALGORITHM key pair. \u003e Enter a file in which to save the key (/Users/YOU/.ssh/id_ALGORITHM): [Press enter] 두번째 문구는 키를 저장할 위치를 묻고 있는데 아무 입력 없이 enter를 누르면 위 문구에 나온대로 .ssh 위치와 id_ALGORITHM이라는 파일명으로 저장된다. 만약 위치와 파일명을 바꾸고 싶다면 이 물음에서 같은 양식으로 작성한 후, enter를 누르면 된다. ex) /Users/YOU/Documents/ssh/id_IamSSH 3. SSH 키 보안암호(passphrase) 입력 2. 파일 저장위치 및 파일명 선택 이후에는 다음과 같은 문구가 나온다. \u003e Enter passphrase (empty for no passphrase): [Type a passphrase] \u003e Enter same passphrase again: [Type passphrase again] 생성한 SSH 키에 대한 암호를 설정하는 화면으로 암호 생성을 무시하고 싶다면 아무것도 입력하지 않은 상태에서 그대로 enter를 누르면 된다. SSH키의 암호 사용 권장 SSH 키의 생성: • SSH 키를 생성하면 **private key(개인 키)**와 public key(공개 키) 한 쌍이 만들어진다. • Public key는 서버에 저장되고, private key는 로컬에 보관된다. • 서버는 접속 시 public key를 사용해 보낸 메시지가 private key와 일치하는지 확인하고, 이 방식으로 인증이 이루어진다. 파일 탈취: • Private key는 매우 중요한 파일이다. 탈취되면 해당 키를 사용하는 모든 서버에 대한 접근 권한이 공격자에게 넘어갈 수 있다. • Public key는 공개되어도 문제가 없지만, private key가 탈취되면 누구나 이 키를 사용하여 서버에 접속할 수 있게 된다. 이로써 SSH key를 생성하는 부분은 끝이 났다. ","date":"2024-10-04","objectID":"/posts/202410041532/:1:1","tags":["macOS"],"title":"GitHub 계정 다중 사용자를 위한 macOS SSH 설정 방법","uri":"/posts/202410041532/"},{"categories":["Env"],"content":"2. ssh-agent에 SSH 키 추가 $ eval \"$(ssh-agent -s)\" \u003e Agent pid 59566 $ eval \"$(ssh-agent -s)\" 입력시 아래 나오는 Agent pid는 디바이스마다 또는 실행될때마다 달라질 수 있기에 신경쓰지 않아도 된다. macOS의 경우 사실 위와 같이 따로이 ssh-agent를 따로이 구동시켜줄 필요가 없다. ssh key를 등록하는 과정에서 ssh-agent가 백그라운드에서 항상 자동으로 실행되고 있기 때문이다. 다만 재부팅시 agent에 등록된 key들은 휘발되기에 다시 등록해줘야 하는 과정이 필요하다. 1. ssh/config 파일 편집 먼저 ~/.ssh/config 파일이 기본 위치에 있는지 확인한다. $ open ~/.ssh/config 만약 다음과 같이 파일이 없다고 나온다면, \u003eThe file /Users/YOU/.ssh/config does not exist. 아래 명령어로 파일을 생성한다. touch ~/.ssh/config 그리고 해당 파일을 사용하는 코드 편집기로 연다. 다음 예시는 vim을 이용해 해당 파일을 열고 있다. vim ~/.ssh/config 해당 파일에 다음과 같이 입력한다. Host github.com-personal HostName github.com AddKeysToAgent yes UseKeychain yes User ABCD IdentitiesOnly yes IdentityFile ~/.ssh/id_ed25519 Host github.com-work HostName github.com AddKeysToAgent yes UseKeychain yes User ABCD-work IdentitiesOnly yes IdentityFile ~/.ssh/id_ed25519_work 옵션 설명 Host : SSH 접속시 사용할 호스트 설정상 주소이다. HostName : 실제로 접속할 원격 호스트의 주소를 따로이 지정할 때 사용한다. Host에서 계정별로 또는 사용처별로 나눈 가상의 별칭을 추가했다면 (github.com-personal, github.com-work 처럼) 여기에 따로이 정확한 HostName으로 github.com이라고 구분해놓을 수 있다. Host와 실제 원격호스트가 같을 경우 HostName은 빼도 무방하다. Note 만약 다계정이 아닌 단일계정에 대한 SSH 키 설정이라면 다음과 같이 Host 별칭을 주지않고 HostName 옵션을 제거한 후 진행하면 된다. Host github.com AddKeysToAgent yes UseKeychain yes User ABCD IdentityFile ~/.ssh/id_ed25519 AddKeysToAgent yes : SSH 에이전트에 SSH 키를 추가할지 여부를 지정한다. SSH 에이전트는 재부팅시 추가된 모든 키들이 휘발되는 특성이 있다. 때문에 이 설정을 해두면 재부팅시 에이전트가 실행됨과 동시에 키를 자동으로 추가한다. UseKeychain yes : SSH 키에 대한 암호(passphrase)가 있는 경우 이 암호를 매번 입력하지 않아도 되도록 macOS의 키체인에 넣어놓는다. 에이전트가 실행되고 해당 키가 에이전트에 등록되어있을 경우, SSH 클라이언트는 해당 키에 대한 암호를 사용자에게 묻지않고 키체인에서 가져다 쓴다. SSH키에 대한 암호가 없다면 이 옵션은 생략한다. User ABCD : SSH를 통해 원격서버에 접속할 때 사용할 사용자 이름을 지정한다. github의 경우 계정명을 입력하면 된다. (ex. github 계정이 만약 inchan이라면 User inchan 이라고 입력하면 된다.) 다계정일 경우 이 User 명에 따라 정해진 SSH 키를 사용함으로 정확히 구분하여 입력한다. identitiesOnly yes : SSH가 IdentityFile로 지정된 키 파일만을 사용하도록 강제하는 옵션이다. 기본적으로 SSH 는 여러 키를 자동으로 시도할 수 있는데, 이 옵션을 사용하면 명시적으로 지정된 키만을 사용한다. 다계정에서 각각 다른 SSH키 사용시 이 옵션 설정을 추천한다. IdentityFile ~/.ssh/id_ed25519 : 사용할 개인 키 파일을 지정한다. 기본적으로 경로와 파일명이다. Question 지금 예시에서는 왜 id_ABCD와 같이 User 명이 아니라 어렵고 복잡하게 이런 이상한 이름을 썼을까?? 추후 설명할 AddKeysToAgent yes 옵션 때문인데 이 글에서 더 자세히 알아볼 수 있다. 2. ssh-agent에 key 추가하기 키를 만들었으니 이제 내가 이 키를 사용하겠다는 뜻으로 등록을 해야한다. ssh-add --apple-use-keychain ~/.ssh/id_ed25519 위와 같은 경로 및 파일명으로 키를 등록한다. –apple-use-keychain은 해당 키에 대한 암호(passphrase)를 앞서 언급한 키체인에 등록하여 사용하겠다는 뜻이다.1 정상적으로 등록되었는지 확인하는 방법은 다음과 같다. ssh-add -l ","date":"2024-10-04","objectID":"/posts/202410041532/:1:2","tags":["macOS"],"title":"GitHub 계정 다중 사용자를 위한 macOS SSH 설정 방법","uri":"/posts/202410041532/"},{"categories":["Env"],"content":"3. github에 SSH key 등록하기 github에는 아까 만든 한 쌍의 SSH 키 중 public 키를 등록한다. 다음 명령어는 터미널에서 public 키의 내용을 복사한다. $ pbcopy \u003c ~/.ssh/id_ed25519.pub 만약 키 복사가 제대로 되지 않는 경우 경로를 재확인해본다. 위 명령어는 기본설정대로 키 파일을 저장했을 경우인 ~/.ssh 폴더내로 경로가 잡혀있다. 등록절차는 매우 간단하다. github website에서 해당 계정으로 로그인 한 후, settings - SSH and GPG keys - SSH keys 에서 우상단에 보이는 New SSH key를 누른다. New SSH key 버튼을 누르면 다음 화면을 볼 수 있다. Title : 나중에 github에 여러 SSH키가 입력되어있을 경우 지금 등록하는 SSH 키를 구분할 수 있도록 적당한 제목을 입력한다. (ex. home-my-macbook) Key type : Authentication Key Key : 방금 전 터미널에서 복사해둔 퍼블릭 키의 내용을 붙여넣는다. 그리고 마지막으로 Add SSH key를 눌러 등록하면 끝! ","date":"2024-10-04","objectID":"/posts/202410041532/:1:3","tags":["macOS"],"title":"GitHub 계정 다중 사용자를 위한 macOS SSH 설정 방법","uri":"/posts/202410041532/"},{"categories":["Env"],"content":"4. SSH - Host 테스트하기 이제 잘 작동하는지 테스트해본다. ssh -T git@github.com-personal ssh/config 파일에서 설정한 옵션 Host 부분의 별칭을 주소로 한다. 만약 Host 별칭을 github.com-work로 했다면 ssh -T git@github.com-work라고 입력하면 Host에 따라 테스트할 수 있다. 정상적인 경우, “Hi 사용자명 ~~ 어쩌구 저쩌구 “와 같은 문구를 볼 수 있다. Warning SSH key를 이용해 깃 레포지토리를 관리하게 되었다면, 이제부턴 해당 계정을 이용하는 git clone 및 기존 레포에 원격 주소 할당시 github/user/repository의 주소복사시 https 주소가 아닌 ssh 주소를 이용해야 한다!! ","date":"2024-10-04","objectID":"/posts/202410041532/:1:4","tags":["macOS"],"title":"GitHub 계정 다중 사용자를 위한 macOS SSH 설정 방법","uri":"/posts/202410041532/"},{"categories":["Env"],"content":"5. git config 수정하기 끝으로 지금부터 기술하는 부분은 진행하지 않더라도, 각기 다른 SSH key를 이용하여 각각의 github 계정으로 접속은 가능하다. 다만, git commit 시 commit 정보로써 기본 git 정보가 서로 다른 계정간에 뒤섞일 수 있기에 git config에서 정보를 명확히 재정리해준다. (예를 들어 실제 personal 계정으로 제대로된 commit이 들어가지만 문제는 그때의 commit 사용자명과 이메일명이 다른 work 계정의 정보로 새겨질 수 있다는 뜻) github 사용자명이 abcd이고 email이 abcd@email.com 인 경우, 해당 레포지토리 경로에서 다음을 입력해준다. git config user.email abcd@email.com git config user.name abcd 제대로 입력되었는지 확인하는 명령어는 다음과 같이 입력값만 빼고 다시 명령하면 된다. git config user.email git config user.name 마찬가지로 github 사용자명이 efgh이고 email이 efgh@email.com 인 경우에는 다음과 같이 입력한다. git config user.email efgh@email.com git config user.name efgh 이렇게 git config 설정까지 마치면 끝이다. 참고문서 새 SSH 키 생성 및 ssh-agent에 추가 - GitHub Docs GitHub 계정에 새 SSH 키 추가 - GitHub Docs macOS 시에라 이전의 OS에서는 --apple-use-keychain 대신 -K 라는 옵션 구문을 사용한다. (예시, ssh-add -K ~/.ssh/id_ed25519) ↩︎ ","date":"2024-10-04","objectID":"/posts/202410041532/:1:5","tags":["macOS"],"title":"GitHub 계정 다중 사용자를 위한 macOS SSH 설정 방법","uri":"/posts/202410041532/"},{"categories":["Apps"],"content":"알프레드 워크플로우 중, 네이버 검색 워크플로우의 지도 검색 기능 문제에 따른 해결 및 개선된 워크플로우 파일 제공","date":"2024-03-17","objectID":"/posts/202403171602/","tags":["macOS","Apps","Alfred"],"title":"Alfred workflow - 네이버 검색 워크플로우 지도 검색 개선","uri":"/posts/202403171602/"},{"categories":["Apps"],"content":" ","date":"2024-03-17","objectID":"/posts/202403171602/:0:0","tags":["macOS","Apps","Alfred"],"title":"Alfred workflow - 네이버 검색 워크플로우 지도 검색 개선","uri":"/posts/202403171602/"},{"categories":["Apps"],"content":"Workflow 소개 macOS에서 가장 많이 사용하는 앱 중 하나가 Alfred이다. 그리고 이 앱에서도 가장 높은 빈도로 사용하는 워크플로우 중 하나가 바로 네이버 검색이다. 이 워크플로우는 Kuniz/alfnaversearch: Naver Search Workflow for Alfred (알프레드 네이버 검색/사전/지도 자동완성 워크플로우) 여기에서 살펴보고 다운로드할 수 있는데 정말 잘 만든 워크플로우다. 대략적인 기능은 다음과 같다. 네이버 검색 : 네이버 웹페이지에서의 기본 검색창에 검색어 입력시 자동완성 목록이 아래로 나타나는데 그 데이터를 Alfred 호출창에서 그대로, 그리고 매우 빠른 속도로 보여준다. 네이버 사전 검색 : 네이버 사전에 있는 거의 모든 언어에 대해 위와 같은 방식으로 보여준다. 네이버 쇼핑 검색 네이버 지도 검색 그런데 얼마전부터 갑자기 ‘네이버 지도’ 검색이 정상적으로 작동하지 않았다. Safari 브라우저의 개발자 도구 중 웹 인스펙터를 열어 살펴봤다. 위와 같은 화면을 볼 수 있는데 검색창에 검색값을 입력할 때마다 화면상 1번의 Instant-search가 추가됨을 볼 수 있다. 2번이 아마도 Naver map 검색에서의 자동완성 API 주소로 보인다. 그리고 자동완성시 입력어에 해당하는 3번의 query와 4번의 coords라는게 추가적으로 보이는데 이건 위치정보를 담고 있는 위도와 경도 정보이다. ","date":"2024-03-17","objectID":"/posts/202403171602/:1:0","tags":["macOS","Apps","Alfred"],"title":"Alfred workflow - 네이버 검색 워크플로우 지도 검색 개선","uri":"/posts/202403171602/"},{"categories":["Apps"],"content":"변경내역 파악 우선 위 API로부터 json 파일을 받아 데이터를 살펴보고 기존과 바뀐 부분을 찾아봤더니, API 주소가 바뀌었다. (map.naver.com/p/api/search -\u003e map.naver.com/p/api/search/instant-search) 위 API로부터 받은 자료의 배열 역시 기존의 코드와 비교해봤을 때 달라진 것으로 보인다. 화면에서 보이는 4번의 coords, 즉 사용자의 현재 위치(위도, 경도)가 필수 제공값이 된듯 하다. ","date":"2024-03-17","objectID":"/posts/202403171602/:2:0","tags":["macOS","Apps","Alfred"],"title":"Alfred workflow - 네이버 검색 워크플로우 지도 검색 개선","uri":"/posts/202403171602/"},{"categories":["Apps"],"content":"문제 수정 ","date":"2024-03-17","objectID":"/posts/202403171602/:3:0","tags":["macOS","Apps","Alfred"],"title":"Alfred workflow - 네이버 검색 워크플로우 지도 검색 개선","uri":"/posts/202403171602/"},{"categories":["Apps"],"content":"오류 해결 이에 따라 해당 워크플로우의 git을 fork해서 수정하기 시작했다. 관련 기능의 스크립트는 파이썬으로 작성되어 있었고, 파이썬은 처음만져보는 언어였지만 조건문이나 반복문의 기본 개념이나 틀은 JS와 크게 다르지 않아 별다른 어려움은 없었다. 우선 변경내역을 차례로 고쳐나가며 코드를 수정하였다. def get_data(word): url = 'https://map.naver.com/v5/api/search/instant-search' params = dict(query=word, type=\"all\", coords= '37.1234,126.1234', lang=\"ko\", caller=\"pcweb\" ) def wrapper(): return get_data(args) res_json = wf.cached_data(f\"navmap_{args}\", wrapper, max_age=30) 1번과 3번의 문제는 위와 같이 간단하게 수정함으로써 정상적으로 워크플로우가 작동함을 확인하였다. 그리고 2번 문제 역시 자료의 배열을 분석하여 그에 맞춰 다시 코드를 수정함으로써 일단 워크플로우는 정상적으로 결과를 내뱉게 되었다. 여기까지의 자세한 코드 수정 내역은 다음에서 확인할 수 있다. https://github.com/inchanS/alfnaversearch/commit/79c441e282f679dd6630c1d93650fce327dded46 ","date":"2024-03-17","objectID":"/posts/202403171602/:3:1","tags":["macOS","Apps","Alfred"],"title":"Alfred workflow - 네이버 검색 워크플로우 지도 검색 개선","uri":"/posts/202403171602/"},{"categories":["Apps"],"content":"개선 1 - IP address로부터 위치정보 활용 여기서 하나의 고민이 생긴다. 사용자의 위치를 제공하는 coords값을 어떻게 유동적으로 만들 수 있을까? macOS의 위치정보를 가져와 활용하는 방법이 가장 먼저 떠올랐지만 이건 앱의 시스템 권한 부여 문제를 우선 해결해야하기에 다른 방법을 강구했다. 그래서 일단 간단하게 적용한 것이 IP address로부터 위치를 가져와 적용하는 것이었다. # 기본 위치 정보: 서울 시청의 좌표 default_latitude = 37.5665 default_longitude = 126.9780 def get_ip_location(): try: r = web.get('http://ip-api.com/json/') r.raise_for_status() data = r.json() return data[\"lat\"], data[\"lon\"] except Exception as e: # 위치 정보를 가져오는 데 실패할 경우 기본값 반환 print(f\"위치 정보를 가져오는 데 실패했습니다: {e}\") return default_latitude, default_longitude # 위치 정보를 가져오고, 실패할 경우 기본값 사용 latitude, longitude = get_ip_location() f get_data(word): url = 'https://map.naver.com/v5/api/search/instant-search' params = dict(query=word, type=\"all\", coords= f'{latitude},{longitude}', lang=\"ko\", caller=\"pcweb\" ) headers = {\"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 12_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.4 Safari/605.1.15\"} r = web.get(url, params, headers=headers) r.raise_for_status() return r.json() 위와 같이 IP로부터 위치 정보를 가져오는 API 함수를 하나 더 추가하여 정보를 가져오고 그로부터 원래의 기능 함수를 작동시키는 방향으로 진행을 했다. 그런데… 뭔가 반응이 굼뜨고 느리다. 이유는 IP address로부터 위치정보를 가져오는 API의 속도가 즉각적이지 않다보니 조금씩 지연이 생기는데, 워크플로우의 자동완성 스크립트는 검색창에서 검색어를 입력할 때마다 즉, 자음과 모음 등 키 스트로크가 발생할 때마다 작동한다. 문제는 바로 이 때, IP주소를 가져오는 함수까지 빈번하게 실행되다 보니 지연이 누적되어 사용자 입장에서는 검색어 입력후 결과를 보기까지 꽤나 잦은 딜레이를 경험하게 되었다. ","date":"2024-03-17","objectID":"/posts/202403171602/:3:2","tags":["macOS","Apps","Alfred"],"title":"Alfred workflow - 네이버 검색 워크플로우 지도 검색 개선","uri":"/posts/202403171602/"},{"categories":["Apps"],"content":"개선 2 - IP address API의 지연 체감 제거 어떻게 하면 좋을까 고민을 했다. 일단 IP address로부터 위치 정보를 가져오는 것은 검색어 입력시 계속해서 실행될 필요가 없었다. 그렇다면 최초의 검색창을 열었을 때 한번만 수행하고 이후 그 값을 계속해서 사용하는 방향으로 진행을 하면 될 것이다. 이에 IP address 관련 함수를 다른 스크립트로 분리하였다. 그리고 IP주소를 활용한 네이버 지도 검색시 방금 분리한 스크립트로부터 받아온 위치 정보를 활용하게끔 수정하였다. 이제 IP주소를 활용한 네이버 지도 검색시 최초의 호출 키워드를 입력할 때만 IP address API 함수가 작동하고 이후, 호출키워드 다음의 검색어를 입력하는동안 워크플로우는 충분히 느긋하게 위치정보를 가져와 사용하기에 사용자는 미처 API 지연을 체감하지 못한다. 꽤나 훌륭한 Fake 설계인듯 생각된다. ","date":"2024-03-17","objectID":"/posts/202403171602/:3:3","tags":["macOS","Apps","Alfred"],"title":"Alfred workflow - 네이버 검색 워크플로우 지도 검색 개선","uri":"/posts/202403171602/"},{"categories":["Apps"],"content":"개선 3 - 기본 Naver Map 검색시 위치 정보값을 설정할 수 있도록 개선 지금까지의 코드에서 위치정보는 스크립트안에서 입력하여 사용하도록 하였는데, 기반 위치가 바뀔 때마다 스크립트를 수정한다는 것은 꽤나 피곤한 일이다. 때문에 Alfred의 Configure Workflow… 에서 간단하게 위도와 경도값을 넣을 수 있도록 변수를 따로이 빼놓았다. ","date":"2024-03-17","objectID":"/posts/202403171602/:3:4","tags":["macOS","Apps","Alfred"],"title":"Alfred workflow - 네이버 검색 워크플로우 지도 검색 개선","uri":"/posts/202403171602/"},{"categories":["Apps"],"content":"개선 4 - 위치 정보를 가져오는 API 변경과 캐시 사용 IP address로부터 위치 정보를 가져오는 더 빠른 API를 찾았다. 답은 역시 네이버 지도 검색 화면의 개발자도구에 있었다. 이전에는 해외 사이트를 사용해서인지 모르겠지만 응답시간이 꽤 있어 나름 Fake 설계(?)까지 하며 그 지연시간을 줄이려 했었는데 이젠 그럴 필요가 없어졌다. 네이버 역시 자체적으로 IP를 통해 위치정보를 찾는 API를 사용하고 있었고 그 주소를 사용했더니 응답시간 뿐만 아니라 위치 정확도까지 더 향상되었다. (하지만 역시 정확한 주소를 기대하기에는 직접 위치정보를 넣어 사용하는것만 못하다.) 때문에 이전의 이중 스크립트 실행(위치정보를 가져오는 스크립트와 검색 스크립트)이 아닌, 검색 스크립트에서 바로 위치정보를 가져오고 이를 캐싱하여 사용하도록 했다. 맥북을 가지고 초고속 열차를 타고 업무를 보는게 아닌 이상 위치 정보라는게 매순간 갱신할 필요가 없다. 그래서 캐싱 유효시간을 설정해두고 적절한 시간마다 갱신하도록 하면 사용자 경험상 지연체감은 더욱 더 최소화될 것이다. 그리고 캐싱 유효시간은 Configure Workflow에서 사용자가 따로이 설정가능하도록 외부에서도 조작가능하게 열어두었다. ","date":"2024-03-17","objectID":"/posts/202403171602/:3:5","tags":["macOS","Apps","Alfred"],"title":"Alfred workflow - 네이버 검색 워크플로우 지도 검색 개선","uri":"/posts/202403171602/"},{"categories":["Apps"],"content":"최종 개선 워크플로우 ","date":"2024-03-17","objectID":"/posts/202403171602/:4:0","tags":["macOS","Apps","Alfred"],"title":"Alfred workflow - 네이버 검색 워크플로우 지도 검색 개선","uri":"/posts/202403171602/"},{"categories":["Apps"],"content":"Configure Workflow… 위치정보 설정 위 이미지와 같이 워크플로우의 Configure 화면에서 사용자 위치를 직접 입력할 수 있도록 개선 만약 미입력시에는 ‘서울 시청’ 위치를 기본값으로 사용하도록 세팅하였다. Naver Map에서 주소 검색시 알고리즘은 사용자의 현재 위치를 기반으로 가까운 곳을 우선하여 보여주는데 이로 인해 사용자의 근처 주소 정보를 더 정확히 제공하기에 설정하고 사용하면 더욱 요긴하다. 위도 경도 정보는 https://www.findlatlng.org 사이트에서 쉽게 확인할 수 있다. ","date":"2024-03-17","objectID":"/posts/202403171602/:4:1","tags":["macOS","Apps","Alfred"],"title":"Alfred workflow - 네이버 검색 워크플로우 지도 검색 개선","uri":"/posts/202403171602/"},{"categories":["Apps"],"content":"네이버 지도 검색 1 - 설정한 위치정보 이용 위 이미지는 워크플로우 설정에서 위치를 인천으로 입력하였을 때의 검색결과이다. ","date":"2024-03-17","objectID":"/posts/202403171602/:4:2","tags":["macOS","Apps","Alfred"],"title":"Alfred workflow - 네이버 검색 워크플로우 지도 검색 개선","uri":"/posts/202403171602/"},{"categories":["Apps"],"content":"네이버 지도 검색 2 - IP address의 위치정보 이용 호출키워드인 nmi를 눌렀을 때 순간적으로 “IP address로부터 위치정보 받기” 라는 선택창이 스쳐지나감을 확인할 수 있는데 바로 이 과정에서 해당 API가 작동하고 사용자는 이후의 검색어를 입력하는 동안 아무런 지연 없이 검색정보를 확인할 수 있다. update 2024.03.19 개선 4 - 위치 정보를 가져오는 API 변경과 캐시 사용 로 인해 이전의 ‘IP address로부터 위치정보 받기’ 라는 안내는 더이상 의미가 없을만큼 지연시간이 대폭 줄어들어 체감상 느끼기 힘들기에, 제거하였다. IP address API로부터 받은 위치정보를 잘 반영하여 결과를 보여주고 있다. ","date":"2024-03-17","objectID":"/posts/202403171602/:4:3","tags":["macOS","Apps","Alfred"],"title":"Alfred workflow - 네이버 검색 워크플로우 지도 검색 개선","uri":"/posts/202403171602/"},{"categories":["Apps"],"content":"워크플로우 다운로드 위 링크에서 본문과 관련된 alfredworkflow 파일을 다운로드 받아 사용할 수 있다. 버전 0.0.5의 코드는 원래 워크플로우 코드 개발자에게 PR을 넣었고 현재 merge 되어 반영되어있는 상태이다.(참고로 0.0.5는 원 코드개발자의 의사에 따라 Configure 설정과 그를 이용한 지도검색은 제외되어있고, 기본 지도검색시 IP주소를 기반으로 하게끔 되어있다.) 지도검색시 ‘장소, 주소, 버스’ 3가지 타입의 결과가 사용자의 위치와 관련한 우선순위(naver 자체 score)에 따라 반환되도록 수정1 (이전버전은 한가지 타입의 결과만을 반환한다.) - 기능 추가 검색어에 해당하는 장소, 주소, 버스의 검색결과가 있을 때만 해당 type의 Search only Place(or Address, Bus) 창이 뜨도록 변경 Search only Place 보기에서 다시 돌아가기 기능 추가 네이버 주식 검색도 추가하였다. - 최신 버전 그리고 다시 사용자가 더욱 세밀한 위치 지정을 할 수 있는 버전과 그 이후의 변경사항을 반영한 워크플로우는 일부 개인적인 설정 및 용도에 맞게 개발하였기에 개별적인 레포지토리인 위 링크에서 다운로드 받을 수 있다. 24.03.22 이 버전 역시 원래 코드개발자에게 PR을 넣었고, 현재 v0.3.2에서 반영되어 있다. ↩︎ ","date":"2024-03-17","objectID":"/posts/202403171602/:4:4","tags":["macOS","Apps","Alfred"],"title":"Alfred workflow - 네이버 검색 워크플로우 지도 검색 개선","uri":"/posts/202403171602/"},{"categories":null,"content":"labs.inchan.dev 블로그의 소개 페이지","date":"2024-03-10","objectID":"/about/","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":" ","date":"2024-03-10","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Info INTJ (장점일까, 단점일까…) BackEnd Free Engineer Maverick Developer Jetbrains의 intellij 프로그램이 예뻐서 코딩하는 중입니다. ","date":"2024-03-10","objectID":"/about/:1:0","tags":null,"title":"About","uri":"/about/"},{"categories":["Apps"],"content":"2024년 3월 현재 브라우저에서 chatGPT 입력이 되지 않는 경우 해결방법에 대한 설명","date":"2024-03-09","objectID":"/posts/202403090724/","tags":["Misc"],"title":"브라우저에서 chatGPT 프롬프트 입력창이 먹통일때 해결방법","uri":"/posts/202403090724/"},{"categories":["Apps"],"content":" ","date":"2024-03-09","objectID":"/posts/202403090724/:0:0","tags":["Misc"],"title":"브라우저에서 chatGPT 프롬프트 입력창이 먹통일때 해결방법","uri":"/posts/202403090724/"},{"categories":["Apps"],"content":"문제상황 브라우저에서 chatGPT 프롬프트의 입력창에 글을 쓰고 아무리 “엔터\"를 눌러도 반응이 가질 않는 상황이 생겼다. 일단 의심되는 변화는 macOS를 sonoma 14.4로 업그레이드 한 정황뿐인데 이래저래 만져보다 문제를 찾았다. 문제는 바로 chatGPT의 local language 지원(Alpha) 기능이었다. (chatGPT 웹사이트의 왼쪽 아래, 자신의 아이디를 클릭하고 나오는 설정에서 확인 가능) 일단 이 기능은 ‘Auto-detect‘가 default로 설정되어있는데, 만약 이 값을 ‘한국어’ 등으로 바꾸었다면 먹통 문제가 발현되었다. 게다가 한번 바꾸면 이 설정에서 아무리 다른 값을 선택해도 먹히질 않는다. 테스트 해본 결과, Safari, chrome 두 브라우저 모두 동일하게 발생했다. 아마도 Beta도 아닌 Alpha 테스트중인 기능이라 생긴 버그가 아닌가 생각된다. 현재 chatGPT에서 알파테스트 중인 브라우저 언어지원 목록에는 한국어가 포함되어있지 않다.1 그런데 아이러니하게도 chatGPT 설정에서 보면 ‘한국어’가 포함되어있어 마치 지원되는 것처럼 선택창에 포함되어있고, 아마도 이러한 부분에서 버그가 발생해 모든 프롬프트 입력이 먹통이 되는 상황이 발생하지 않았나 싶다. 이 문제는 2024년 3월 이후 chatGPT의 업데이트에 따라 자연히 사라질 수 있음! 즉, 브라우저 언어지원에 한국어가 포함되거나 또는 Beta 테스트 이후의 단계에 들어서게 되면 언제든 현재 이 문제는 자연히 해결 될 수 있는 한시적 문제일 것으로 보인다. ","date":"2024-03-09","objectID":"/posts/202403090724/:1:0","tags":["Misc"],"title":"브라우저에서 chatGPT 프롬프트 입력창이 먹통일때 해결방법","uri":"/posts/202403090724/"},{"categories":["Apps"],"content":"해결방법 해결 방법은 브라우저의 site 쿠키 등을 저장한 정보를 지워주면 된다. 웹사이트의 쿠키 및 정보를 지우고 나면 다시 로그인해줘야 하는 번거로움이 있지만 한번만 수행하면 되기에 큰 문제는 없었다. ","date":"2024-03-09","objectID":"/posts/202403090724/:2:0","tags":["Misc"],"title":"브라우저에서 chatGPT 프롬프트 입력창이 먹통일때 해결방법","uri":"/posts/202403090724/"},{"categories":["Apps"],"content":"safari에서 site 정보 지우는 방법 safari 브라우저의 설정에서 “개인정보 보호 / 웹사이트 데이터 관리… \" 메뉴를 클릭한다. “openai\"를 검색해 해당 사이트의 데이터를 제거한다. 다시 chatGPT 사이트에 들어가 로그인하면 끝. chatGPT 상단에 나타나는 배너 조심 이게 간헐적으로 나타나는지 모르겠지만 새로 로그인 하고 chatGPT 페이지에 접속하면 상단에 chatGPT local 언어 지원에 참여하시겠습니까? ‘예’ 뭐 이런 문구의 배너가 뜰때가 있는데 이 배너에서 ‘예’를 선택하면 자동으로 위에서 언급한 local language 설정이 한국어(또는 해당 브라우저의 언어설정에 따라)로 적용되면서 문제가 발생한다. ","date":"2024-03-09","objectID":"/posts/202403090724/:2:1","tags":["Misc"],"title":"브라우저에서 chatGPT 프롬프트 입력창이 먹통일때 해결방법","uri":"/posts/202403090724/"},{"categories":["Apps"],"content":"chrome에서의 해결 방법 위 safari에서의 방법과 비슷한데 설정에 진입하는 방법이 조금 다르다. 아래 사이트를 참고하여 마찬가지로 ‘openai’를 검색해 해당 사이트의 정보만 지워주면 된다. [Chrome] 특정 사이트의 쿠키만 삭제하는 방법 해당 지원목록은 다음 주소에서 확인할 수 있다. ChatGPT language support - alpha (web) | OpenAI Help Center ↩︎ ","date":"2024-03-09","objectID":"/posts/202403090724/:2:2","tags":["Misc"],"title":"브라우저에서 chatGPT 프롬프트 입력창이 먹통일때 해결방법","uri":"/posts/202403090724/"},{"categories":["Dev"],"content":"AWS에서 2024년 2월부터 퍼블릭 IPv4 주소에 대한 요금이 부과되기 시작하면서, 비용 절감을 위해 AWS 인프라를 IPv6로 전환하고 퍼블릭 IP 사용을 최소화하려는 노력이 필요해졌다. 이에 퍼블릭 IP 없이 RDS와 같은 AWS 리소스를 외부에서 접근하는 방법에 대해 자세히 설명한다.","date":"2024-03-05","objectID":"/posts/202403051335/","tags":["AWS","Back-End"],"title":"AWS public IP 요금 부과에 따른 대처 및 EC2를 이용한 RDS 외부 연결하기","uri":"/posts/202403051335/"},{"categories":["Dev"],"content":" 작년 말 즈음 AWS로부터 한통의 메일을 받았고, 그 내용은 AWS에서의 모든 public IP에 대한 요금을 부과 하겠다는 것이었다. 공지 – AWS Public IPv4 주소 요금 변경 및 Public IP Insights 기능 출시 | Amazon Web Services 한국 블로그 AWS에서 퍼블릭(Public) IPv4 주소에 대한 새로운 요금이 도입됩니다. 2024년 2월 1일부터 서비스 연결 여부에 관계없이 모든 퍼블릭 IPv4 주소에 대해 시간당 IP당 0.005 USD의 요금이 부과됩니다. 계정에 할당했지만 EC2 인스턴스에 연결하지 않은 퍼블릭 IPv4 주소에 대해서는 이미 요금이 부과되고 있습니다. 당시 AWS 크레딧이 꽤 여유가 있었기에 차일 피일 미루다 2월 중순쯤 작업(?)을 시작하였다. AWS에서 백엔드 서버와 DB를 이용하고 있던 나는 현재 총 4개의 public IP를 사용하고 있었고, 이 중 프리티어 EC2에 부여된 하나의 주소를 제외한 나머지 3개 주소에 대한 요금이 2월부터 청구되고 있었다.1 ","date":"2024-03-05","objectID":"/posts/202403051335/:0:0","tags":["AWS","Back-End"],"title":"AWS public IP 요금 부과에 따른 대처 및 EC2를 이용한 RDS 외부 연결하기","uri":"/posts/202403051335/"},{"categories":["Dev"],"content":"IPv6 전환 일단 AWS에서 준 문서대로 보안그룹과 VPC 서브넷을 이리 저리 만져보며 IPv6로 세팅을 끝냈다. 그런데 아무리 해도 SSH 접속이 안되었다. 한참을 살펴보다 현재 이용중인 인터넷 서비스업체 (KT 또는 LGT)가 IPv6를 제대로 지원하지 않는다는 사실을 알게 되었다. (만약 이 글을 보는 이의 환경이 IPv6를 지원한다면 여기서 모든게 해결된다!!) IPv6을 테스트합니다. 여기에서 현재 네트워크가 IPv6를 지원하는지 테스트해볼 수 있으며 구글링하면 이외에도 더 있다. 허탈했고 이대로라면 AWS에서 프리티어를 이용하더라도 소액의 금액을 지불하지 않을 수 없는 상황이었다. 현재 이용중인 public IPv4 IP는 다음과 같았다. EC2 public 주소 - 서버 배포용으로 탄력적 IP를 이용중2 RDS public 주소 - DB 클라이언트와 IDE에서 연결하기 위해 사용 Load Balancer - HTTPS와 HTTP를 따로이 연결하며 2개의 public IPv4주소 사용중3 ","date":"2024-03-05","objectID":"/posts/202403051335/:1:0","tags":["AWS","Back-End"],"title":"AWS public IP 요금 부과에 따른 대처 및 EC2를 이용한 RDS 외부 연결하기","uri":"/posts/202403051335/"},{"categories":["Dev"],"content":"issue 즉 현재 상황에서 비용지출을 줄이기 위한 당면과제는 다음과 같았다. EC2의 경우 다행히 하나의 public 주소는 무료이다. HTTPS 연결을 위해 세팅했던 Load Balancer의 public 주소는 다른 방법을 강구해야한다. RDS의 경우 public 주소가 없으면 외부 개발도구에서 연결하기가 힘들다. 1번은 프리티어 플랜으로서 자연스레 해결되었고 2번의 경우는 Route53과 Load Balancer를 이용한 방법에서, 도메인 서비스의 프록싱을 이용한 직접 연결 방법으로 해결하였다. (관련 글 - AWS EC2 서버에 HTTPS 적용하기(2) - Cloudflare 이용 방법 - inchan.dev) 그리고 3번 RDS에 관한 해결안은 이제 아래에서 다룬다. ","date":"2024-03-05","objectID":"/posts/202403051335/:2:0","tags":["AWS","Back-End"],"title":"AWS public IP 요금 부과에 따른 대처 및 EC2를 이용한 RDS 외부 연결하기","uri":"/posts/202403051335/"},{"categories":["Dev"],"content":"public 엑세스 없이 RDS를 외부에서 연결하기 AWS의 RDS 이용시 SQL 클라이언트로 연결할 수 없다면 정말로 불편한게 이만저만이 아니었다. 오랜시간 끙끙대며 여러 문서들을 살펴보고 이것저것 세팅을 바꿔가며 테스트해보고 고민했다. 그렇게 AWS의 여러 공식문서들을 살펴보다가 AWS의 네트워킹 시스템에서 방법을 찾았다. 위 이미지와 같이 외부에서는 EC2의 Public subnet으로 연결할 수 있고, 이 Public subnet은 또다시 RDS의 Private subnet으로 연결할 수 있다. 마치 모니터 연결시 bridge 방식으로 연결하는 데이지 체인과 같은 원리로 연결을 할 수 있었다. (참고문서 - EC2 인스턴스와 DB 인스턴스를 자동으로 연결 - Amazon Relational Database Service) ","date":"2024-03-05","objectID":"/posts/202403051335/:3:0","tags":["AWS","Back-End"],"title":"AWS public IP 요금 부과에 따른 대처 및 EC2를 이용한 RDS 외부 연결하기","uri":"/posts/202403051335/"},{"categories":["Dev"],"content":"AWS EC2와 RDS 연결 AWS의 RDS는 EC2에 연결해둘 수가 있다.4 즉, EC2의 public IPv4주소를 이용하여 외부에서 AWS EC2 내부 넷망으로 들어갈 수 있고 이때, RDS가 연결되어있다면 내부 넷망을 통해 RDS 역시 접속 가능하다는 것이다. 바로 세팅을 해보려는데 EC2와 RDS를 연결하려면 동일한 가용영역 내부에 있어야 무료이며, 다른 가용영역에 있다면 데이터 전송비용이 추가로 부과될 수 있는데… 안타깝게도 다른 영역에 설치되어 있었다. 이에 기존 RDS의 스냅샷을 생성후, 스냅샷 마이그레이션을 통해 현재 EC2와 동일한 가용영역 내에서 새로운 RDS 인스턴스를 생성했다. 그리고 EC2를 연결해주면 일단 외부에서도 접속할 수 있는 길은 트이게 된다. 새로운 RDS 인스턴스 생성시 주의사항 주의할 점은 새로운 RDS 인스턴스 생성 및 마이그레이션시, 설정에서 public 액세스는 반드시 “아니오\"를 택해야 한다. 만약 “예\"를 택할 경우 새로운 RDS의 인스턴스에는 자동으로 public IPv4 주소가 할당되며 곧 비용청구로 이어진다. 새로운 RDS를 생성하고 세팅중이라면 아래와 같이 EC2와 연결해준다. 위와 같이 진행하면 EC2와 같은 가용영역으로 확인하며 설치 진행할 수 있으며 이후의 세팅은 용도에 맞춰 진행하면 된다. 만약 이미 이용중인 RDS를 연결하고자 한다면 해당 RDS의 인스턴스 메뉴 중 “작업” 섹션에서 “EC2 연결하기\"와 같은 메뉴를 볼 수 있다. 가용영역이 같다면 손쉽게 연결할 수 있다. ","date":"2024-03-05","objectID":"/posts/202403051335/:3:1","tags":["AWS","Back-End"],"title":"AWS public IP 요금 부과에 따른 대처 및 EC2를 이용한 RDS 외부 연결하기","uri":"/posts/202403051335/"},{"categories":["Dev"],"content":"SQL 클라이언트에 연결하기 현재 DBeaver, DataGrip 등의 SQL 클라이언트를 사용중인데 연결 방법은 모두 대동소이했다. 아래에서는 DataGrip의 예시로 설명을 한다. UI나 일부 메뉴 이름이 다르더라도 이해하는데 큰 지장은 없으리라 본다. 우선 세션 연결을 위해 “새로운 연결\"을 진행하고, SSH 구성에서 “…“을 클릭한다. 호스트에는 EC2 public IPv4 주소를 입력 포트는 SSH이기에 22 그대로 둔다. 그리고 인증타입은 AWS EC2의 보안그룹에 따라 key.pem 파일이나 비밀번호 등 세팅한 방식에 맞게 맞춰준다. 그리고 연결테스트를 하면 확인 끝! 마무리로 다시 일반 탭(general) 으로 가서 Host 부분을 입력하고 적용 및 확인을 누르면 된다. 이제 EC2의 public주소와 SSH 연결 방법을 통해 RDS 역시 외부망에서 접속할 수 있다. 대략 보름 정도 사용했을 때 5.63USD 정도 청구되었다. 즉 한달이면 약 10USD 정도가 청구된다는 뜻이었다. ↩︎ EC2용 AWS 프리 티어에는 2024년 2월 1일부터 처음 12개월 동안 매월 750시간의 퍼블릭 IPv4 주소 사용이 포함됩니다. - 공지 – AWS Public IPv4 주소 요금 변경 및 Public IP Insights 기능 출시 | Amazon Web Services 한국 블로그 =\u003e 즉 하나의 EC2에서 하나의 IP 주소만 사용한다면 무료이기에 신경쓸 필요가 없다. ↩︎ AWS EC2 서버에 HTTPS 적용하기(1) - Route53과 로드밸런서 이용 - inchan.dev ↩︎ 참고문서 : 자습서: Amazon RDS 데이터베이스에 Amazon EC2 인스턴스 연결 - Amazon Elastic Compute Cloud ↩︎ ","date":"2024-03-05","objectID":"/posts/202403051335/:3:2","tags":["AWS","Back-End"],"title":"AWS public IP 요금 부과에 따른 대처 및 EC2를 이용한 RDS 외부 연결하기","uri":"/posts/202403051335/"},{"categories":["Dev"],"content":"AWS EC2에 HTTPS를 연결하는 방법중 도메인 서비스의 프록싱을 통해 바로 연결하는 방법 소개","date":"2024-03-05","objectID":"/posts/202403051214/","tags":["AWS","Back-End"],"title":"AWS EC2 서버에 HTTPS 적용하기(2) - Cloudflare 이용 방법","uri":"/posts/202403051214/"},{"categories":["Dev"],"content":" 관련 글 AWS EC2 서버에 HTTPS 적용하기(1) - Route53과 로드밸런서 이용 AWS EC2에 HTTPS를 연결하는 방법은 몇가지가 있는데 그 중 하나는 Route53을 이용하는 것이고, 다른 하나는 도메인 서비스의 프록싱을 통해 바로 연결하는 방법이다. 전자의 경우 위 관련 글의 링크에서 참고할 수 있으며 이 글은 후자에 관한 글이다. 우선 EC2 인스턴스에는 public IPv4 주소를 하나 부여할 수 있다. 이는 인스턴스 생성시 설정에서 ‘자동으로 부여하기’를 통해 부여할 수도 있고, 따로이 탄력적 IP를 하나 생성하여 부여할 수도 있다. 바로 이 주소를 다음과 같이 적용할 수 있다. ","date":"2024-03-05","objectID":"/posts/202403051214/:0:0","tags":["AWS","Back-End"],"title":"AWS EC2 서버에 HTTPS 적용하기(2) - Cloudflare 이용 방법","uri":"/posts/202403051214/"},{"categories":["Dev"],"content":"도메인 연결 이름에는 하위도메인을 입력한다. 예를 들어 도메인 주소가 abc.com이라고 한다면, EC2에 연결할 하위도메인으로 ‘api’이라고 입력하면 이제부터 EC2의 주소는 api.abc.com으로 사용할 수 있다. 그리고 콘텐츠 부분에 EC2에 연결되어있는 Public IPv4 주소를 입력하면 된다. 이렇게 하면 일단 EC2주소는 바로 소유하고 있는 도메인으로 연결된다. 누군가 브라우저에서 또는 개발툴에서 등등 위에서 설정한 하위도메인을 포함한 주소로 접속하면 Cloudflare는 이 주소를 EC2의 public IPv4 주소로 연결하여 준다. 매우 간단한 방법이다. ","date":"2024-03-05","objectID":"/posts/202403051214/:1:0","tags":["AWS","Back-End"],"title":"AWS EC2 서버에 HTTPS 적용하기(2) - Cloudflare 이용 방법","uri":"/posts/202403051214/"},{"categories":["Dev"],"content":"서버의 port 연결 하지만 주의할 점이 있는데, 바로 EC2에서 서버의 port 적용 부분이다. 예를 들어 EC2에서 어떤 서버를 작동할때 해당 port를 8000이라고 했다면, 실제 서버의 주소는 http://3.33.333.333:8000이 된다. (여기서 3.33.333.333은 임의의 AWS EC2의 public IPv4 주소이다.) 하지만 Cloudflare에서 프록싱해주는 주소는 3.33.333.333까지이지 :8000 포트를 포함하지 않는다. 물론 위 이미지에서 콘텐츠부분에 포트번호를 넣어봤자 지원하지 않는 IPv4주소라며 오류를 내뱉을 뿐이다. 이때 Cloudflare의 “규칙” 서비스를 이용할 수 있다. Cloudflare의 해당 도메인에서 좌측 사이드메뉴의 규칙 / Origin Rules를 선택한다.1 규칙 이름 : 적당한 규칙이름을 적는다. (아무거나 적어도 상관없다.) 필드 : 전체 URI 연산자 : 다음으로 시작 값 : 아까 DNS에서 설정한 하위도메인의 풀 주소를 적는다. (https로 시작한다.) 대상 포트 : EC2 서버를 가동시킬때 적용한 port번호를 적어준다. 그리고 화면 우측 아래에 있는 ‘배포’ 버튼을 누르면 끝. 보통 1~2분 내에 적용되며 매우 간단하고 쉽다. 프록싱을 통한 HTTPS 적용의 단점 현재 내가 알기로 이 방법이 가장 간단하고 편하며 경제적이다.[^1] 단, 치명적인(?) 단점이 있다. 바로 프록싱 우회로 인한 속도 저하이다. 다음은 현재 Cloudflare의 무료 플랜에서 이용중인 HTTPS 프록싱 우회시 최고 속도와 최저속도이다. 빨라도 812ms 정도이고 느릴땐 무려 1초가 넘을 때도 있었다. 이 정도의 속도는 프론트에서 아무리 최적화를 하더라도 실제 최종사용자는 화면 전환시 ‘깜.빡’하는 딜레이를 느끼며 1초가 넘는 지연시 답답함을 느낄 수 있는 속도이다. 다음은 AWS의 Route53과 로드밸런서를 이용한 HTTPS 적용시 속도이다. 16ms는 거의 클릭과 동시라는 표현이 맞을 정도로 빠른 속도이다. 앞서 언급한 HTTPS를 적용하는 두가지 방법간 속도 비교에 있어 충분한 고민이 필요하다고 생각된다. Cloudflare의 Origin Rules는 무료 플랜에서 10개를 사용할 수 있다. ↩︎ ","date":"2024-03-05","objectID":"/posts/202403051214/:2:0","tags":["AWS","Back-End"],"title":"AWS EC2 서버에 HTTPS 적용하기(2) - Cloudflare 이용 방법","uri":"/posts/202403051214/"},{"categories":["Dev"],"content":"mySQL에서 timestamp 타입 필드를 사용하고, TypeORM의 dateStrings와 timezone 옵션을 적절히 활용하여 시간대를 제어하는 방법을 다룬다. UTC 시간대를 기준으로 데이터를 처리하면서 서버와 클라이언트에서 각각 다른 시간대를 적용할 수 있는 방안에 대해 고민한 결과를 실험과 코드 예시를 통해 공유","date":"2024-03-01","objectID":"/posts/202403010802/","tags":["JavaScript","typeORM","mySQL","NodeJS","Back-End"],"title":"typeORM 시간대 설정에 관한 고찰 - typeORM의 dateStrings와 timezone 옵션에 따른 시간대 혼란","uri":"/posts/202403010802/"},{"categories":["Dev"],"content":" 먼저 알아두기 ","date":"2024-03-01","objectID":"/posts/202403010802/:0:0","tags":["JavaScript","typeORM","mySQL","NodeJS","Back-End"],"title":"typeORM 시간대 설정에 관한 고찰 - typeORM의 dateStrings와 timezone 옵션에 따른 시간대 혼란","uri":"/posts/202403010802/"},{"categories":["Dev"],"content":"mySQL - Date 필드의 타입 종류 mySQL에서 Date 필드의 타입 종류는 다음과 같다. DATE - 날짜만 TIME - 시간만 DATETIME - 시간대가 반영되지 않은 현재 서버 시간대의 날짜와 시간 데이터 TIMESTAMP - 시간대가 반영된 날짜와 시간 데이터 여기서 주의할 점은 TIMESTAMP 이다. 이 데이터는 해당 서버의 시간대를 반영하기에 DB에 저장될 때에는 UTC(+00:00) 시간대로 변환되어 저장된다. 단, 조회시 세션 설정에 따른 시간대를 반영하여 해당 시간대로 변환된 데이터로 조회된다. ","date":"2024-03-01","objectID":"/posts/202403010802/:0:1","tags":["JavaScript","typeORM","mySQL","NodeJS","Back-End"],"title":"typeORM 시간대 설정에 관한 고찰 - typeORM의 dateStrings와 timezone 옵션에 따른 시간대 혼란","uri":"/posts/202403010802/"},{"categories":["Dev"],"content":"typeORM의 dateStrings와 timezone 옵션 dateStrings와 timezone에 대한 개념을 확실하게 잡지 않은 상태에서 서비스를 운영해보니 굉장히 혼란스러웠다. 정확한 서버나 DB의 상태를 내가 통제하고 있지 않은 상태에서 일단 결과물은 내가 원하는대로 나오니까 그동안은 이게 전부이고 이게 맞다고 생각했었다. 하지만 코드를 하나만 바꾸어도 내가 전혀 의도치 않은 방향대로 결과물이 나오는 것을 보며 이는 곧 나의 코드가 다른 환경에서는 언제든지 내가 원치않는 결과를 낼 수 있음을 의미하기에 내심 찝찝했다. 때문에 이에 대한 문서를 살펴보고 다음과 같이 직접 테스트를 해본다. ","date":"2024-03-01","objectID":"/posts/202403010802/:1:0","tags":["JavaScript","typeORM","mySQL","NodeJS","Back-End"],"title":"typeORM 시간대 설정에 관한 고찰 - typeORM의 dateStrings와 timezone 옵션에 따른 시간대 혼란","uri":"/posts/202403010802/"},{"categories":["Dev"],"content":"현재 조건 mySQL의 Date 필드의 타입은 timestamp이다. ","date":"2024-03-01","objectID":"/posts/202403010802/:1:1","tags":["JavaScript","typeORM","mySQL","NodeJS","Back-End"],"title":"typeORM 시간대 설정에 관한 고찰 - typeORM의 dateStrings와 timezone 옵션에 따른 시간대 혼란","uri":"/posts/202403010802/"},{"categories":["Dev"],"content":"dateStrings dateStrings: true DB로부터 Date 타입의 데이터를 String으로 변환하여 가져온다. 이 때, 시스템의 Local TimeZone이 반영된다. 단점은 당연하게도 반환값이 String타입이기에 해당 컬럼과 관련된 서비스 로직에서 Date 타입을 이용한 변환을 사용할 수 없다. dateStrings: false Date 타입을 js 객체 형태로 가지고 있으며 시스템의 Local TimeZone을 적용되지 않고 UTC 시간대 그대로 출력한다. (번거롭지만) 데이터를 개발자가 더 세밀하게 제어할 수 있다. ","date":"2024-03-01","objectID":"/posts/202403010802/:1:2","tags":["JavaScript","typeORM","mySQL","NodeJS","Back-End"],"title":"typeORM 시간대 설정에 관한 고찰 - typeORM의 dateStrings와 timezone 옵션에 따른 시간대 혼란","uri":"/posts/202403010802/"},{"categories":["Dev"],"content":"timezone timezone에 대한 실험 mySQL 세션 시간대 설정과의 관계 SET GLOBAL TIME_ZONE = '+00:00' -- 시간대 설정 SHOW GLOBAL VARIABLES LIKE 'TIME_ZONE' -- 시간대 확인 -- 시간대 변환 후, mySQL의 세션(typeORM)을 종료후 다시 연결 하여야 반영된다. 위와 같은 mySQL 서버의 시간대 설정과 typeORM의 timezone을 통한 시간대 설정은 서로 무관하다. 위 설정은 mySQL의 세션 실행시 적용할 시간대를 설정해주는 건데, 바로 이 부분을 typeORM의 세션에서는 timezone 설정이 이를 따로이 대신 설정해주는 개념이라고 이해하면 편하다. timezone: 'Z' 현재 DB에서 받아오는 시간대를 Zulu 시간대(+00:00)라고 해석하여 받아들인다. 실험 typeORM - timezone = 'Z' 옵션을 추가 한국표준시(KST) 기준 24.02.05 20:04 데이터 입력 결과 2024-02-05T11:04:52.788Z 실제 DB에서는 이 시간대로 저장되어 있으며 typeorm에서도 이 시간으로 나온다. (물론 DB 조회시 조회 세션의 시간대 설정에 따라 변환되어 조회될 수는 있다.) 이 때 서버의 시간대는 KST(한국표준시)라 하더라도 이것은 전혀 무관하게 작동한다. timezone = '\u003cLOCAL\u003e' DB로부터 가져온 데이터를 서버의 시간대에 맞춰 변환한다. 즉, timezone = 'Asia/Seoul로 설정되어있을 때, DB에서 09:00로 내보냈다면 서버는 이 데이터를 서버의 시간대인 KST로 변환하여 반영한다. timezone = 'Z'와 '(LOCAL)'의 차이점 ‘Z’ 형태 : 2024-02-05T11:04:52.788Z typeof : Object (Date 객체) ‘(LOCAL)’ 형태 : 2024-02-05 11:04:52.78829 typeof : string 즉 서버의 시간대를 반영하여 내보내고 싶을 경우, 이 옵션을 사용하여 해당 시간대로 맞추어 사용할 수 있다. ","date":"2024-03-01","objectID":"/posts/202403010802/:1:3","tags":["JavaScript","typeORM","mySQL","NodeJS","Back-End"],"title":"typeORM 시간대 설정에 관한 고찰 - typeORM의 dateStrings와 timezone 옵션에 따른 시간대 혼란","uri":"/posts/202403010802/"},{"categories":["Dev"],"content":"고민 ","date":"2024-03-01","objectID":"/posts/202403010802/:2:0","tags":["JavaScript","typeORM","mySQL","NodeJS","Back-End"],"title":"typeORM 시간대 설정에 관한 고찰 - typeORM의 dateStrings와 timezone 옵션에 따른 시간대 혼란","uri":"/posts/202403010802/"},{"categories":["Dev"],"content":"서버별 시간대가 다르더라도 이를 자동반영할 수 있는 방법? 최종사용자에 따른 시간대를 각각 적용하는 방법은 백엔드 입장에선 사실 굉장히 단순하고 쉽게 생각된다. 그저 Date필드의 값을 UTC 기준으로 내보내고 클라이언트에서는 최종사용자의 브라우저로부터 수집된 국가코드를 판별해 이를 바탕으로 변환하면 되지 싶다. 대충 다음과 같은 코드가 만들어진다. // UTC로 받은 시간 문자열 const utcDate = '2024-03-02T12:00:00Z'; // 로컬 시간대로 변환 const localDate = new Date(utcDate).toLocaleString('ko-KR', { timeZone: 'Asia/Seoul' }); console.log(localDate); // 변환된 로컬 시간 출력 하지만 백엔드 서버를 관리하는 입장에서는 UTC로 데이터가 나오니 테스트를 하면서도 시간대에 대한 감이 맞지 않아 불편하다. 즉, 어떤 시간대에서 애플리케이션 서버가 돌아가더라도 따로이 코드를 수정하지 않고 현지 시간대로 Date값을 반환할 수 있는 방법을 찾고 싶었다. 구글링을 해보면 간단하게 다음 라이브러리들이 나온다. moment - npm moment-timezone - npm 그런데… moment - npm 라이브러리를 살펴보던 중 해당 공식문서에서 이상한 문구를 발견했다. You Probably Don’t Need Moment.js Anymore You don’t (may not) need Moment.js Why you shouldn’t use Moment.js… 4 alternatives to moment.js for internationalizing dates ‘사실 당신은 아마 이 라이브러리가 필요없을지도 모른다‘라니… 살펴보니 무겁게 라이브러리를 사용할 필요 없이 간단한 플랫폼의 메소드만으로도 충분히 가능했다. 무조건 무거운 라이브러리 패키지를 사용하는 것만이 능사는 아니라고 말하고 있다.1 ","date":"2024-03-01","objectID":"/posts/202403010802/:2:1","tags":["JavaScript","typeORM","mySQL","NodeJS","Back-End"],"title":"typeORM 시간대 설정에 관한 고찰 - typeORM의 dateStrings와 timezone 옵션에 따른 시간대 혼란","uri":"/posts/202403010802/"},{"categories":["Dev"],"content":"적용방안 조건 다음 조건으로 DB로부터 UTC 기준의 시간대를 서버로 가져온다. dateStrings = false (따로이 설정하지 않으면 false가 기본값이다.) timezone은 따로 설정하지 않고 DB의 시간대를 그대로 받아온다는 조건을 만든다. 코드 DB서버에서 받아온 Date 정보(UTC 시간대)로부터 애플리케이션 서버의 현지 시간대(getTimezoneOffset())와의 차이를 계산하여 반환함으로써, 어느 시간대의 서버에서든지 서버가 설치된 현지시간대로 데이터를 받아볼 수 있다. // 애플리케이션 서버의 타임존을 고려하여 Date타입을 재가공 // (ex. 2021-08-01T00:00:00.000Z -\u003e 2021-08-01 00:00:00) public formatDate(date: Date): string { const localDateTime: Date = new Date( date.getTime() - date.getTimezoneOffset() * 60 * 1000 ); return localDateTime.toISOString().substring(0, 19).replace('T', ' '); } 코드 적용 class DateUtils { public static formatDate(date: Date): string { const localDateTime: Date = new Date( date.getTime() - date.getTimezoneOffset() * 60 * 1000 ); return localDateTime.toISOString().substring(0, 19).replace('T', ' '); } } class UserService { async getUser(targetId: number): Promise\u003cExtendedUser\u003e { const result = await UserRepository.findOne( {where: {id: targetId} } ); if (!result) { return null; // 사용자를 찾을 수 없는 경우 } return { ...result, created_at: DateUtils.formatDate(user.created_at), updated_at: DateUtils.formatDate(user.updated_at), // deleted_at은 선택적 필드이므로, 존재할 때만 변환 deleted_at: user.deleted_at ? DateUtils.formatDate(user.deleted_at) : null, }; async getUsers(condition: any): Promise\u003cExtendedUser[]\u003e { const result = await SomeRepository.find(condition); return result.map((data: User) =\u003e ({ ...data, created_at: DateUtils.formatDate(data.created_at), updated_at: DateUtils.formatDate(data.updated_at), deleted_at: data.deleted_at ? DateUtils.formatDate(data.deleted_at) : null, })); } } 위와 같이 필요한 컬럼에 위 함수를 적용하여 시간대와 형태를 바꿀 수 있다. 단점은 이로 인해 해당 entity의 반환 타입이 깨진다는 문제가 생긴다. 예를들어 원래 User entity에서 created_at 컬럼의 타입은 Date타입이다. 그런데 해당 함수로 인해 반환되는 값의 타입은 string이기에 타입 에러가 나타나고 따라서 추가적인 작업을 해줘야 한다. entity의 타입 확장하기 위에서 나타나는 타입 에러를 해결하는 방법은 2가지가 있다. 먼저 간단하게 반환 타입을 Promise\u003cany\u003e로 처리하는 방법이 있다. 무적의 any이다. 두번째는 아래와 같이 확장된 타입 또는 인터페이스를 만들어주어 반환 타입으로 적용해주는 것이다. // 원래의 User 엔티티 타입, Date 관련 컬럼의 타입은 `Date`이다. class User extends BaseEntity { @PrimaryGeneratedColumn() id!: number; @CreateDateColumn({ type: 'timestamp', transformer: dateTransformer }) created_at!: Date; @UpdateDateColumn({ type: 'timestamp', transformer: dateTransformer }) updated_at!: Date; @DeleteDateColumn({ type: 'timestamp', transformer: dateTransformer }) deleted_at?: Date | null; @Column({ unique: true }) nickname: string; @Column({ select: false }) password: string; @Column({ unique: true }) email: string; } // User 타입에서 Date관련 컬럼의 타입을 string으로 바꾼 인터페이스를 생성한다. interface ExtendedUser extends Omit\u003cUser, 'created_at' | 'updated_at' | 'deleted_at'\u003e { created_at: string; updated_at: string; deleted_at: string | null; } 장점 DB 서버의 시간대는 DB 담당자 편한 시간대로 바꾸어도 필드는 timestamp이기에 앱서버에서는 균일하게 UTC 시간대로 받아온다. 따라서 애플리케이션 서버 담당자는 서버의 시간대만 신경쓰면 된다. 만약, 클라이언트에서 브라우저별 시간대를 고려하겠다면 localDateTime 변수대신 date 인자를 그대로 반환하면 된다. 그리고 그 저 함수를 클라이언트에서 그대로 다시 사용할 수도 있다. ","date":"2024-03-01","objectID":"/posts/202403010802/:2:2","tags":["JavaScript","typeORM","mySQL","NodeJS","Back-End"],"title":"typeORM 시간대 설정에 관한 고찰 - typeORM의 dateStrings와 timezone 옵션에 따른 시간대 혼란","uri":"/posts/202403010802/"},{"categories":["Dev"],"content":"추가 업데이트 1 - formatDate함수를 Entity에 바로 적용하기 위 함수를 서비스 로직에서 적용하고자 하는 해당 Date 필드에 따로이 사용할 수도 있지만 애시당초 typeORM에서 모든 Date 필드에 한번에 적용하게끔 하는 방법도 있다. 무엇보다 이 방법은 위에서 언급한 추가적인 interface를 사용하지 않고 entity 본래의 타입 그대로 반환 타입에서 사용할 수 있다는 장점이 있다. 바로 typeORM의 transformer 옵션을 사용하면 된다. 현재 프로젝트의 entities에서 id, created_at, updated_at, deleted_at과 같은 기본 필드는 base entity라는 추상화 클래스로 따로이 만들어놓고 여기에서 확장하여 각각의 entity를 작성하였다. 즉 base entity에서 해당 컬럼에 다음과 같이 적용하면 이제 모든 Date 필드는 자동으로 위 함수가 적용된 채로 조회할 수 있다. import { DateUtils } from '../utils/dateUtils'; export const dateTransformer: ValueTransformer = { from: (value: Date) =\u003e value instanceof Date ? DateUtils.formatDate(value) : value, to: (value: Date) =\u003e value, }; export abstract class Base extends BaseEntity { @PrimaryGeneratedColumn() id!: number; @CreateDateColumn({ type: 'timestamp', transformer: dateTransformer }) created_at!: Date; @UpdateDateColumn({ type: 'timestamp', transformer: dateTransformer }) updated_at!: Date; @DeleteDateColumn({ type: 'timestamp', transformer: dateTransformer }) deleted_at?: Date | null; } 이 방법은 일일이 서비스로직에서 해당 필드에 함수를 적용했을 때보다 훨씬 더 코드가 깔끔해지고 단일게 공통적으로 적용할 수 있다는 장점이 있다. ","date":"2024-03-01","objectID":"/posts/202403010802/:2:3","tags":["JavaScript","typeORM","mySQL","NodeJS","Back-End"],"title":"typeORM 시간대 설정에 관한 고찰 - typeORM의 dateStrings와 timezone 옵션에 따른 시간대 혼란","uri":"/posts/202403010802/"},{"categories":["Dev"],"content":"추가 업데이트 2 만약 클라이언트는 UTC 기준의 시간대로 데이터를 받아보려하고, 서버관리자는 현지시간대로 확인을 하려한다면?? 여러가지로 고민해봤지만 일단 Entity의 transformer를 동적으로 건드릴 수는 없었다. 왜냐하면 entity파일은 DB연결시점에 고정적으로 작동하기 때문이다. 때문에 결국은 모든 서비스 로직을 손봐야한다. 우선, 아래와 같이 미들웨어를 만들어준다. // '/global' 경로를 포함하는 요청에 대한 미들웨어 추가 (최상단 위치시킬 것) router.use((req, res, next) =\u003e { // 요청 경로가 '/global'로 시작하는지 확인 req.global = req.path.startsWith('/global'); next(); }); // 이후 router.use 코드들... 그리고 formatDate함수의 클래스에 추가적인 메소드를 더해준다. 아래의 processDateValues() 메소드가 그것이다. export class DateUtils { const localDateTime: Date = new Date( date.getTime() - date.getTimezoneOffset() * 60 * 1000 ); return localDateTime.toISOString().substring(0, 19).replace('T', ' '); } // DB에서 가져오는 반환 값 중 Date 타입의 모든 값을 formatDate 함수로 처리해주는 함수 // typeORM Entities의 options 중 transformer를 사용하지 않고 service 로직에서 처리를 할 때 사용 public static async processDateValues(result: any): Promise\u003cany\u003e { // 쿼리 실행 후에 반환된 결과를 가공 if (Array.isArray(result)) { // 반환된 결과가 배열인 경우 return result.map(item =\u003e this.processItem(item)); } else { // 반환된 결과가 단일 객체인 경우 return this.processItem(result); } } private static processItem(item: any): any { // 객체의 모든 속성을 순회하면서 Date 타입인 경우에만 가공 for (const key in item) { if (item.hasOwnProperty(key) \u0026\u0026 item[key] instanceof Date) { item[key] = DateUtils.formatDate(item[key]); } else if (typeof item[key] === 'object') { // 객체인 경우 재귀적으로 processItem 호출 item[key] = this.processItem(item[key]); } } return item; } } 그리고 모든 서비스 로직을 손본다. class SomeService { async processData(req) { const result = await someRepository.find(); // 요청 경로에 '/global'이 포함되어 있지 않은 경우에만 데이터 변환 적용 if (!req.isGlobal) { return DateUtils.processDateValues(result); } return result; } } 우선은 원하는대로, 클라이언트에서의 UTC 시간대 값과 서버의 현지시간대를 고려한 값 모두의 요구를 맞출 수 있도록 구색은 갖춰봤지만… 아무래도 위 방법은 모든 서비스로직을 번거롭고 거추장스럽게 하고 있고, 깨끗한 코드(Clean Code)라고 하기 어렵기에 추천하지 않지만 이런 방법을 이용할 수 있다는 의도로 기록을 남긴다. 가장 추천하는 방법은 백엔드에서는 시간대를 UTC로 내보내고, 클라이언트에서 이를 따로이 변환하는 것이 가장 깔끔하다고 생각한다. github 관련 커밋 보기 - 함께 읽어볼만한 글 자바스크립트에서 타임존 다루기 (2) : NHN Cloud Meetup 2024년 자바스크립트 부피 팽창 | GeekNews 관련된 재밌는 기사가 있다. 2024년 자바스크립트 부피 팽창 | GeekNews 에서는, 필요기능만을 구현하기위한 가장 가볍고 최적화된 코드는 숙련된 순수 자바스크립트 코드라고도 한다. ↩︎ ","date":"2024-03-01","objectID":"/posts/202403010802/:2:4","tags":["JavaScript","typeORM","mySQL","NodeJS","Back-End"],"title":"typeORM 시간대 설정에 관한 고찰 - typeORM의 dateStrings와 timezone 옵션에 따른 시간대 혼란","uri":"/posts/202403010802/"},{"categories":["Env"],"content":"프로젝트별로 node 버전이 다를 경우, nvm을 통해 버전별 node를 설치하고 프로젝트에 맞춰 해당 버전을 자동으로 적용시킬 수 있는 방법 소개","date":"2024-02-01","objectID":"/posts/202401311413/","tags":["terminal"],"title":"프로젝트별 각각의 node 버전을 nvm으로 자동 적용하기","uri":"/posts/202401311413/"},{"categories":["Env"],"content":" source : nvm-sh/nvm: Node Version Manager - POSIX-compliant bash script to manage multiple active node.js versions 새로운 프로젝트는 Node 버전 중 현재 최신 LTS 버전인 20 버전으로 진행하려는데, 기존 진행중인 프로젝트가 AWS-ec2에서 node@16으로 세팅되어있던 터라, macOS에서 2개 버전을 설치해 사용해야하는 상황이 왔다.1 때문에 여러 node 버전을 사용할 수 있는 nvm(Node version Manager)을 설치 설치는 brew를 통해 설치했고 이후 과정은 brew에서 자세하게 설명해준다. ","date":"2024-02-01","objectID":"/posts/202401311413/:0:0","tags":["terminal"],"title":"프로젝트별 각각의 node 버전을 nvm으로 자동 적용하기","uri":"/posts/202401311413/"},{"categories":["Env"],"content":"장점 OS에 하나 이상의, 여러 version별 node를 설치하고 프로젝트별로 각각 다른 버전의 node를 사용할 수 있다. ","date":"2024-02-01","objectID":"/posts/202401311413/:1:0","tags":["terminal"],"title":"프로젝트별 각각의 node 버전을 nvm으로 자동 적용하기","uri":"/posts/202401311413/"},{"categories":["Env"],"content":"단점 node 프로젝트 진행시 터미널에서 해당 프로젝트를 적정 버전으로 실행시키기 위해서는 다음 명령어로 node의 버전을 사용할 때마다 항상 지정해줘야 한다. # node 특정 버전 설치 (아래 3가지 명령어중 택일) nvm install --lts // 가장 최신 LTS 버전을 설치 nvm install 18.10.2 // 특정 버전 설치 nvm install 20 // 20.x.x에서 가장 최신버전을 설치 # nvm default 버전 설정 방법 nvm alias default 14.17.0 # 설치된 node version들 확인 nvm list # 또는 아래 명령어를 사용해도 된다. nvm ls # 특정 버전 사용을 명령 (버전 번호만 적어주면 된다.) # 예시 nvm use 18 // 설치된 18.x.x에서 가장 최신버전을 사용 nvm use 16.12.5 // 16.12.5 버전을 사용 ","date":"2024-02-01","objectID":"/posts/202401311413/:2:0","tags":["terminal"],"title":"프로젝트별 각각의 node 버전을 nvm으로 자동 적용하기","uri":"/posts/202401311413/"},{"categories":["Env"],"content":"보완 ","date":"2024-02-01","objectID":"/posts/202401311413/:3:0","tags":["terminal"],"title":"프로젝트별 각각의 node 버전을 nvm으로 자동 적용하기","uri":"/posts/202401311413/"},{"categories":["Env"],"content":"1. IDE 사용하기 IntelliJ 또는 WebStorm과 같은 IDE에서는 따로 실행 환경을 설정해줄 수가 있다. ","date":"2024-02-01","objectID":"/posts/202401311413/:3:1","tags":["terminal"],"title":"프로젝트별 각각의 node 버전을 nvm으로 자동 적용하기","uri":"/posts/202401311413/"},{"categories":["Env"],"content":"2. .nvmrc 사용하기 .nvmrc 관련 문서 프로젝트별 root 폴더에 .nvmrc 파일을 만들고 아래와 같이 버전만 딱 한줄 넣어주면 된다. 16.20.2 또는 16버전의 최신 버전 사용시 16이라고만 입력해도 된다. 그리고 터미널에서 해당프로젝트에 위치해있을 때, 버전을 입력할 필요없이 nvm use 명령어만 입력해줘도 .nvmrc파일에서 버전을 읽어와 해당 node버전을 적용한다. note 이제 프로젝트별로 nvm에서 노드 버전을 지정할 때, 그동안 타이핑하기 힘들었던 3단어(nvm, use, \u003cversion\u003e)에서 무려 1단어를 줄이고 단 2단어만(nvm, use) 입력하면 된다. 별거 아닌것 같지만 프로젝트별 각각의 노드 버전을 사용할때마다 찾아보는 것도 일이다. 프로젝트별 node 버전을 자동으로 적용하기 위 방법도 편리하지만 여전히 사용할 때마다 명령어를 입력해줘야 하기는 마찬가지다. 이제 이 명령어조차 생략하는 방법을 소개한다. 이 방법은 바로 위 .nvmrc 사용하기에서 더 나아가 nvm use 명령까지 자동화하는 방법으로서 다음의 과정을 따른다. .nvmrc파일이 존재할 경우, 자동으로 nvm use 명령을 내리고 해당 버전을 적용한다. 해당 파일이 없는 root 폴더로 진입했을 땐, default 버전으로 되돌린다. zshrc를 사용하고 있다면 다음 문서의 내용대로 진행할 수 있다.2 # place this after nvm initialization! # nvm을 설치하면서 추가했던 스크립트 바로 다음에 위치시키면 정상 작동한다. autoload -U add-zsh-hook load-nvmrc() { local nvmrc_path nvmrc_path=\"$(nvm_find_nvmrc)\" if [ -n \"$nvmrc_path\" ]; then local nvmrc_node_version nvmrc_node_version=$(nvm version \"$(cat \"${nvmrc_path}\")\") if [ \"$nvmrc_node_version\" = \"N/A\" ]; then nvm install elif [ \"$nvmrc_node_version\" != \"$(nvm version)\" ]; then nvm use fi elif [ -n \"$(PWD=$OLDPWD nvm_find_nvmrc)\" ] \u0026\u0026 [ \"$(nvm version)\" != \"$(nvm version default)\" ]; then echo \"Reverting to nvm default version\" nvm use default fi } add-zsh-hook chpwd load-nvmrc load-nvmrc 터미널 테마 powerlevel10k 사용자는 주의 zsh에서 혹시 Powerlevel10k를 사용하고 있다면 기본 터미널 앱 또는 IDE의 터미널 등에서 다음과 같은 경고 메세지를 볼 수 있다. [WARNING]: Console output during zsh initialization detected. When using Powerlevel10k with instant prompt, console output during zsh initialization may indicate issues. You can: Recommended: Change ~/.zshrc so that it does not perform console I/O after the instant prompt preamble. See the link below for details. * You will not see this error message again. * Zsh will start quickly and prompt will update smoothly. Suppress this warning either by running p10k configure or by manually defining the following parameter: typeset -g POWERLEVEL9K_INSTANT_PROMPT=quiet * You will not see this error message again. * Zsh will start quickly but prompt will jump down after initialization. Disable instant prompt either by running p10k configure or by manually defining the following parameter: typeset -g POWERLEVEL9K_INSTANT_PROMPT=off * You will not see this error message again. * Zsh will start slowly. Do nothing. * You will see this error message every time you start zsh. * Zsh will start quickly but prompt will jump down after initialization. 위 경고 메세지에서 친절하게 제시한 권장 해결방법대로 nvm 자동 스크립트를 Powerlevel10k 스크립트보다 더 윗부분으로 올리면 된다. 대개 Powerlever10k 스크립트는 ~/.zshrc 파일의 가장 상단에 위치하고 있다. 단점은 IDE에서 터미널을 실행시켰을 때, 이전에 비해 조금 더 늦게 프롬프트가 나타난다. EC2에서 node 버전 업 호환을 위해서는 최초 인스턴스 세팅시 진행하는 Linux 버전을 최신버전으로 바꾸면 가능하다. 하지만 그 과정이 꽤나 번거롭다. ↩︎ zshrc에서 명령 스크립트 공식문서 ↩︎ ","date":"2024-02-01","objectID":"/posts/202401311413/:3:2","tags":["terminal"],"title":"프로젝트별 각각의 node 버전을 nvm으로 자동 적용하기","uri":"/posts/202401311413/"},{"categories":["Dev"],"content":"OOP로 작성된 NodeJS 백엔드 코드에서 리포지토리의 단일 인스턴스 생성을 위한 리팩토링 과정 설명","date":"2024-01-26","objectID":"/posts/202401261149/","tags":["project_review","NodeJS","Back-End"],"title":"Node.js에서 OOP 기반 리포지토리에 적용한 싱글톤 디자인패턴","uri":"/posts/202401261149/"},{"categories":["Dev"],"content":" Project Tech Stack Overview Language: TypeScript Platform: Node.js Web Framework: Express.js Database: MySQL ORM: TypeORM Cloud Storage: AWS S3 Development Tool: WebStorm Version Control and Issue Tracking: GitHub Issue 얼마 전, 모든 코드를 OOP로 리팩토링한 뒤, 한 문제를 발견했다. 리포지토리 역시 클래스 기반으로 내보내고 있었는데 때문에 각각의 서비스 로직에서는 이를 new Class() 형태로 불러오면서 단일 인스턴스가 아닌 여러 인스턴스의 생성 위험이 있었다. OOP 리팩토링 전, 함수기반에서는 최초 호출된 datasource 인스턴스 하나를 계속해서 사용하기에 문제가 없었는데 이 부분을 간과했다. ","date":"2024-01-26","objectID":"/posts/202401261149/:0:0","tags":["project_review","NodeJS","Back-End"],"title":"Node.js에서 OOP 기반 리포지토리에 적용한 싱글톤 디자인패턴","uri":"/posts/202401261149/"},{"categories":["Dev"],"content":"싱글톤 디자인 패턴 적용 ","date":"2024-01-26","objectID":"/posts/202401261149/:1:0","tags":["project_review","NodeJS","Back-End"],"title":"Node.js에서 OOP 기반 리포지토리에 적용한 싱글톤 디자인패턴","uri":"/posts/202401261149/"},{"categories":["Dev"],"content":"싱글톤 디자인 패턴의 장점 일단 Node.js는 싱글스레드 방식이라 단일 인스턴스가 가지는 멀티스레드에서의 동시성 문제 걱정이 없다. OOP로 작성된 클래스의 리포지토리를 서비스 로직에서 각각 new RepositoryClass() 방식으로 불러오는건 메모리 낭비이며 각 인스턴스간의 상태 동기화 문제 발생 가능성이 있다. ","date":"2024-01-26","objectID":"/posts/202401261149/:1:1","tags":["project_review","NodeJS","Back-End"],"title":"Node.js에서 OOP 기반 리포지토리에 적용한 싱글톤 디자인패턴","uri":"/posts/202401261149/"},{"categories":["Dev"],"content":"처음 시도한 코드 export class EntityRepository extends Repository { private static instance: EntityRepository; private constructor() { super(Entity, dataSource.createEntityManager()); } public static getInstance(): EntityRepository { if (!EntityRepository.instance) { EntityRepository.instance = new Repository(); } return Repository.instance; } // ... 이후 로직 } 현재 프로젝트에서는 총 5개의 리포지토리가 사용되고 있었고, 최초 코드를 각 리포지토리에 적용하였다. ","date":"2024-01-26","objectID":"/posts/202401261149/:1:2","tags":["project_review","NodeJS","Back-End"],"title":"Node.js에서 OOP 기반 리포지토리에 적용한 싱글톤 디자인패턴","uri":"/posts/202401261149/"},{"categories":["Dev"],"content":"싱글톤 패턴 확인 // 단일 인스턴스 확인 함수 function isSingleton(repository: { getInstance(): T }): string { const instance1: T = repository.getInstance(); const instance2: T = repository.getInstance(); const isInstanceEquel: boolean = instance1 === instance2; const message: string = 'only instance is '; const result: string = message + isInstanceEquel; return result; } // 적용 router.get('/singletons', (req: Request, res: Response) =\u003e { const testUserRepository = isSingleton(UserRepository); const testFeedRepository = isSingleton(FeedRepository); const testFeedListRepository = isSingleton(FeedListRepository); const testCommentRepository = isSingleton(CommentRepository); const testFeedsymbolRepository = isSingleton(FeedSymbolRepository); res.status(200).json({ testUserRepository, testFeedRepository, testFeedListRepository, testCommentRepository, testFeedsymbolRepository, }); }) 간단하게 위와 같이 함수를 만들고 테스트용 API 주소를 하나 만들고, 라우터에서 적용해보았다. 결과는 다음과 같이 확인할 수 있다. HTTP/1.1 200 OK Content-Type: application/json; charset=utf-8 Res: { \"testUserRepository\": \"only instance is true\", \"testFeedRepository\": \"only instance is true\", \"testFeedListRepository\": \"only instance is true\", \"testCommentRepository\": \"only instance is true\", \"testFeedsymbolRepository\": \"only instance is true\" } 리팩토링 완성 후, 이 단일 인스턴스 확인 함수를 테스트코드에서 사용할 수 있다. ","date":"2024-01-26","objectID":"/posts/202401261149/:1:3","tags":["project_review","NodeJS","Back-End"],"title":"Node.js에서 OOP 기반 리포지토리에 적용한 싱글톤 디자인패턴","uri":"/posts/202401261149/"},{"categories":["Dev"],"content":"리팩토링 시도 여러개의 리포지토리 클래스의 상단에 꽤 여러 줄이 들어가다보니 이를 줄이고 싶었고, 다음 3가지 방법을 시도해보았다. singletonFactory() - 단일 인스턴스를 관리하는 함수를 만들어 사용 싱글톤 디자인패턴으로 만드는 factory Class를 만들고 각 리포지토리를 extends 를 이용하여 확장 OOP에서의 컨트롤러를 내보내는 방식처럼 애초에 클래스 생성 후, 단일 인스턴스로 내보내는 방법 ","date":"2024-01-26","objectID":"/posts/202401261149/:2:0","tags":["project_review","NodeJS","Back-End"],"title":"Node.js에서 OOP 기반 리포지토리에 적용한 싱글톤 디자인패턴","uri":"/posts/202401261149/"},{"categories":["Dev"],"content":"1. singletonFactory() 함수 도전 // 함수 생성 export function singletonFactory\u003cT extends { instance: T | null; new (): T; }\u003e (Class: T): T { if (!Class.instance) { Class.instance = new Class(); } return Class.instance; }; // 적용 const entityRepository = singletonFactory(EntityRepository); const anotherRepository = singletonFactory(AnotherRepository); 실패! 우선, 함수에서는 적용하려는 클래스에서 static instance를 생성할 수가 없다. **즉, 리포지토리 클래스에서 결국 하나하나 static instance`를 넣어줘야 했다.** 이렇게 되면 리포지토리 클래스 코드작성에도 신경써야하면서 굳이 함수로 불러와 써야하다보니 혹떼려다 혹 하나 더 붙인 꼴이 되었다. 생각해보니 만약 함수를 성공적으로 만들었다 하더라도 export하고 다른 서비스로직에서 import할 때 역시 꽤 번거롭다. 서비스 로직에서 리포지토리를 불러올 때 일반적인 code convention상의 문제로써, 리포지토리 클래스가 대문자가 아닌 소문자 형태의 함수로 불러와지다 보니 코드 가독성은 차치하더라도 새로운 서비스 로직에서 사용할 때, 이 함수를 간과하고 그대로 리포지토리를 불러와 작성할 혼동 역시 예상되었다. 코드 작성시 이렇게 고려해야할 요소들이 늘어나는건 좋지 않다. // 현재 불러오는 형태 const entityRepository = EntityRepository.getInstance(); // 함수형에서 불러오는 형태 const entityRepository = singletonFactory(EntityRepository); ","date":"2024-01-26","objectID":"/posts/202401261149/:2:1","tags":["project_review","NodeJS","Back-End"],"title":"Node.js에서 OOP 기반 리포지토리에 적용한 싱글톤 디자인패턴","uri":"/posts/202401261149/"},{"categories":["Dev"],"content":"2. 상속을 통한 팩토리 클래스 실패 두번째 extends factory Class 방식을 시도해보았는데, 리포지토리는 factory 클래스와 Repository를 모두 상속해야하는 터라 메서드 상속이 도저히 되지 않았다. 찾아보니 JavaScript에서는 다중상속이 불가능하다고 한다.1 ","date":"2024-01-26","objectID":"/posts/202401261149/:2:2","tags":["project_review","NodeJS","Back-End"],"title":"Node.js에서 OOP 기반 리포지토리에 적용한 싱글톤 디자인패턴","uri":"/posts/202401261149/"},{"categories":["Dev"],"content":"3. export default new Repository() 방식은? repository class를 생성한 후, export default new EntityRepository로 내보내는건 어떨까? 결과는 성공 테스트를 해보니 단일 인스턴스는 보장되었다. 또한 전역 접근도 가능하다. 하지만, 모듈이 로드될 때 인스턴스가 생성됨으로 테스트 코드 작성시 모의하기가 굉장히 까다로워졌다. 반면, 최초의 코드인 정적메서드 Repository.getInstance()를 사용하는 싱글톤 패턴에서는 메서드가 호출될 때 생성되는 지연 초기화(Laze Initialization) 방식이기에 테스트 용이성이 확실히 좋다. ","date":"2024-01-26","objectID":"/posts/202401261149/:2:3","tags":["project_review","NodeJS","Back-End"],"title":"Node.js에서 OOP 기반 리포지토리에 적용한 싱글톤 디자인패턴","uri":"/posts/202401261149/"},{"categories":["Dev"],"content":"최종 확정 코드 export class EntityRepository extends Repository { private static instance: EntityRepository; private constructor() { super(Entity, dataSource.createEntityManager()); } public static getInstance(): EntityRepository { if (!this.instance) { this.instance = new this(); } return this.instance; } // ... 이후 로직 } 결국 위와 같이 최초 코드에서 this를 사용하는 정도로 타협(?)하고 이를 Webstorm의 Live Template로 만들어 사용하기로 하였다.2 // WebStorm - Live Template Code export class $1$ extends Repository\u003c$2$\u003e { private static instance: $1$; private constructor() { super($2$, dataSource.createEntityManager()); } public static getInstance(): $1$ { if (!this.instance) { this.instance = new this(); } return this.instance; } 그런데 this는 실행 컨텍스트를 따지는 까다로운 키워드라 혹시 서비스로직에서 사용되다 다른 객체를 참조하진 않을까 우려되었다. 몇가지 테스트를 해보고 문서를 찾아보니 다행히 static 메소드 안에서의 this는 해당 클래스에 바인딩 되듯이 작동하였고 전혀 문제 없었다.3 ","date":"2024-01-26","objectID":"/posts/202401261149/:2:4","tags":["project_review","NodeJS","Back-End"],"title":"Node.js에서 OOP 기반 리포지토리에 적용한 싱글톤 디자인패턴","uri":"/posts/202401261149/"},{"categories":["Dev"],"content":"적용 후, 차이점 확인 싱글톤 디자인패턴의 장점 중 하나인 메모리효율은 적용 전과 비교하여 과연 얼마나 차이가 날까? Node --inspect4 와 Chrome DevTools5 를 이용해 적용 전후의 수치를 확인해보았다. DevTools-Comparison view에서의 컬럼 설명 보기 (click) Size Delta (크기 차이): 이 값은 두 스냅샷 간의 총 메모리 사용량의 차이를 나타낸다. 양수 값은 메모리 사용량이 증가했음을, 음수 값은 감소했음을 의미하며 메모리 사용량의 변화를 직접적으로 보여준다. New (새로 생성됨) 및 Deleted (삭제됨): 새로 생성된 객체와 삭제된 객체의 수를 비교한다. 많은 수의 객체가 생성되었지만 삭제되지 않는 경우, 메모리 누수나 비효율적인 메모리 사용이 발생했을 가능성이 있다. Delta (차이): 객체 유형별로 얼마나 많은 객체가 추가되거나 제거되었는지를 보여준다. 특정 유형의 객체가 지속적으로 증가하는 경향을 보이는 경우, 이는 메모리 누수의 신호일 수 있다. Alloc. Size (할당 크기) 및 Freed Size (해제 크기): 할당된 메모리와 해제된 메모리의 양을 비교한다. 메모리 할당과 해제가 균형을 이루고 있는지 확인할 수 있다. 총 메모리 사용율 before: 57.179 MB after: 54.386 MB 차이 : 2.793 MB 메모리 감소율 약 -4.8% SnapShot의 Comparison view에서 보이는 바와 같이 싱글톤 코드 작업 전과 비교했을 때, 일단 전체적으로 메모리 사용율이 확연히 줄었다. 그리고 객체 수에서도 Repository(파란 동그라미 부분)를 살펴보면 확실하게 하나의 객체만을 보여주고 있다. github 관련 커밋 보기 - 참고문서 - 믹스인 - [JavaScript] - 다중상속  ↩︎ 같이 볼만한 글 - WebStorm - console.log(변수) 자동완성 라이브 템플릿 만들기 - inchan.dev ↩︎ static - JavaScript | MDN - 위 사이트에서 한글 번역본을 보면 해당 설명이 누락되어있다. 영문판으로 참조하기를 추천. ↩︎ Node.js — Debugging - Getting Started ↩︎ Chrome DevTools | Chrome for Developers 메모리 용어 | DevTools | Chrome for Developers ↩︎ ","date":"2024-01-26","objectID":"/posts/202401261149/:3:0","tags":["project_review","NodeJS","Back-End"],"title":"Node.js에서 OOP 기반 리포지토리에 적용한 싱글톤 디자인패턴","uri":"/posts/202401261149/"},{"categories":["Dev"],"content":"Node.js환경에서 트랜잭션 롤백이 제대로 되지 않아 발생한 데이터 무결성 문제의 원인 분석과 해결방법 기술","date":"2024-01-24","objectID":"/posts/202401241017/","tags":["typeORM","mySQL","project_review","Debugging"],"title":"Node.js와 TypeORM에서 겪은 트랜잭션 롤백 문제 - 원인 분석과 해결 방법","uri":"/posts/202401241017/"},{"categories":["Dev"],"content":" Project Tech Stack Overview Language: TypeScript Platform: Node.js Web Framework: Express.js Database: MySQL ORM: TypeORM Cloud Storage: AWS S3 Server: AWS EC2 Development Tool: WebStorm Collaboration Tool: Slack Version Control and Issue Tracking: GitHub Issue ","date":"2024-01-24","objectID":"/posts/202401241017/:0:0","tags":["typeORM","mySQL","project_review","Debugging"],"title":"Node.js와 TypeORM에서 겪은 트랜잭션 롤백 문제 - 원인 분석과 해결 방법","uri":"/posts/202401241017/"},{"categories":["Dev"],"content":"문제 상황 프로젝트에서 발견된 주요 문제는 TypeORM을 사용하는 Node.js 환경에서 트랜잭션 관리와 롤백이 제대로 이루어지지 않는 것이었다. 이는 게시글을 등록하는 로직에서 발견되었으며, 예기치 않은 오류가 발생했을 경우 데이터베이스에 불필요한 데이터가 잔존하는 결과를 초래했다. 좀 더 자세히 풀어보자면, 현재 게시글을 등록하는 로직은 다음과 같다. 요청을 받으면 트랜잭션을 시작한다. DB는 새로운 게시글 데이터를 생성한다. 전달받은 파일 링크의 유효성 검사를 실행한다. 파일링크의 유효성 검사가 끝나면, 해당 파일 링크를 1번에서 생성한 게시글에 정상적으로 연결한다. 이후 나머지 로직을 실행한다. 2번부터 5번까지는 트랜잭션 내부에서 처리되며 에러가 발생하지 않는다면 트랜잭션을 커밋한다. 6번의 과정 중 에러가 발생한다면 2번부터 5번까지의 모든 작업을 취소하고 롤백하여 DB의 상태를 되돌린다. 이 중, 3번 과정에서 에러를 발생시켰더니 status code, error message와 함께 에러는 정상적으로 반환되었지만, 1번에서 생성한 게시글의 데이터가 롤백되지 않고 그대로 DB에 잔존해있었다. 즉, 트랜잭션 롤백이 제대로 작동하고 있지 않는 문제였다. 일러두기 사실, 이 문제는 typeORM transaction에서 repository 사용하기 - inchan.dev 이 글에서 이미 동일하게 발생했던 이슈이고 해결했었다. 그런데 왜 똑같은 문제가 다시 발생했을까? 이전과 지금의 로직에서 바뀐 점은 바로 해당 레포지토리의 성격 변화이다. 이전 글에서는 해당 레포지토리가 함수적 확장 레포지토리로 선언되어있었지만 OOP 리팩토링 과정에서 클래스 기반 레포지토리로 수정되었다. 때문에 함수가 사용되는 맥락이 아닌 새로운 인스턴스에서 트랜잭션이 실행되고 있기에 다시 한번 문제가 생긴 것이다. 함수적 확장 레포지토리에서 queryRunner를 사용할 때, 이것은 이미 시작된 트랜잭션 내에서의 작업을 의미한다. 즉, queryRunner.startTransaction()을 호출한 후, queryRunner의 컨텍스트 내에서 따로이 다른 레포지토리의 save, update, delete 같은 메소드를 호출하면, 이 메소드들은 queryRunner에 의해 시작된 트랜잭션 내에서 실행되었다. 하지만 클래스 기반 레포지토리에서는 save, update, delete 같은 메소드가 다르게 작동한다. 클래스를 구성하는 코드 안에서는 super(Feed, dataSource.createEntityManager())를 호출하여 각 인스턴스가 고유의 엔티티 매니저를 가지도록 구성되었고, 이렇게 사용자 정의 클래스 레포지토리를 사용함으로써, 해당 레포지토리는 더 이상 기본 dataSource의 연결과 컨텍스트를 직접 사용하지 않고, 대신 각 인스턴스별로 독립된 엔티티 매니저를 통해 작업을 수행하게 되면서 별개로 트랜잭션 범위 관리를 해줘야 하는 상황이 된 것이다. ","date":"2024-01-24","objectID":"/posts/202401241017/:1:0","tags":["typeORM","mySQL","project_review","Debugging"],"title":"Node.js와 TypeORM에서 겪은 트랜잭션 롤백 문제 - 원인 분석과 해결 방법","uri":"/posts/202401241017/"},{"categories":["Dev"],"content":"문제상황의 서비스 코드 private executeTransactionWithRetry = async ( attempt: number, feedInfo: TempFeedDto | FeedDto, fileLinks: string[], options: FeedOption ): Promise\u003cFeed\u003e =\u003e { const queryRunner = dataSource.createQueryRunner(); await queryRunner.connect(); await queryRunner.startTransaction(); try { const newFeedInstance = plainToInstance(Feed, feedInfo); const newFeed = await queryRunner.manager .withRepository(this.feedRepository) .createFeed(newFeedInstance); // ^^^^^^^^^^^ 이 메서드가 롤백되지 않고 그대로 커밋되어버리는 문제 발생 await this.uploadFileService.updateFileLinks( queryRunner, newFeed, fileLinks ); // 여기서 에러를 발생시키고 롤백을 작동시켰다. // ... 이후 코드 } catch (err: any) { await queryRunner.rollbackTransaction(); // ... 이후 에러 코드 } } ","date":"2024-01-24","objectID":"/posts/202401241017/:1:1","tags":["typeORM","mySQL","project_review","Debugging"],"title":"Node.js와 TypeORM에서 겪은 트랜잭션 롤백 문제 - 원인 분석과 해결 방법","uri":"/posts/202401241017/"},{"categories":["Dev"],"content":"수정 이전의 메서드 코드 // 위 문제상황의 서비스코드에서 문제되고 있는 createFeed 메소드의 코드를 살펴본다. // await queryRunner.manager // .withRepository(this.feedRepository) // .createFeed(newFeedInstance); async createFeed(feedInfo: Feed) { const feed = this.create(feedInfo); await this.save(feed); // \u003c= 1차 수정 return await this.findOne({ // \u003c= 2차 수정 loadRelationIds: true, where: { user: { id: feedInfo.user.id } }, order: { id: 'DESC' }, }); } ","date":"2024-01-24","objectID":"/posts/202401241017/:1:2","tags":["typeORM","mySQL","project_review","Debugging"],"title":"Node.js와 TypeORM에서 겪은 트랜잭션 롤백 문제 - 원인 분석과 해결 방법","uri":"/posts/202401241017/"},{"categories":["Dev"],"content":"트랜잭션 중첩 문제 트랜잭션을 관리하는 해당 서비스를 요청하고 터미널에서 mySQL의 흐름을 살펴보니, START TRANSACTION이 두번 연속으로 호출되고 있었다. 그리고 문제의 메서드 코드부분은 트랜잭션 과정이 끝나기 전에 먼저 커밋해버리고 이후 따로이 트랜잭션 롤백이 일어나고 있었다. START TRANSACTION 이후 트랜잭션 내부 로직에서는 커밋과 롤백, 둘 중 하나만 이루어져야 한다. 그런데 이상하게도 커밋과 롤백이 연이어 발생하고 있었다. 문제가 되고있는 typeORM의 save 메서드 문서를 찾아보았다. save - Saves a given entity or array of entities. If the entity already exist in the database, it is updated. If the entity does not exist in the database, it is inserted. It saves all given entities in a single transaction (in the case of entity, manager is not transactional). Also supports partial updating since all undefined properties are skipped. Returns the saved entity/entities. typeorm gitbook 문서 중 TypeORM의 save 및 update 메서드는 내부적으로 자체 트랜잭션을 생성하고 커밋한다.1 이는 별도의 트랜잭션 관리 없이 사용될 때는 문제가 되지 않지만, 따로이 트랜잭션을 관리하는 경우 문제를 일으킬 수 있다. 특히, createQueryRunner를 사용하여 시작된 트랜잭션 내에서 save 메서드를 호출하면 트랜잭션이 중첩되어, 기대했던 롤백 동작이 정상적으로 이루어지지 않게 된다. ","date":"2024-01-24","objectID":"/posts/202401241017/:2:0","tags":["typeORM","mySQL","project_review","Debugging"],"title":"Node.js와 TypeORM에서 겪은 트랜잭션 롤백 문제 - 원인 분석과 해결 방법","uri":"/posts/202401241017/"},{"categories":["Dev"],"content":"1차 수정된 메서드 코드 async createFeed(feedInfo: Feed, queryRunner: QueryRunner) { const feed = queryRunner.manager.create(Feed, feedInfo); await queryRunner.manager.save(feed); // \u003c= parameter에 queryRunner를 추가하고 함께 수정한 코드 const result = await this.findOne({ loadRelationIds: true, where: { user: { id: feedInfo.user.id } }, order: { id: 'DESC' }, }); return result; } ","date":"2024-01-24","objectID":"/posts/202401241017/:2:1","tags":["typeORM","mySQL","project_review","Debugging"],"title":"Node.js와 TypeORM에서 겪은 트랜잭션 롤백 문제 - 원인 분석과 해결 방법","uri":"/posts/202401241017/"},{"categories":["Dev"],"content":"Isolation Level의 중요성 1차 수정 이후 새로운 문제가 생겼다. 해당 함수는 새로이 생성된 게시글 데이터의 정보를 반환해야하는데, 새로 생긴 데이터가 아닌 그 이전의 데이터를 반환하는 문제가 발생된 것이다. 트랜잭션의 고립 수준(Isolation Level)은 다수의 트랜잭션이 동시에 실행될 때 각각의 트랜잭션이 서로에게 미치는 영향을 정의한다. TypeORM에서는 이 고립 수준을 설정하여, 트랜잭션 간의 간섭을 최소화하고, 데이터의 일관성과 무결성을 유지할 수 있다. 이번 문제에서는 findOne 메서드가 트랜잭션 외부의 데이터에 접근하고 있었기에, 트랜잭션 내에서 생성된 데이터를 올바르게 찾지 못한 것이었다. 데이터 트랜잭션의 ACID Atomicity(원자성): 트랜잭션의 모든 단계가 완료되지 않으면 트랜잭션이 끝나지 않는다. 이는 데이터가 항상 올바른 상태를 유지하도록 돕는다. 예를 들어, 계좌 이체의 경우 돈을 보내는 계좌에서 돈을 빼기만 하고 받는 계좌에 돈을 더하지 않으면 데이터 무결성이 깨진다. Consistency(일관성): 트랜잭션은 데이터베이스의 상태를 일관된 상태에서 다른 일관된 상태로 이동하도록 한다. 만약 트랜잭션 중간에 문제가 발생하면 트랜잭션은 롤백되어 데이터베이스를 이전 일관된 상태로 되돌린다. Isolation(고립성 또는 독립성): 동시에 여러 트랜잭션이 수행되더라도 각 트랜잭션은 서로에게 영향을 주지 않는다. 이는 각 트랜잭션이 독립적으로 수행되도록 보장하며, 이는 병렬 처리를 가능하게 한다. Durability(내구성 또는 지속성): 트랜잭션이 성공적으로 완료되면 그 결과는 영구적으로 데이터베이스에 저장된다. 시스템이 중단되더라도 트랜잭션으로 인한 변경사항은 손실되지 않는다. 참조링크 - ACID - 위키백과, 우리 모두의 백과사전 ","date":"2024-01-24","objectID":"/posts/202401241017/:3:0","tags":["typeORM","mySQL","project_review","Debugging"],"title":"Node.js와 TypeORM에서 겪은 트랜잭션 롤백 문제 - 원인 분석과 해결 방법","uri":"/posts/202401241017/"},{"categories":["Dev"],"content":"2차 수정된 메서드 코드 async createFeed(feedInfo: Feed, queryRunner: QueryRunner) { const feed = queryRunner.manager.create(Feed, feedInfo); await queryRunner.manager.save(feed); const result = await queryRunner.manager.findOne(Feed, { loadRelationIds: true, where: { user: { id: feedInfo.user.id } }, order: { id: 'DESC' }, }); // findOne 메소드 역시 dataSource 레포지토리에 직접 연결이 아닌 queryRunner를 끌어와 해당 트랜잭션 내에서 수행될 수 있도록 한다. return result; } 수정된 코드에서는 queryRunner.manager.save와 queryRunner.manager.findOne을 사용하여 해당 기능을 트랜잭션 내부에서 동작할 수 있도록 확실히 제어한다. 이를 통해, 트랜잭션 중첩 문제를 해결하고, 트랜잭션의 고립성을 보장하고 있다. ","date":"2024-01-24","objectID":"/posts/202401241017/:3:1","tags":["typeORM","mySQL","project_review","Debugging"],"title":"Node.js와 TypeORM에서 겪은 트랜잭션 롤백 문제 - 원인 분석과 해결 방법","uri":"/posts/202401241017/"},{"categories":["Dev"],"content":"롤백되지 않는 MySQL의 AUTO_INCREMENT 필드 특이점을 발견했다. 트랜잭션 롤백 후에도 MySQL의 AUTO_INCREMENT 필드(특히 PK ID)는 증가된 상태를 유지하고 있다는 사실이다. 예를 들자면, 현재 가장 최근 데이터의 ID는 10번이다. 트랜잭션 로직 내부에서 데이터가 생성되었다가(11번 ID 부여) 에러발생으로 인해 롤백하며 데이터베이스는 새로운 데이터 생성 이전의 상태로 되돌아갔다. 그런데 이후 정상적으로 생성된 데이터의 ID는 11번이 아닌 12번이 되었다. 이는 동시성 제어 및 성능 최적화를 위한 설계 특성인듯 싶다. 대규모 웹서비스의 상황으로 예를 넓혀보자면, (상황) 여러 사용자가 동시에 데이터 생성 로직을 요청하고 있을 때, (조건) 그 중 한 트랜잭션이 롤백된 후,AUTO_INCREMENT 필드가 이전 값으로 되돌아간다면, (결과) 동시 실행중인 다른 로직에서는 ID 충돌의 위험이 발생할 것이다. 즉, 여러 트랜잭션이 동시에 실행되고 있을 때 한 트랜잭션의 롤백이 다른 트랜잭션에 영향을 주면서 다른 여러 트랜잭션에서의 ID 할당에 혼란이 야기되는 상황이 그려진다. 이에 AUTO_INCREMENT 값의 연속성을 유지함으로써, 트랜잭션 간의 독립성을 보장하고, 데이터베이스의 일관성 및 무결성을 유지할 수 있음을 알 수 있다. github 해당 커밋 보기 - 참고문서 Transactions - typeorm Repository APIs - typeorm 관련 글 - typeORM transaction에서 repository 사용하기 - inchan.dev TypeORM에서 자체 트랜잭션을 진행하는 메소드는 save, remove, insert, update, delete 등이 있다. ↩︎ ","date":"2024-01-24","objectID":"/posts/202401241017/:4:0","tags":["typeORM","mySQL","project_review","Debugging"],"title":"Node.js와 TypeORM에서 겪은 트랜잭션 롤백 문제 - 원인 분석과 해결 방법","uri":"/posts/202401241017/"},{"categories":["Misc"],"content":"github Pages를 이용한 블로그에 Cloudflare에서 관리중인 도메인 연결 중, 리다이렉션 문제로 해당 웹페이지가 정상적으로 나타나지 않을 때의 문제 기술","date":"2024-01-12","objectID":"/posts/202401122324/","tags":["Misc"],"title":"Github Pages에 Cloudflare 도메인 연결 중 리다이렉션 실패가 난다면","uri":"/posts/202401122324/"},{"categories":["Misc"],"content":" ","date":"2024-01-12","objectID":"/posts/202401122324/:0:0","tags":["Misc"],"title":"Github Pages에 Cloudflare 도메인 연결 중 리다이렉션 실패가 난다면","uri":"/posts/202401122324/"},{"categories":["Misc"],"content":"문제상황 구글이 도메인 사업을 매각한 후, 도메인 이전을 차일피일 미루다 최근에서야 Cloudflare로 도메인을 이전하였다.1 모든 DNS 역시 정상적으로 이전하였고, 메일서비스를 비롯한 다른 서비스들도 정상작동함을 확인하였다. 그런데 갑자기 해당 도메인의 주소가 정상적으로 작동하지 않는 문제가 생겼다. 브라우저에서 20회 이상의 리다이렉션으로 페이지 불러오기 실패 ","date":"2024-01-12","objectID":"/posts/202401122324/:1:0","tags":["Misc"],"title":"Github Pages에 Cloudflare 도메인 연결 중 리다이렉션 실패가 난다면","uri":"/posts/202401122324/"},{"categories":["Misc"],"content":"해결방법 Cloudflare side menu - SSL/TLS - 개요 : SSL/TLS 암호화 모드를 가변에서 전체로 변경했더니 문제 해결! ","date":"2024-01-12","objectID":"/posts/202401122324/:2:0","tags":["Misc"],"title":"Github Pages에 Cloudflare 도메인 연결 중 리다이렉션 실패가 난다면","uri":"/posts/202401122324/"},{"categories":["Misc"],"content":"그런데 왜?? 현재 다른 설정은 항상 HTTPS 사용이 체크되어있는데 이 설정과 충돌해서일까?? github Setting - pages에서는 다음과 같이 설정해놓았다. HTTPS provides a layer of encryption that prevents others from snooping on or tampering with traffic to your site. When HTTPS is enforced, your site will only be served over HTTPS. ","date":"2024-01-12","objectID":"/posts/202401122324/:3:0","tags":["Misc"],"title":"Github Pages에 Cloudflare 도메인 연결 중 리다이렉션 실패가 난다면","uri":"/posts/202401122324/"},{"categories":["Misc"],"content":"이유 발견 도메인 서비스는 위 이미지와 같은 흐름을 가지고 있다. 깃허브에서 Enforce HTTPS에 체크가 되어있다면, 아래와 같이 도메인 서버는 깃허브에게 HTTPS로만 보내야 응답을 받을 수 있다. 즉, 클라우드플레어의 SSL/TLS 암호화 모드에서 원본서버로 가는 길 역시 암호화를 해줘야함으로, 전체모드로 설정해야한다. 만약 깃허브의 Enforce HTTPS에 체크하지 않았다면 반대로 클라우드 플레어의 SSL/TLS 암호화 모드에서는 아래와 같이 가변으로 설정해도 무방하다.2 Warning github pages와 연결시, SSL/TLS 암호화모드를 없음으로 설정하면 웹페이지는 무조건 처음의 문제가 발생한다. 이는 깃허브에서 custom domain으로 redirect 시킬 때 무조건 https로 보내기때문이다. ","date":"2024-01-12","objectID":"/posts/202401122324/:3:1","tags":["Misc"],"title":"Github Pages에 Cloudflare 도메인 연결 중 리다이렉션 실패가 난다면","uri":"/posts/202401122324/"},{"categories":["Misc"],"content":"요약 즉, 브라우저에서는 무조건 https로 시작하여 클라우드플레어로 들어가야하고 클라우드플레어에서 깃허브 서버로 가는 길은 Enforce HTTPS 여부에 따라 설정을 맞춰줘야 한다. 참고로 구글도메인보다 더 싸다. ↩︎ ‘전체’로 설정해도 상관없음 ↩︎ ","date":"2024-01-12","objectID":"/posts/202401122324/:3:2","tags":["Misc"],"title":"Github Pages에 Cloudflare 도메인 연결 중 리다이렉션 실패가 난다면","uri":"/posts/202401122324/"},{"categories":["Dev"],"content":"AWS Route 53을 이용하여 EC2 서버의 퍼블릭 주소에 인증서를 받고 HTTPS 보안 설정을 하는 방법 소개","date":"2024-01-12","objectID":"/posts/202401122216/","tags":["AWS","Back-End"],"title":"AWS EC2 서버에 HTTPS 적용하기(1) - Route53과 로드밸런서 이용","uri":"/posts/202401122216/"},{"categories":["Dev"],"content":" 관련 글 AWS EC2 서버에 HTTPS 적용하기(2) - Cloudflare 이용 방법 팀 프로젝트로 백엔드 서버를 EC2에서 배포하여 사용중인데 여기에 HTTPS를 적용하여 보았다. 적용방법은 몇가지가 있지만 크게 AWS의 Route53을 이용하는 방법과 도메인서비스의 프록싱을 이용한 방법 등이 있다. 그 중 첫번째 방법에 대해 기술한다. 겁먹지 말기 AWS내에서 각 서비스를 이리저리 다니며 무언가를 자꾸 생성하고 또 세팅하다보니 자칫 어렵다고 지레 겁먹기 쉬운데 실제로 해보면 생각보다 간단하고 쉽다. 구글링을 하다보면 여러 관련 글이 있는데 아래 참고한 2개의 웹페이지가 가장 잘 설명되어있다. EC2 HTTPS로 연결하기 EC2에 HTTPS 적용하기 미리 말해두기 해당 참고 사이트의 글을 보다보면 친절하게 잘 설명되어있음에도 살짝씩 어리둥절할 때가 있다. 왜냐면 AWS 웹 UI가 생각보다 자주 바뀌는듯 해서 설명에 참고된 스크린샷과 현재 시점의 AWS 화면이 다르기 때문이다. 이에 스크린 샷 없이 AWS 서비스의 흐름에 따라 outline 형식으로 설명을 한다. ","date":"2024-01-12","objectID":"/posts/202401122216/:0:0","tags":["AWS","Back-End"],"title":"AWS EC2 서버에 HTTPS 적용하기(1) - Route53과 로드밸런서 이용","uri":"/posts/202401122216/"},{"categories":["Dev"],"content":"1. 도메인 준비 우선 사전 작업으로 도메인이 반드시 필요하다. 도메인 구입에 관해서는 워낙 많은 글들이 있기에 설명은 생략한다. ","date":"2024-01-12","objectID":"/posts/202401122216/:1:0","tags":["AWS","Back-End"],"title":"AWS EC2 서버에 HTTPS 적용하기(1) - Route53과 로드밸런서 이용","uri":"/posts/202401122216/"},{"categories":["Dev"],"content":"2. AWS - Route 53 서비스 호스팅 영역 생성 생성한 호스팅 영역의 레코드 추가 유형 : A-IPv4 주소 및 AWS 리소스로 트래픽 라우팅 값 : EC2 Public IP 추가 이후, NS 유형의 값 4개가 있는 레코드가 더불어 생성됨을 확인 🚀 중요 - 이 NS유형의 값을 1번에서 준비한 도메인 서비스에서 DNS 레코드로 추가한다. ","date":"2024-01-12","objectID":"/posts/202401122216/:2:0","tags":["AWS","Back-End"],"title":"AWS EC2 서버에 HTTPS 적용하기(1) - Route53과 로드밸런서 이용","uri":"/posts/202401122216/"},{"categories":["Dev"],"content":"3. AWS - ACM (AWS Certivicate Manager) 서비스 퍼블릭 인증서 요청 도메인 입력후 검증 (검증 대기는 보통 10분 이내 완료된다.) ‘Route 53에서 레코드 생성’ 버튼을 클릭 AWS-Route 53 서비스로 가서 CNAME 레코드가 대상에 등록되어 완료됨을 확인 ","date":"2024-01-12","objectID":"/posts/202401122216/:3:0","tags":["AWS","Back-End"],"title":"AWS EC2 서버에 HTTPS 적용하기(1) - Route53과 로드밸런서 이용","uri":"/posts/202401122216/"},{"categories":["Dev"],"content":"4. AWS - EC2 서비스 좌측 사이드메뉴에서 로드 밸런싱 - 대상그룹 - 대상그룹 생성 대상유형 : 인스턴스 프로토콜 : HTTP 대상그룹 메뉴에서 ‘편집’ 클릭 본인의 EC2 인스턴스를 선택하고 ‘등록된 항목에 추가’ 버튼을 클릭한 후, ‘저장’ 사이드메뉴 - 로드 밸런싱 - 로드밸런서 - 로드밸런서 생성 Load Balancer 생성 ALB(Application Load Balancer) 선택 기본 구성 매핑 - 가용영역 2개 이상 선택 🚀 가용영역에는 사용중인 EC2가 위치한 영역이 반드시 포함되어야 한다. 보안 그룹 EC2에서 사용중인 보안그룹 또는 새로 생성하여 선택 🚀 중요 - 해당 보안그룹의 인바운드 규칙에는 HTTPS 유형의 규칙이 추가되어있어야 한다. 리스너 및 라우팅 🚀 HTTPS 프로토콜을 리스너 추가 (포트는 자동으로 443에 맞춰지는데 특별한 상황이 아니라면 그대로 둔다.) 주의 - ‘리스너 태그 추가’가 아닌 ‘리스너 추가’ 버튼으로 진행 🚀 중요 - 로드 밸런싱에서 만든 ‘대상그룹’을 선택! 보안 리스너 설정 (이 메뉴는 위에서 HTTPS 리스너를 추가해야만 나타난다.) 기본 SSL/TLS 서버 인증서 - 인증서 소스는 ‘ACM에서’ 선택 방금 발급받은 인증서 선택 ","date":"2024-01-12","objectID":"/posts/202401122216/:4:0","tags":["AWS","Back-End"],"title":"AWS EC2 서버에 HTTPS 적용하기(1) - Route53과 로드밸런서 이용","uri":"/posts/202401122216/"},{"categories":["Dev"],"content":"5. 마지막 - 도메인 설정은 다시 Route 53 서비스에서 Route 53 - 호스팅 영역 - 아까 생성한 도메인 클릭 레코드 편집 2. AWS - Route 53 서비스에서 만든 A 유형의 레코드를 편집 한다. 기존 값은 아마도 EC2의 public IPv4주소가 들어있을 것이다. 편집화면에 보이는 옵션 중 별칭을 선택하면 Application/Classic Load Balancer라는 메뉴를 선택할 수 있다. 이를 선택한 후, 값으로 아까 만든 로드밸런서를 선택한다. 라우팅 정책은 단순 라우팅으로 하면 된다. 하위 도메인으로 설정하고 싶다면 레코드 이름을 기입한다. 예를 들어 등록한 도메인이 abc.com 이라면, HTTPS로 연결할 해당 서버에만 유효한 하위 도메인을 추가할 수 있다. 즉 하위도메인으로 ‘api’를 기입하면, 실제 AWS에서 사용하게 될 해당 주소는 https://api.abc.com/API주소들 이렇게 된다. 1 레코드 유형 : A-IPv4 주소 및 AWS 리소스로 트래픽 라우팅 트래픽 라우팅 대상에서 아까 생성한 로드밸런서 선택 API 툴을 이용해 새로 만든 HTTPS 주소로 API test가 잘 되는지 확인하면 끝! 이 방법을 사용하면 이미 소유하여 사용중인 루트 도메인을 AWS에 등록하여도 별도의 하위도메인으로써 따로이 AWS에서만 사용할 수 있다. 예를 들어 현재 본 사이트에서 사용중인 inchan.dev의 도메인에 api.inchan.dev 의 하위도메인을 뽑아내 연결하여 사용할 수도 있다. 일종의 RESTful API 방법처럼… ↩︎ ","date":"2024-01-12","objectID":"/posts/202401122216/:5:0","tags":["AWS","Back-End"],"title":"AWS EC2 서버에 HTTPS 적용하기(1) - Route53과 로드밸런서 이용","uri":"/posts/202401122216/"},{"categories":["Dev"],"content":"jest.spyOn() 사용중 모듈 분리부터 OOP까지의 리팩토링 과정 기술","date":"2023-06-29","objectID":"/posts/202306291258/","tags":["project_review","Jest","Debugging"],"title":"error log - jest.spyOn()에서 재사용함수에 대한 모의 불가","uri":"/posts/202306291258/"},{"categories":["Dev"],"content":" 사용환경 : Node.Js, Express 언어 : typeScript 테스트 프레임워크 : Jest ","date":"2023-06-29","objectID":"/posts/202306291258/:0:0","tags":["project_review","Jest","Debugging"],"title":"error log - jest.spyOn()에서 재사용함수에 대한 모의 불가","uri":"/posts/202306291258/"},{"categories":["Dev"],"content":"상황 Abstract 같은 파일 (모듈) 내에 있는 함수의 재사용시, 테스트 코드를 작성하는데 모의하기가 상당히 까다롭고 불편함. 특히 테스트 코드가 지나치게 지저분해짐 user.service.ts 파일 내부 다음은 user.service.ts 파일의 내부 코드 요약이다. 초점을 맞춰 볼 곳은, updateUserInfo 함수 내부에서 checkDuplicateEmail 함수와 checkDuplicateNickname 함수가 재사용되고 있다. // ... const checkDuplicateEmail = async ( email: string ) =\u003e { ... } const checkDuplicateNickname = async ( nickname: string ) =\u003e { ... } const updateUserInfo = async (userId: number, userInfo: UserDto) =\u003e { const originUserInfo = await UserRepository.findOne({ where: { id: userId }, }); if ( userInfo.nickname === originUserInfo.nickname \u0026\u0026 userInfo.email === originUserInfo.email \u0026\u0026 !userInfo.password ) { const error = new Error('NO_CHANGE'); error.status = 400; throw error; } if ( userInfo.nickname \u0026\u0026 userInfo.nickname !== originUserInfo.nickname \u0026\u0026 !userInfo.password ) { // 아래 라인이 동일 파일 내 재사용되는 함수 1. await checkDuplicateNickname(userInfo.nickname); } if (userInfo.email \u0026\u0026 userInfo.email !== originUserInfo.email) { // 아래 라인이 동일 파일 내 재사용되는 함수 2. await checkDuplicateEmail(userInfo.email); } if (userInfo.password) { const salt = await bcrypt.genSalt(); userInfo.password = await bcrypt.hash(userInfo.password, salt); } await UserRepository.update(userId, userInfo); return await UserRepository.findOne({ where: { id: userId }, }); }; Question 즉, 내가 원하는 부분은 간단하게 checkDuplicateNickname 함수만을 모의하여 updateUserInfo 함수내부에서 재사용된 checkDuplicateNickname 함수가 호출되었는지 아닌지만 확인하고 싶다. 그런데… 문제는 테스트하려는 함수 내에서 이렇게 재사용되는 함수는 모의할 수가 없다는 것이다 !! 때문에 같은 파일 내부에 있는 함수가 재사용되는 경우, 원래의 함수가 아닌, 내부에 있는 재사용 함수는 모의가 되지 않기에 해당 함수의 내부에서 사용되는 메소드를 모두 모의해야하는 상황이 나타난다. user.service.ts 파일 내에 있는 checkDuplicateNickname 함수 const checkDuplicateNickname = async (nickname: string): Promise\u003cobject\u003e =\u003e { if (!nickname) { const error = new Error(`NICKNAME_IS_UNDEFINED`); error.status = 400; throw error; } // const checkData = await userRepository.findOneBy({ nickname: nickname }); const checkData = await User.findByNickname(nickname); if (!checkData) { return { message: 'AVAILABLE_NICKNAME' }; } if (checkData.nickname === nickname) { const err = new Error( `${checkData.nickname}_IS_NICKNAME_THAT_ALREADY_EXSITS` ); err.status = 409; throw err; } }; user.service.ts 파일 내에 있는 checkDuplicateNickname 함수 const checkDuplicateEmail = async (email: string): Promise\u003cobject\u003e =\u003e { if (!email) { const error = new Error(`EMAIL_IS_UNDEFINED`); error.status = 400; throw error; } const checkData = await User.findByEmail(email); if (!checkData) { return { message: 'AVAILABLE_EMAIL' }; } if (checkData.email === email) { const err = new Error(`${checkData.email}_IS_EMAIL_THAT_ALREADY_EXSITS`); err.status = 409; throw err; } }; 위에서 언급된 재사용되는 checkDuplicateNickname 함수와 checkDuplicateEmail 함수의 내부를 살펴보면, User.findByEmail User.findByNickname 2개의 메소드가 각각 사용되고 있음을 알 수 있다. 하지만 이미 checkDuplicateNickname, checkDuplicateEmail 함수는 따로 유닛 테스트 코드가 작성되어있으므로 굳이 해당 함수 내부까지 모의하여 테스트코드를 작성할 필요는 없다. 하지만 현재의 상황에서는 해당 함수 내부까지 모의를 해야만 하는 상황으로 테스트코드가 상당히 지저분해진다. 작성된 Jest 테스트 코드 describe('updateUserInfo', () =\u003e { const userId = 1; const originUserInfo = { id: userId, nickname: 'oldNickname', email: 'oldEmail@test.com', password: 'oldPassword', }; beforeEach(() =\u003e { jest.resetAllMocks(); UserRepository.findOne = jest.fn().mockResolvedValue(originUserInfo); UserRepository.update = jest.fn().mockResolvedValue(true); // 지저분한 모의 코드 // 모의하는 함수가 아닌 그 함수의 메소드를 모의함으로 코드 해석시 가독성이 떨어진다. // 만약 재사용되는 함수 내부의 메소드가 많으면 많을수록 불필요한 모의가 더 늘어나게 된다. User.findByNickname = jest.fn().mockResolvedValue(null); User.findByEmail = jest.fn().mockResolvedValue(null); bcrypt.genSalt = jest.fn().mockResolvedValue('salt'); bcrypt.hash = jest.fn().mockResolvedValue('hashedPassword'); }); afterAll(() =\u003e { jest.restoreAllMocks(); }); test('사용자 정보 수정 - 닉네임 변경 성공시, 닉네임 중복 여부 확인', async () =\u003e { const newUserInfo: UserDto = { nickname: 'newNickname', email: 'newEmail@test.com', password: 'newPassword', }; const checkDuplicateNicknameSpy = jest.spyOn( usersService, 'checkDuplicateNickname' ); await usersService.updateUserI","date":"2023-06-29","objectID":"/posts/202306291258/:1:0","tags":["project_review","Jest","Debugging"],"title":"error log - jest.spyOn()에서 재사용함수에 대한 모의 불가","uri":"/posts/202306291258/"},{"categories":["Dev"],"content":"과정 여러번의 코드 수정을 해가며 테스트를 해본 결과, 동일 파일 내부에서의 함수를 재사용하는 상황에서 해당 함수를 모의 했을 때, jest는 원래의 함수를 모의하고 추적하며 재사용되는 함수 자체는 기능을 할지라도 그 함수를 모의하고 추적하는 것이 아니었다. 때문에 jest.spyOn() 메소드에 대해 다시 한번 제대로 살펴보았고, 다음은 그에 대한 나의 생각을 정리 기록이다. Note jest.spyOn() 문법은 객체안의 메소드를 모의하고 추적한다. 여기서 중요한 부분은 ‘객체안의 메소드’라는 말이다. user.service.ts 파일안의 함수들을 export default { function1, function2, function3 } 이런 식으로 내보내기 한다는 것은, 저 문법 그대로 user.service.ts라는 객체를 만들고 그 안에 함수들을 메소드화 시킨다는 의미가 된다. 즉, user.service.ts 객체 안의 function1이라는 메소드가 만들어진다는 뜻. jest.spyOn(객체, ‘메소드’)의 사용법대로 적용시켜보면 우리가 흔히 사용하는 다음과 같은 코드가 나온다. jest.spyOn( userService, 'function1') 여기서 중요한 게 export한 메소드, 즉 그 함수만이 내보내기가 된다. 헷갈릴 수 있는데 function2 내부에 있는 function1은 user.service.ts에서 내보내기 한 function1 함수와는 다른 존재이다. function2 내부에 있는 function1의 구현이 function1 이라고 명명된 함수의 코드를 재사용하는 것일 뿐, jest.spyOn( userService, 'function1') 에서 추적하는 메소드는 function2 안에 있는 function1이 아니라 원래의 함수라는 뜻이다. 이를 굳이 jest.spyOn()의 문법대로 표현을 해보자면, function2 내부에서 재사용된 function1 함수를 모의하기 위해서는 userService.function2(객체), function1(메소드)처럼 사용되어야 한다는 뜻이다. 현재 내가 추적하고자 하는 것은 jest.spyOn( userService, 'function1') 이 아니라, jest.spyOn(userService.function2, function1)이다. 그러면 이대로 jest.spyOn(userService.function2, 'function1')으로 사용하면 될것 같지만 이건 사용할 수 없는 문법이다. 왜냐하면 userService.function2 는 객체가 아닌 메소드이기에 객체 자리에서 쓸수 없다는 문법 에러가 발생한다. 때문에 쉬운 방법은 function2안에 있는 function1과 user.service.ts안의 function1을 일치시켜주면 된다. 이렇게 하기 위해서는 객체지향 프로그래밍(OOP) 을 사용할 수 있다. 그게 아니라면 userService.function2 이 부분을 따로이 객체화만 시켜주면 된다. 이때의 가장 간단한 방법은 파일(모듈) 분리이다. 즉 user.service2.ts라는 또다른 파일을 만들고 여기서 내보내기를 하면, jest.spyOn(userService2, 'function1') 이라는 객체안 메소드라는 문법이 완성된다. ","date":"2023-06-29","objectID":"/posts/202306291258/:2:0","tags":["project_review","Jest","Debugging"],"title":"error log - jest.spyOn()에서 재사용함수에 대한 모의 불가","uri":"/posts/202306291258/"},{"categories":["Dev"],"content":"해결방안 ","date":"2023-06-29","objectID":"/posts/202306291258/:3:0","tags":["project_review","Jest","Debugging"],"title":"error log - jest.spyOn()에서 재사용함수에 대한 모의 불가","uri":"/posts/202306291258/"},{"categories":["Dev"],"content":"1. 객체지향 프로그래밍으로 전환 다음과 같이 user.service.ts 파일의 함수들을 OOP로 리팩토링한다. export class UserService { checkDuplicateEmail = async ( email: string ) =\u003e { ... } checkDuplicateNickname = async ( nickname: string ) =\u003e { ... } updateUserInfo = async (userId: number, userInfo: UserDto) =\u003e { ... // 아래 라인이 동일 파일 내 재사용되는 함수 1. await this.checkDuplicateNickname(userInfo.nickname); // 아래 라인이 동일 파일 내 재사용되는 함수 2. await this.checkDuplicateEmail(userInfo.email); ... }; }; 이렇게 하면, 재사용 함수 역시 jest가 스코프하는 부분에서 동일한 함수명으로 사용되기 때문에 테스트 코드 작성시 아주 간단하게 해당 메소드만으로 모의가 가능해진다. 이는 this.checkDuplicateNickname 함수가 UserService 객체 내에 있는 원 함수인 checkDuplicateNickname 그 자체이기 때문이다. ","date":"2023-06-29","objectID":"/posts/202306291258/:3:1","tags":["project_review","Jest","Debugging"],"title":"error log - jest.spyOn()에서 재사용함수에 대한 모의 불가","uri":"/posts/202306291258/"},{"categories":["Dev"],"content":"2. 함수 모듈 분리 앞서 언급한 OOP 방법과 기본 원리는 같다. 재사용되는 함수를 다른 파일로 이동시킨 후, 해당 파일을 import 하여 사용하면 된다. 이때의 user.service.ts 파일 내 updateUserInfo 함수에서의 재사용함수는 validate.service.ts라는 파일 내부로 이동시켰고 이에 따라 다음과 같이 코드를 작성할 수 있다. import validateService from './validate.service.ts' const updateUserInfo = async (userId: number, userInfo: UserDto) =\u003e { // 이전 코드... // 아래 라인이 동일 파일 내 재사용되는 함수 1. =\u003e import한 메소드로 변경 await validateService.checkDuplicateNickname(userInfo.nickname); } if (userInfo.email \u0026\u0026 userInfo.email !== originUserInfo.email) { // 아래 라인이 동일 파일 내 재사용되는 함수 2. =\u003e import한 메소드로 변경 await validateService.checkDuplicateEmail(userInfo.email); } // ... 이후 코드... }); }; 그리고 테스트코드는 아래와 같이 가독성 좋게 바뀌고 성공적으로 테스팅도 보장되었다. describe('updateUserInfo', () =\u003e { // 이전 코드... beforeEach(() =\u003e { jest.resetAllMocks(); UserRepository.findOne = jest.fn().mockResolvedValue(originUserInfo); UserRepository.update = jest.fn().mockResolvedValue(true); // 간단하게 모의 끝!! 해당 함수의 메소드가 아닌 함수 그 자체를 모의함으로 유지보수시 가독성 또한 높아진다. validateService.checkDuplicateNickname = jest.fn(); validateService.checkDuplicateEmail = jest.fn(); // 이후 코드 .. }); afterAll(() =\u003e { jest.restoreAllMocks(); }); it('사용자 정보 수정 - 닉네임 변경 성공시, 닉네임 중복 여부 확인', async () =\u003e { const newUserInfo: UserDto = { nickname: 'newNickname', email: originUserInfo.email, }; await usersService.updateUserInfo(userId, newUserInfo); // 테스트 성공!! expect(usersService.checkDuplicateNickname).toHaveBeenCalled(); expect(usersService.checkDuplicateNickname).toHaveBeenCalledWith(newUserInfo.nickname); 모듈분리로 인한 프로젝트 조직 변경 - 눌러서 보기 기존 조직 구성도 ├── services ├── categories.service.ts ├── comments.service.ts ├── feeds.service.ts ├── search.service.ts ├── symbol.service.ts ├── upload.service.ts ├── uploadFile.service.ts └── users.service.ts 변경된 조직 구성도 ├── services ├── categories.service.ts ├── comments.service.ts ├── feeds.service.ts ├── search.service.ts ├── symbol.service.ts ├── upload.service.ts ├── uploadFile.service.ts └── users ├── auth.service.ts ├── user.service.ts ├── userContent.service.ts └── validator.service.ts 기존 user.service.ts 파일 내 함수들 checkDuplicateNickname - 재사용 함수 checkDuplicateEmail - 재사용 함수 signUp signIn resetPassword updateUserInfo deleteUser findUserInfoByUserId - 재사용 함수 findUserFeedsByUserId - 재사용 함수 findUserCommentsByUserId - 재사용 함수 findUserFeedSymbolsByUserId 위 파일과 관련 함수를 논리적으로 그룹화하고, 도메인 중심의 디렉토리 구조를 사용하여 유사한 기능을 가진 함수를 함께 배치하여 보았다. auth.service.ts - 인증과 관련된 함수들 signUp signIn resetPassword user.service.ts - 사용자 정보 변경과 관련된 기본 함수들 updateUserInfo deleteUser userContent.service.ts - 사용자 콘텐츠 관련 함수들 findUserInfoByUserId findUserFeedsByUserId findUserCommentsByUserId findUserFeedSymbolsByUserId validator.service.ts - DB 입력시 유효성 검사와 관련된 함수들 checkDuplicateNickname checkDuplicateEmail 이런 식으로 파일을 분리시 장점은 각 파일은 그에 해당하는 목적에만 집중하게 된다. 그리고 Jest를 사용하여 테스트 코드를 작성시, 각각의 서비스 파일이 보다 명확한 책임을 가지고 있기 때문에 테스트가 더 쉬워질 뿐더러 테스트 코드 작성 또한 매우 용이해진다. 이에 따라 현재 2개의 git branch를 분기하여 1번 방법인 OOP와 2번의 파일 분리를 대략적으로 진행하여 테스트를 진행해 보았고, 당연한 결과겠지만 아주 간단하게 test 코드가 작성되면서 성공적으로 테스팅이 이루어졌다. ","date":"2023-06-29","objectID":"/posts/202306291258/:3:2","tags":["project_review","Jest","Debugging"],"title":"error log - jest.spyOn()에서 재사용함수에 대한 모의 불가","uri":"/posts/202306291258/"},{"categories":["Misc"],"content":"2023년 6월 19일 Wanted 주최하 'ChatGPT 시대에 구성원의 역량을 어떻게 육성할 것인가?' 웨비나 참석 후의 정리와 생각 기록","date":"2023-06-21","objectID":"/posts/202306211131/","tags":["Misc"],"title":"Webina 참여 회고 - 원티드 주최 'ChatGPT 시대에 구성원의 역량을 어떻게 육성할 것인가?'","uri":"/posts/202306211131/"},{"categories":["Misc"],"content":" 지난 월요일 저녁 “ChatGPT 시대에 구성원의 역량을 어떻게 육성할 것인가?” 라는 아주 흥미로운 주제의 웨비나가 있어 참여하였다. 비단 HR 관계자가 아니더라도 각 팀의 리더 또는 구성원이라면, 또는 조직에 속해있다면, 그리고 발빠르게 변화하는 현시대적 흐름에서 자기 스스로를 육성하는 방면에서도 몹시 듣고 싶은 주제였다. 현재 Chat GPT를 필두로 한 AI가 대다수의 업계에 미치는 파급효과는 어마어마하다. OpenAI로 인한 영향, 방법 등에 관한 콘텐츠가 마구잡이로 쏟아져나오고 있는 현재이다. 이러한 시대적 상황에서 OpenAI가 조금이라도 나의 업무에 미치는 영향이 있다면 이번 웨비나는 쉽게 지나치기 힘든 주제였다. 웨비나 내용중 서두 요약 강사는 현재 미래인재연구소의 김봉준 소장님. 관련기사 주제와 관련된 웨비나 내용을 아주 간략히 정리해보면 다음과 같다. 김봉준 소장님은 현대 사회에서 조직의 인재가 갖추고 있는 역량으로서 다음 3가지를 말한다. 이른바 K.S.A K (knowledge) 지식 S (skill) 기술 A (Attitude) 태도 이걸 AI와 대입해보면 다음과 같다. K (knowledge) 지식 : Open AI가 더 많은 지식을 가지고 있음 S (skill) 기술 : 실수하지 않는 로봇을 따라갈 수 없음 (Human Error) A (Attitude) 태도 : 어떤 모습으로 일 할 것인가?! 즉, ChatGPT가 관여할 수 없는 영역은 바로 3번, 태도. 태도는 다음과 같은 것들을 함께 살펴볼 수 있다. 그 일이 필요하다고 느끼는 것 그 일을 통해 만들고 싶은 가치를 생각하는 것 그 일에 대한 의문을 던지는 것 그 일에 대해 윤리적으로 판단하는 것 ","date":"2023-06-21","objectID":"/posts/202306211131/:0:0","tags":["Misc"],"title":"Webina 참여 회고 - 원티드 주최 'ChatGPT 시대에 구성원의 역량을 어떻게 육성할 것인가?'","uri":"/posts/202306211131/"},{"categories":["Misc"],"content":"회고 OpenAI는 우리의 질문에 대해 매우 훌륭한 답변을 주지만 안타깝게도 위에서 언급된 태도와 같은 맥락의 질문을 먼저 시작하여 던지지는 못한다. 설사 제안을 하더라도 그에 대한 판단과 선택을 제공하지 않는다. 애시당초 질문이라는 input에 대한 ‘인공지능 답변’이란 output 시스템이기에 어쩌면 당연한 걸지도 모른다. 그럼 우리는 어떤 질문을 해야할까? 어떠한 질문으로 OpenAI의 도움을 받을 수 있을까? 쉽게 던질 수 있는 질문은 누구나 할 수 있는 질문이고, 이는 굳이 ‘내’가 아니어도 되는 일이다. 무작정 어려운 질문을 하는 것이 능사란 말이 아니라, ‘질문다운 질문’ 을 던지는 것이 핵심이라는 뜻이다. 프로세스 도중 연관된 사안에서 파생되는 질문들도 있겠지만, 최초의 시작에서 나오는 질문에는 ‘창의성’을 함께 생각하지 않을 수 없다. 길포드(Guilford)가 정의한 창의성 이론에 따르면 창의성은 다음 몇가지의 개념들로 설명된다. 민감성(sensitivity) 일상적인 상황이나 사물을 자세히 관찰하며 작은 변화에도 호기심을 갖고 적극적으로 탐색하고 반응하는 특성 유창성(fluency) 제한된 시간 내 많은 아이디어를 빨리 생산해 내는 능력 융통성(flexibility) 고정된 사고 양식에서 벗어나 사물이나 상황을 여러 관점에서 조망할 수 있는 능력 독창성(originality) 아이디어의 양보다 질적인 측면에서 일반적으로 찾아 볼 수 없는 새롭고 독특한 생각이나 산물을 생산해 내는 능력 정교성(elaboration) 사고의 깊이에 대한 개념으로 기존의 생각이나 산물을 분석하고 확장하여 보다 세밀하고 구체화시키는 능력 재구성력(Reconfiguring Ability) 기존의 개념이나 아이디어를 재구성하거나 변형하여 새로운 관점이나 해결책을 찾는 능력으로 다양한 아이디어나 개념을 유연하게 재조합하는 능력 이 중 ChatGPT에서 단연 돋보이는 기능은 유창성이다. 일단 주제가 주어지고 방향이 잡히면 기존의 학습된 데이터를 기반으로 한 수많은 아이디어들을 엄청난 속도로 내뱉는다. 정교성 부분도 상황과 맥락을 정확히 짚어준다면 큰 도움을 받을 수 있으리라 생각된다. 하지만 그외 모든 요소에서는 위에서 말한 태도와 더불어 사람만이 할 수 있다. (뭐 적어도 아직까지는…) 1 즉, 단순 사고에서 좀 더 고차원적 사고로의 중심 이동이랄까?? 실제로 OpenAI를 사용하여 코딩을 해보면 비교적 단순 반복 유형의 작업 비중은 AI쪽으로 쉽게 돌릴 수 있지만, 고민과 판단이 필요한 시점에서는 결국 나 스스로 온전히 결정하게 되며, 작업에서도 후자의 시간으로 깊게 집중되는 경향을 볼 수 있었다. 살펴보고, 판단하고, 선택하고, 다시 돌이켜 살펴보고, 자료조사와 검증 및 분석을 하는데 이 중 OpenAI의 도움을 받는 곳은 ‘자료 검증 및 분석’에서의 일부일 뿐이었다. 개발을 하면서 ChatGPT를 처음 접했을 때, 영향권의 직무로서 마주했던 두려움에 비해 현재는 아주 좋은 도구로써 사용하고 있다. 오히려 OpenAI는 더욱 잘 활용할 수 있도록 숙지해야할 기술중 하나라는 표현이 더 맞지 않을까 싶기도 하다. 지금까지 ChatGPT를 사용해본 결과 학습된 데이터가 있다면 융통성 역시 어느정도는 모방하지만 역시 사람만 못하다는 느낌은 지울수가 없다. 아무래도 기존에 학습된 데이터를 기반으로 해서인듯 싶다. ↩︎ ","date":"2023-06-21","objectID":"/posts/202306211131/:0:1","tags":["Misc"],"title":"Webina 참여 회고 - 원티드 주최 'ChatGPT 시대에 구성원의 역량을 어떻게 육성할 것인가?'","uri":"/posts/202306211131/"},{"categories":["Dev"],"content":"AWS EC2에서 테스트 프로젝트 운용시 배포용 서버 외에 추가서버 및 추가비용 없이 '개발용 서버'를 상시 여는 방법 기술","date":"2023-06-19","objectID":"/posts/202306191659/","tags":["AWS","project_review","Back-End"],"title":"AWS EC2에서 product 서버와 개발용 서버 같이 사용하기","uri":"/posts/202306191659/"},{"categories":["Dev"],"content":" 프로젝트를 진행하다보면 필연적으로 프론트엔드와의 통신교류가 필요하고, 이 때마다 local server로 열기보단 상시 열려있는 server가 있는 것이 훨씬 편하다. 때문에 이러한 개발과정상 필요한 back-end의 통신교류용 API 서버를 AWS EC2에서 간편하게 열어 사용하는 방법에 대해 기술하려 한다. ","date":"2023-06-19","objectID":"/posts/202306191659/:0:0","tags":["AWS","project_review","Back-End"],"title":"AWS EC2에서 product 서버와 개발용 서버 같이 사용하기","uri":"/posts/202306191659/"},{"categories":["Dev"],"content":"초기 처음 통신교류를 위한 서버는 VScode의 Live Share Extension을 이용하였다. 방법은 다음과 같다. extensions에서 Live Share 설치 좌측 사이드 메뉴에서 Live Share 아이콘 클릭 Share 버튼을 클릭하면 실시간 공유가 시작된다. 여기서 Shared Servers를 클릭하고 상단 검색창에 포트 번호를 입력후, enter를 누르면 해당 포트로 서버가 공유된다. ","date":"2023-06-19","objectID":"/posts/202306191659/:0:1","tags":["AWS","project_review","Back-End"],"title":"AWS EC2에서 product 서버와 개발용 서버 같이 사용하기","uri":"/posts/202306191659/"},{"categories":["Dev"],"content":"과도기 VScode의 Live Share를 이용한 통신 교류 확인시, 몇가지 단점이 있었다. 백엔드는 자신의 local 컴퓨터에서 항상 이 서버를 켜놔야한다. 해당 서버가 공유되고 있는 동안에 코드를 수정하기가 난감하다. 코드 수정 중 백엔드의 express가 중단될 때, 프론트에서는 아무 통보없이 서버가 멈추는 상황을 맞이하게 된다. 그때마다 실시간으로 프론트에게 상황을 전달해줘야 한다. 때문에 상시 가동가능한 AWS의 EC2를 이용하여 개발용 서버를 하나 열어주는 방법을 선택하였다. 예를 들어, version 1.0.0의 product 서버가 8000 port로 배포중이라면, version 1.0.1의 dev 서버를 8001 port로 또 달리 배포를 하는 것이었다. 이 때 EC2 운용은 해당 프로젝트의 동일한 코드를 또다시 git clone하여 서버를 여는 방법이었다. 즉, test-project라는 프로젝트의 경우 해당 EC2에는 test-project test-project-dev 이렇게 2개의 동일한 폴더가 설치되고, product 배포용 main 브랜치와 통신교류용이자 개발용 dev 브랜치에서의 코드로 각각 서버를 돌리는 것이다. 당연히 몇가지 문제가 있었다. EC2의 프리티어용 인스턴스를 사용해서인지 무거운 프로젝트의 경우 두번째 서버를 돌릴때, 일단 npm install에서부터 굉장히 오랜 시간이 걸린다. 굳이 똑같은 코드인데 EC2에서 복수로 용량을 차지한다. ","date":"2023-06-19","objectID":"/posts/202306191659/:0:2","tags":["AWS","project_review","Back-End"],"title":"AWS EC2에서 product 서버와 개발용 서버 같이 사용하기","uri":"/posts/202306191659/"},{"categories":["Dev"],"content":"현재 몇 번의 테스트 결과 현재의 방법은 과도기의 방법과 크게 다르지 않지만 일단 과도기 방법의 단점은 모두 해결하였다. 간단하게, 하나의 프로젝트 폴더에서 git switch로 브랜치 전환 후, npm distribute 명령을 dev용으로 하나 더 추가 실행하는 것이다. \"scripts\": { \"distribute\": \"npm i --verbose \u0026\u0026 pm2 start dist/main.js --name \u003c배포용 서버명\u003e -i max\", \"distribute-dev\": \"npm i --verbose \u0026\u0026 pm2 start dist/main.js --name \u003c개발용 서버명\u003e -i max\", } 배포용 서버를 가동할 땐, main 브랜치로 이동 npm run distribute 명령어 실행 개발용 서버를 가동할 땐, dev 브랜치로 이동 npm run distribute-dev 명령어 실행 이후 코드 업데이트로 인한 서버 reload시에도 간단하게 git branch를 이동한 후, 개발용인지 아닌지 서버명만 정확하게 입력하여 reload 시켜주면 된다. 이런 식으로 명령어를 따로이 할 때 주의 사항은, scripts 명령어와, 명령어의 내용중 서버명 위 두가지 항목을 반드시 다르게 지정해야 한다. 자칫 기존 배포용 서버에 개발용 코드가 덮어씌기 될 수도 있기 때문이다. 현재의 이 방법은 무거운 npm install 과정을 main과 dev 모두 한번으로 처리할 수 있어서 AWS의 프리티어 인스턴스에서도 생각보다 가볍게(?) 잘 돌아간다. ","date":"2023-06-19","objectID":"/posts/202306191659/:0:3","tags":["AWS","project_review","Back-End"],"title":"AWS EC2에서 product 서버와 개발용 서버 같이 사용하기","uri":"/posts/202306191659/"},{"categories":["Dev"],"content":"AWS RDS의 데이터를 '다른 계정'의 AWS RDS로 이동 또는 복사하는 방법","date":"2023-06-19","objectID":"/posts/202306191507/","tags":["AWS","project_review","Back-End"],"title":"AWS RDS 데이터 - 다른 계정으로 이전하기","uri":"/posts/202306191507/"},{"categories":["Dev"],"content":" 서로 다른 AWS의 계정간 RDS의 데이터를 이전하는 방법에 대해 소개한다. ","date":"2023-06-19","objectID":"/posts/202306191507/:0:0","tags":["AWS","project_review","Back-End"],"title":"AWS RDS 데이터 - 다른 계정으로 이전하기","uri":"/posts/202306191507/"},{"categories":["Dev"],"content":"AWS RDS 계정간 이동 우선 간략히 설명하자면 순서는 다음과 같다. Info 구 계정에서 스냅샷 생성 생성이 완전히 완료되면 스냅샷 공유 탭이 활성화된다. 스냅샷 공유에서 private 선택후, 계정란에 이전할 새로운 계정의 ID 입력 이전할 새 계정으로 로그인 후, RDS에서 좌측 사이드바의 스냅샷 메뉴 - 상단 나와 공유된 스냅샷 탭 클릭 마이그레이션 선택! (RDS 세팅시 프리티어라고 따로 설정하는 안내가 없으니 잘 보고 세팅) 끝! RDS 이전은 이렇게 쉬워도 되나?! 싶을 정도로 몹시 간단하다. ","date":"2023-06-19","objectID":"/posts/202306191507/:1:0","tags":["AWS","project_review","Back-End"],"title":"AWS RDS 데이터 - 다른 계정으로 이전하기","uri":"/posts/202306191507/"},{"categories":["Dev"],"content":"1. 스냅샷 생성 우선 원래의 데이터가 있는 계정으로 로그인 하고, RDS 서비스를 불러온다. 그리고 이전하고자 하는 데이터베이스를 선택한 후, 위 이미지와 같이 스냅샷 생성 버튼을 클릭한다. ","date":"2023-06-19","objectID":"/posts/202306191507/:1:1","tags":["AWS","project_review","Back-End"],"title":"AWS RDS 데이터 - 다른 계정으로 이전하기","uri":"/posts/202306191507/"},{"categories":["Dev"],"content":"2. 스냅샷 공유 다음은 좌측 사이드바의 스냅샷으로 들어가 정상적으로 스냅샷이 생성되었는지 확인하고, 스냅샷 공유 버튼을 누른다. 이 때 주의할 점은, 스냅샷이 완전히 생성될때까지는 스냅샷 공유 버튼은 비활성화 되어있다. 데이터베이스의 크기에 따라 스냅샷이 생성되기까지 몇 분 정도 시간이 걸리기에 완전히 생성이 완료되면 공유 버튼은 활성화 된다. 스냅샷 공유 버튼을 누르고 권한 설정을 해준다. 특별한 상황이 아니라면 보통 다음과 같이 설정해주면 된다. Private 와 함께 이 데이터베이스를 이전할 새로운 계정의 ID를 입력 후, 추가 버튼을 누른다. (참고로 계정 ID는 숫자로 되어있다. ) ","date":"2023-06-19","objectID":"/posts/202306191507/:1:2","tags":["AWS","project_review","Back-End"],"title":"AWS RDS 데이터 - 다른 계정으로 이전하기","uri":"/posts/202306191507/"},{"categories":["Dev"],"content":"3. 스냅샷 마이그레이션 이전할 새 계정으로 로그인 후, RDS에서 좌측 사이드바의 스냅샷 메뉴 - 상단 나와 공유된 스냅샷 탭 클릭 이제 방금 공유했던 스냅샷이 잘 들어와있음을 확인할 수 있다. 해당 스냅샷을 체크하고 마이그레이션 선택 마이그레이션 버튼을 누르고 난 이후의 진행과정은 일반적인 AWS - 데이터베이스 생성 과정과 동일하다. 다만, 템플릿 선택창과 인스턴스 구성시 프리티어 라는 안내가 따로 나타나지 않을 수 있으니, 혹여 프리티어로 구성한다면 잘 보고 선택해야 한다. Note 주의할 사항은, 스냅샷으로 데이터베이스를 이전할 때 원래 데이터베이스의 데이터베이스 이름과 사용자명, 패스워드까지 그대로 마이그레이션 된다. 따라서 원래의 데이터베이스에 패스워드가 있다면 따로이 기록해둬야 한다. ","date":"2023-06-19","objectID":"/posts/202306191507/:1:3","tags":["AWS","project_review","Back-End"],"title":"AWS RDS 데이터 - 다른 계정으로 이전하기","uri":"/posts/202306191507/"},{"categories":["Dev"],"content":"git commit message 작성시 일관된 규칙의 제목줄을 통해 가시성을 높일 수 있는 방법 고찰 중 컨벤션 설정","date":"2023-06-17","objectID":"/posts/202306171531/","tags":["git","project_review"],"title":"Git commit message 컨벤션 설정","uri":"/posts/202306171531/"},{"categories":["Dev"],"content":" ","date":"2023-06-17","objectID":"/posts/202306171531/:0:0","tags":["git","project_review"],"title":"Git commit message 컨벤션 설정","uri":"/posts/202306171531/"},{"categories":["Dev"],"content":"문제 발견 git commit message 작성시 보통 첫줄에 위치한 제목줄에 들어가는 가이드로서, 이전에 내가 사용하던 규칙은 다음과 같았다. Add - 레이아웃 / 기능 추가 Remove - 내용 삭제 (폴더 / 파일 삭제) Modify - 수정 (JSON 데이터 포맷 변경 / 버튼 색깔 변경 / 폰트 변경) Fix - 버그/오류 해결 Refactor - 코드 리팩토링 (멘토 리뷰 반영 / 스스로 리팩토링 / 중복 코드 제거 / 불필요 코드 제거 / 성능 개선) 작은 프로젝트에서는 보통 위 5가지의 규칙만으로도 충분했었는데, 팀으로 개발을 하고 프로젝트의 크기가 커지면서, 위 규칙만으로는 부족함을 느끼게 되었다. ","date":"2023-06-17","objectID":"/posts/202306171531/:1:0","tags":["git","project_review"],"title":"Git commit message 컨벤션 설정","uri":"/posts/202306171531/"},{"categories":["Dev"],"content":"정보 인용 그러다 며칠 전, Git 커밋 메시지 자동화 가이드 | DevSecOps 구축 컨설팅, 교육, 기술지원 서비스 제공 이란 아주 유용한 글을 보게 되었다. 그 중, git commit 메세지 작성시 제목줄의 컨벤션 부분을 정리하며 앞으로의 개인 개발시 사용해보고자 한다. ✨ Feat(페이지 경로 또는 컴포넌트): 새로운 기능 추가 또는 기능 업데이트 🔨 Fix(페이지 경로 또는 컴포넌트): 버그 또는 에러 수정 ⭐️ Style(페이지 경로 또는 컴포넌트): 코드 포맷팅, 코드 오타, 함수명 수정 등 스타일 수정 🧠 Refactor(페이지 경로 또는 컴포넌트): 코드 리팩토링(똑같은 기능인데 코드만 개선) 📁 File(페이지 경로 또는 컴포넌트): 파일 이동 또는 제거, 파일명 변경 🎨 Design(페이지 경로 또는 컴포넌트): 디자인, 문장 수정 🏷 Comment(페이지 경로 또는 컴포넌트): 주석 수정 및 삭제 🍎 Chore: 빌드 수정, 패키지 추가, 환경변수 설정 📝 Docs: 문서 수정, 블로그 포스트 추가 🔥 Hotfix: 핫픽스 수정 그리고 여기에 더해 따로이 추가할까 고민중인 메세지 🔧 Modify(페이지 경로 또는 컴포넌트): 기능 변경 또는 기능 업데이트 ","date":"2023-06-17","objectID":"/posts/202306171531/:2:0","tags":["git","project_review"],"title":"Git commit message 컨벤션 설정","uri":"/posts/202306171531/"},{"categories":["Dev"],"content":"적용전 고민 Modify라는 타이틀은 사실 위에서 정의한 Feat의 ‘기능 업데이트’ 라는 부분에서 성격이 겹친다. 이 부분을 고민하는 이유는 기능 업데이트 라는 부분의 제목 분류를 따로 할 것인가 Feat에 포함시킬 것인가라는 의문에서 나왔다. 실제로 지금까지 개발을 하며 git을 다루어본 경험으로, Pull Request에서는 기능 소개나 버그 수정이 주가 될 수 있지만, 일반적인 Commit message에서는 기능개선, 수정 등이 주가 되었다. 즉, Modify가 가장 많단 말이 된다. 이 때, 이 모든 것을 Feat으로 처리하면 실제로 커밋 메세지를 쭉 훑어나가면서 하나의 기능이 언제 처음 추가가 되었고, 어떻게 변경되어 나가는가를 살피기에 Modify라는 제목 분류가 훨씬 더 보기 편하였다. ","date":"2023-06-17","objectID":"/posts/202306171531/:3:0","tags":["git","project_review"],"title":"Git commit message 컨벤션 설정","uri":"/posts/202306171531/"},{"categories":["Dev"],"content":"정리 때문에 끝으로 결정하였던 나의 Commit Message 규칙은 다음과 같이 정리되었다. Note \u003c추가 계열\u003e ✨ Feat(페이지 경로 또는 컴포넌트): 새로운 기능 추가 🍎 Chore: 빌드 수정, 패키지 추가, 환경변수 설정 📝 Docs: ‘문서’ 추가 및 수정 🚨 Test (API 또는 함수): 테스트 코드 추가 및 업데이트 \u003c기능관련 수정 계열\u003e 🔧 Modify(페이지 경로 또는 컴포넌트): 기능 변경, 기능 업데이트 또는 기능 삭제 🎨 Design(페이지 경로 또는 컴포넌트): 디자인 수정 🔨 Fix(페이지 경로 또는 컴포넌트): 버그 또는 에러 수정 🔥 Hotfix: 핫픽스 수정 \u003c코드관련 수정 계열\u003e ⭐️ Style(페이지 경로 또는 컴포넌트): 코드 포맷팅, 코드 오타, 함수명 수정 등 스타일 수정 🧠 Refactor(페이지 경로 또는 컴포넌트): 코드 리팩토링(똑같은 기능인데 코드만 개선) \u003c기타\u003e 🏷 Comment(페이지 경로 또는 컴포넌트): 주석 수정 및 삭제 📁 File(페이지 경로 또는 컴포넌트): 파일 이동 또는 제거, 파일명 변경 정작 위 방식을 쓰려고 하면 매번 git commit message를 작성할 때마다 해당 컨벤션을 기억해내야하고 또 해당 이모지를 찾아서 입력을 해야하는 수고로움이 생길 수 있다. 물론 메모장 같은 곳에 따로 메모를 해놓고 그때그때 살펴보며 복사-붙여넣기를 할수도 있겠지만, Gitmoji라고 해서 링크의 글을 가보면 IntelliJ에 Plugins로 추가설치하여 사용할 수도 있다. 하지만 플러그인 방식은 다른 IDE를 사용하거나 일반적인 터미널에서는 사용하기가 애매해서 나는 이를 Alfred app의 Snippets 기능을 활용한 상용구 방식으로 입력할 수 있도록 세팅하였다. git commit은 한번에 뭉쳐서 하기보다 성격에 따라 세세하게 나눠 기록하는 것이 추후 유지보수에 더 좋다는 글을 여러 번 봤는데, 실제로 뭉쳐진 commit은 나중에 추적하기 여간 불편한게 아니었다. 될 수 있으면 간단하면서도 알아보기 쉬운 설명으로 메세지를 기록하고, 이 때 머릿글이 되는 제목줄의 위와 같은 컨벤션이 앞으로의 개발 과정에 더 큰 도움이 되리라 기대된다. ","date":"2023-06-17","objectID":"/posts/202306171531/:4:0","tags":["git","project_review"],"title":"Git commit message 컨벤션 설정","uri":"/posts/202306171531/"},{"categories":["Dev"],"content":"MySQL에서 회원탈퇴시 탈퇴한 회원 정보를 DB에서 완전히 삭제하지 않고 softDelete 처리했을 때, 해당 이메일을 중복확인에서 통과시키고 재사용할 수 있게한 고민들","date":"2023-06-07","objectID":"/posts/202306070208/","tags":["typeORM","mySQL","project_review","Back-End"],"title":"MySQL에서 이메일 재사용 가능하게 하기- Soft Delete와 Unique를 함께 활용하다.","uri":"/posts/202306070208/"},{"categories":["Dev"],"content":" ","date":"2023-06-07","objectID":"/posts/202306070208/:0:0","tags":["typeORM","mySQL","project_review","Back-End"],"title":"MySQL에서 이메일 재사용 가능하게 하기- Soft Delete와 Unique를 함께 활용하다.","uri":"/posts/202306070208/"},{"categories":["Dev"],"content":"개발환경 Javascript 런타임 플랫폼: Node.js 언어: TypeScript 프레임워크: Express DB: MySQL ORM: TypeORM ","date":"2023-06-07","objectID":"/posts/202306070208/:1:0","tags":["typeORM","mySQL","project_review","Back-End"],"title":"MySQL에서 이메일 재사용 가능하게 하기- Soft Delete와 Unique를 함께 활용하다.","uri":"/posts/202306070208/"},{"categories":["Dev"],"content":"현재 조건 상황 회원가입시, 이메일과 패스워드가 필수 입력입력인데, 이메일의 경우 mySQL에서 unique 처리 회원가입 과정에서 이메일 중복 여부를 확인하고 통과했을 경우에만 회원가입 가능 회원 삭제시, 실제 회원정보를 DB에서 지우지 않고 TypeORM의softDelete 방식으로 deleted_at 컬럼에 삭제일시가 기록되는 방식 (일정기간 이후 삭제할 요량으로 단기간 데이터 보존) Question 이미 회원 삭제된 정보의 이메일로 재가입하려는 경우, 해당 이메일은 이미 mySQL에서 unique로 입력되어있는 이메일이기 때문에 이메일 중복확인에서 통과되지 않는다. 해당 이메일을 재사용하려면? ","date":"2023-06-07","objectID":"/posts/202306070208/:2:0","tags":["typeORM","mySQL","project_review","Back-End"],"title":"MySQL에서 이메일 재사용 가능하게 하기- Soft Delete와 Unique를 함께 활용하다.","uri":"/posts/202306070208/"},{"categories":["Dev"],"content":"3가지 방안들 mySQL - entity - email column 에서 unique 조건을 제거하고, 중복여부를 필터링하는 코드를 추가하는 방법으로 진행한다. DB에서의 unique 조건을 유지시키기 위해 삭제 로직 중, email을 변경하여 저장한다. 삭제된 email로 회원가입이 들어올 경우, 복구 과정을 추가한다. ","date":"2023-06-07","objectID":"/posts/202306070208/:3:0","tags":["typeORM","mySQL","project_review","Back-End"],"title":"MySQL에서 이메일 재사용 가능하게 하기- Soft Delete와 Unique를 함께 활용하다.","uri":"/posts/202306070208/"},{"categories":["Dev"],"content":"1번의 경우 mySQL의 unique 제약 조건을 제거하면 이메일 중복 확인 로직을 코드에서 추가적으로 수정하여 처리해야 한다. 즉, deleted_at 컬럼이 null인지 아닌지에 대한 확인이 필요하기에 Users DB에서의 조건이 실행되어야 한다. 이 경우, 데이터베이스에서의 조회 로직에 Indexing이 들어간 Unique 컬럼 조회와 더불어 전체스캔을 필요로 하는 MySQL의 Is Null절을 추가적으로 사용해야하며 , 이로 인해 성능에 약간의 영향을 미칠 것으로 예상된다. 또한 개발 도중의 문제로 deleted_at 컬럼의 데이터에 문제가 생긴다면 걷잡을 수 없는 혼란을 야기할 수도 있다. 그러나 이 방법은 프로그래밍적으로 더 유연하게 사용할 수 있고, 사용자의 이메일을 변경하지 않아도 되기에 데이터의 원형을 유지하면서도 새 사용자가 이전에 사용되었던 이메일을 재사용할 수 있다는 이점이 있다. ","date":"2023-06-07","objectID":"/posts/202306070208/:3:1","tags":["typeORM","mySQL","project_review","Back-End"],"title":"MySQL에서 이메일 재사용 가능하게 하기- Soft Delete와 Unique를 함께 활용하다.","uri":"/posts/202306070208/"},{"categories":["Dev"],"content":"2번의 경우 예를 들어, 사용자가 삭제되면 해당 사용자의 이메일을 \"email@example.com.deleted.20210601\"와 같이 변경하여 mySQL에서의 unique 조건을 그대로 유지할 수 있다. 이 방법은 데이터베이스의 무결성을 유지하면서도 이메일 재사용 문제를 해결할 수 있다. 하지만 이 경우 사용자의 원래 이메일을 변경해야 하므로 데이터의 원형을 유지하는 데 어려움이 있을 수 있다. ","date":"2023-06-07","objectID":"/posts/202306070208/:3:2","tags":["typeORM","mySQL","project_review","Back-End"],"title":"MySQL에서 이메일 재사용 가능하게 하기- Soft Delete와 Unique를 함께 활용하다.","uri":"/posts/202306070208/"},{"categories":["Dev"],"content":"3번의 경우 복구과정을 추가할 시, 애초에 이메일을 오타로 입력하여 등록이 되어버리면 이미 이 이메일은 이전 회원의 소유로 귀속되기에 사실상 원래 이메일의 소유자가 회원가입을 하려고 할 때 불가능해진다. 따라서 이 방법은 패스! ","date":"2023-06-07","objectID":"/posts/202306070208/:3:3","tags":["typeORM","mySQL","project_review","Back-End"],"title":"MySQL에서 이메일 재사용 가능하게 하기- Soft Delete와 Unique를 함께 활용하다.","uri":"/posts/202306070208/"},{"categories":["Dev"],"content":"고려사항 1번과 2번의 방안을 두고 저울질을 해봤을 때, 1번 : unique 조건을 해제하고 코드를 추가하여 개발 유연성을 확보하면 상시적으로 시스템의 성능에 영향이 간다. 2번 : unique 조건을 유지시키고, 회원 삭제시 데이터 원형을 변형시키는 방법을 하였을 때에는 추후 복구시 다시 데이터 원형을 살릴 수 있는 코드가 추가되어야 한다. 즉, 추가코드는 필수적이지만 상시적으로 해당 코드를 운용할 것인가? 간헐적인 사용시 해당 코드를 운용할 것인가의 문제로 귀결되면서 2번의 방법을 사용하기로 결정하였다. 그리고 롤백시 데이터 원형을 살릴 수 있으면서 알아보기 쉽도록 deleted.현재시간 이라는 규칙을 가진 string을 추가하여 변형된 email 주소를 업데이트 하기로 했다. 또한 이 방법은 추후 deleted_at 컬럼의 내용에 대한 백업 역할을 기대해볼 수도 있다. 이렇게 하면 추후에 .deleted 이후의 문자를 모두 날려버리는 간단한 아래 코드만으로 복구가 가능해진다. const recoveredEmail = email.replace(/\\.deleted\\.\\d+/, ''); 이리하여 변경된 프로젝트의 코드는 다음과 같다. ","date":"2023-06-07","objectID":"/posts/202306070208/:4:0","tags":["typeORM","mySQL","project_review","Back-End"],"title":"MySQL에서 이메일 재사용 가능하게 하기- Soft Delete와 Unique를 함께 활용하다.","uri":"/posts/202306070208/"},{"categories":["Dev"],"content":"변경 전 // user.service.ts의 userDelete 함수 중, // ... 이전 코드 // 사용자 정보의 유효성 검사 함수를 불러온다. await findUserInfoByUserId(userId); // transaction을 시작한다. const queryRunner = dataSource.createQueryRunner(); await queryRunner.connect(); await queryRunner.startTransaction(); try { // 사용자의 User entity를 삭제한다. await queryRunner.manager.softDelete(User, userId); // ... 이후 코드 ","date":"2023-06-07","objectID":"/posts/202306070208/:4:1","tags":["typeORM","mySQL","project_review","Back-End"],"title":"MySQL에서 이메일 재사용 가능하게 하기- Soft Delete와 Unique를 함께 활용하다.","uri":"/posts/202306070208/"},{"categories":["Dev"],"content":"변경 후 // user.service.ts의 userDelete 함수 중, // ... 이전 코드 // 사용자 정보의 유효성 검사 함수를 불러온다. const userInfo = await findUserInfoByUserId(userId); // transaction을 시작한다. const queryRunner = dataSource.createQueryRunner(); await queryRunner.connect(); await queryRunner.startTransaction(); try { // 사용자의 email을 변경한다. 추후 해당 email의 재사용을 위한 고민중 230607 추가 const email = `${userInfo.email}.deleted.${Date.now()}`; // 객체 리터럴 단축구문으로 email의 변경내용을 간략하게 표현한다. 230607 추가 await queryRunner.manager.update(User, userId, { email }); // 사용자의 User entity를 삭제한다. await queryRunner.manager.softDelete(User, userId); // ... 이후 코드 변경 후, 실행결과 출력 ","date":"2023-06-07","objectID":"/posts/202306070208/:4:2","tags":["typeORM","mySQL","project_review","Back-End"],"title":"MySQL에서 이메일 재사용 가능하게 하기- Soft Delete와 Unique를 함께 활용하다.","uri":"/posts/202306070208/"},{"categories":["Dev"],"content":"typeORM migration 설정간 dotenv.config 설정 문제를 기술","date":"2023-04-01","objectID":"/posts/202304010529/","tags":["typeORM","project_review","Debugging"],"title":"error log - TypeORM migration 할 때 cross ENV 설정 문제","uri":"/posts/202304010529/"},{"categories":["Dev"],"content":" ","date":"2023-04-01","objectID":"/posts/202304010529/:0:0","tags":["typeORM","project_review","Debugging"],"title":"error log - TypeORM migration 할 때 cross ENV 설정 문제","uri":"/posts/202304010529/"},{"categories":["Dev"],"content":"TypeORM migration ","date":"2023-04-01","objectID":"/posts/202304010529/:1:0","tags":["typeORM","project_review","Debugging"],"title":"error log - TypeORM migration 할 때 cross ENV 설정 문제","uri":"/posts/202304010529/"},{"categories":["Dev"],"content":"TypeORM migration:create typeorm migration을 시도해보면서 준비한건 몇가지 되지 않았다. 우선, 터미널에서 typeorm migration 파일을 생성하는 명령어. $ npx typeorm migration:create \u003cpath/file_name\u003e 예를 들면 npx typeorm migration:create ./src/migrations/default-tables 이런 식으로 사용할 수 있다. 그리고 마이그레이션 실행시 마이그레이션 파일들의 위치를 알려줄 수 있도록 datasource config 부분에 다음과 같이 설정을 추가한다. import { DataSource } from 'typeorm'; const dataSource = new DataSource({ type: process.env.TYPEORM_CONNECTION, host: process.env.TYPEORM_HOST, port: process.env.TYPEORM_PORT, username: process.env.TYPEORM_USERNAME, password: process.env.TYPEORM_PASSWORD, database: process.env.TYPEORM_DATABASE, entities: [__dirname + '/../**/*.entity.{js,ts}'], migrations: [__dirname + '/../migrations/*.{js,ts}'], // \u003c- 추가한 부분 migrationsTableName: 'migrations', // \u003c- 추가한 부분 }); export default dataSource; ","date":"2023-04-01","objectID":"/posts/202304010529/:1:1","tags":["typeORM","project_review","Debugging"],"title":"error log - TypeORM migration 할 때 cross ENV 설정 문제","uri":"/posts/202304010529/"},{"categories":["Dev"],"content":"TypeORM migration:run 그리고 마이그레이션을 실행하는 명령어는 2가지 정도 사용할 수 있다. npx typeorm-ts-node-commonjs migration:run -d ./src/repositories/data-source.ts ts-node ./node_modules/typeorm/cli.js migration:run -d ./src/repositories/data-source.ts 현재 typeORM 버전인 0.3.10에서는 두 가지 명령어 모두 다 잘 된다. 그리고 cross Env 설정으로 NODE_ENV를 세팅하려고 위 두 가지 명령어 중 제일 앞에 추가해주면 된다. (두 명령어 모두 가능) $ NODE_ENV=develop ts-node ./node_modules/typeorm/cli.js migration:run -d ./src/repositories/data-source.ts 그런데… Bug 위 명령으로 datasource 인스턴스가 있는 파일을 불러와야 하는데, NODE_ENV 종류별로 나눈 dotenv 설정을 어떻게 해도 가져오질 못한다. dotenv 값이 텅텅 빈채 data-source.ts파일이 실행되었다. 즉, TypeORM 설정이 전혀 먹히질 않아 dataSource 실행이 되지 않는다. error.log를 살펴보자니 아무래도 migration 실행시 ts-node가 돌긴 하지만 정작 작동은 명령어에 지정된 단일파일로만 움직이는 듯 하다. 때문에 app.ts 또는 main.ts에서 import 하고 있는 dotenv.config() 를 가져오지 못하는 문제! 여러가지 방법을 시도해보다가 dotenv 설정 뭉치(?)를 옮겨보았다. (사실 지저분해 보여서 이 방법은 정말 하고 싶지 않았다.) 보통 dotenv는 프로젝트의 진입점인 app.ts, index.ts, main.ts 등에서 불러오는게 일반적이지만, console.log로 테스트 해본 결과, 현재의 프로젝트에서는 data-source 파일이 가장 먼저 열렸다. 추가적으로 몇몇 테스트를 더 해보았지만 별다른 문제는 보이지 않았다. // data-source.ts import { DataSource } from 'typeorm'; // ↓ 여기서부터 ---------------------------------- import { config } from 'dotenv'; if (process.env.NODE_ENV === 'production') { config({ path: './env/.env.production' }); } else if (process.env.NODE_ENV === 'develop') { config({ path: './env/.env.dev' }); console.log('process.env.NODE_ENV is ', process.env.NODE_ENV); } else if (process.env.NODE_ENV === 'test') { config({ path: './env/.env.test' }); } else { throw new Error('process.env.NODE_ENV IS_NOT_SET!!'); } // ↑ 여기까지가 원래 main.ts 상단에 있던 코드 -------- const dataSource = new DataSource({ type: process.env.TYPEORM_CONNECTION, host: process.env.TYPEORM_HOST, port: process.env.TYPEORM_PORT, username: process.env.TYPEORM_USERNAME, password: process.env.TYPEORM_PASSWORD, database: process.env.TYPEORM_DATABASE, entities: [__dirname + '/../**/*.entity.{js,ts}'], migrations: [__dirname + '/../migrations/*.{js,ts}'], migrationsTableName: 'migrations', }); export default dataSource; test code도 실행시켜보고 다른 몇가지 테스트들을 더 해봤지만 딱히 문제는 나타나지 않고 잘 해결되었다. 자 그럼 실제로 마이그레이션 할 때, 위 명령어는 너무 기니까, script로 정리해두자. \"scripts\": { \"build\": \"npx tsc\", \"start\": \"NODE_ENV=develop node dist/main.js\", // ... \"typeorm\": \"NODE_ENV=develop ts-node ./node_modules/typeorm/cli.js migration:run -d ./src/repositories/data-source.ts\" }, ","date":"2023-04-01","objectID":"/posts/202304010529/:1:2","tags":["typeORM","project_review","Debugging"],"title":"error log - TypeORM migration 할 때 cross ENV 설정 문제","uri":"/posts/202304010529/"},{"categories":["Dev"],"content":"TypeORM migration:generate generate는 설명을 보고서는 과연 실제 product 배포 이후의 단계에서 쓸일이 있을까 싶은데 여튼 개발단계에서는 알아두면 이래저래 요긴하게 쓸 수도 있을 것 같다. NODE_ENV=develop npx typeorm-ts-node-commonjs migration:generate ./src/migrations/default-tables -d ./src/repositories/data-source.ts create와는 달리, 기존에 있는 migration파일과 현재의 entity의 차이를 비교하여 수정해주는데, 정확히 수정이라기보다 기존 데이터(컬럼)을 drop하고, 명령어에 설정한 파일명으로 새로 만들어 세팅을 한다. 실제 적용해보면 query 부분에 선명한 drop 이 눈에 들어온다. 이미 많은 문서에서도 언급되고 있지만 혹시라도 product로서 운용중인 서비스에서는 주의를 요한다. ","date":"2023-04-01","objectID":"/posts/202304010529/:1:3","tags":["typeORM","project_review","Debugging"],"title":"error log - TypeORM migration 할 때 cross ENV 설정 문제","uri":"/posts/202304010529/"},{"categories":["Dev"],"content":"jest에서 mock 함수를 초기화하는 2가지 메소드에 대한 기술","date":"2023-03-31","objectID":"/posts/202303311059/","tags":["Jest"],"title":"Jest에서 jest.clearAllMocks()와 jest.resetAllMocks() 차이점","uri":"/posts/202303311059/"},{"categories":["Dev"],"content":" ","date":"2023-03-31","objectID":"/posts/202303311059/:0:0","tags":["Jest"],"title":"Jest에서 jest.clearAllMocks()와 jest.resetAllMocks() 차이점","uri":"/posts/202303311059/"},{"categories":["Dev"],"content":"Introduction Jest는 페이스북에서 개발한 JavaScript 테스트 프레임워크로, 간편한 사용법과 빠른 실행 속도로 인기가 있다. Jest를 이용하여 테스트 코드를 작성하던 중, 수 많은 함수나 루트를 mocking하게 되는데 지저분해진 테스트 코드를 리팩토링 하다가 mock함수의 초기화에 대한 고찰을 하게 되었다. Jest의 모의 함수(mock functions)를 사용할 때 중요한 두 가지 함수인 jest.clearAllMocks()와 jest.resetAllMocks()의 차이에 대해 기술한다. ","date":"2023-03-31","objectID":"/posts/202303311059/:1:0","tags":["Jest"],"title":"Jest에서 jest.clearAllMocks()와 jest.resetAllMocks() 차이점","uri":"/posts/202303311059/"},{"categories":["Dev"],"content":"jest.clearAllMocks() jest.clearAllMocks() 함수는 모든 모의 함수의 호출 정보를 초기화하는 데 사용한다. 이를 통해 테스트 간 독립성을 보장하며, 다양한 테스트 시나리오에서 중요한 역할을 한다. 다음은 해당 메소드를 테스트해볼 수 있는 예시 코드이다. describe('jest.clearAllMocks test', () =\u003e { const mockFn = jest.fn().mockImplementation(() =\u003e 'Default return value'); mockFn.mockImplementation(() =\u003e 'New return value'); mockFn('args1'); // 1st call mockFn('args2'); // 2nd call test('before jest.clearAllMocks', () =\u003e { console.log( 'clearAllMocks 전, mockFn 호출 횟수 = ', mockFn.mock.calls.length ); // 2 console.log('clearAllMocks 전, mockFn 호출 인자 = ', mockFn.mock.calls); // [ [ 'args1' ], [ 'args2' ] ] expect(mockFn.mock.calls.length).toBe(2); expect(mockFn()).toBe('New return value'); }); test('after jest.clearAllMocks', () =\u003e { jest.clearAllMocks(); console.log( 'clearAllMocks 후, mockFn 호출 횟수 = ', mockFn.mock.calls.length ); // 0 console.log('mockFn 호출 인자 = ', mockFn.mock.calls); // [] expect(mockFn.mock.calls.length).toBe(0); expect(mockFn()).toBe('New return value'); }); }) 위 코드를 풀어쓰자면 다음과 같다. jest.clearAllMocks() 실행 이후, 이전에 mock함수를 2번 호출한 횟수는 초기화 되었다. mock함수의 인스턴스 ‘args1’, ‘args2’ 역시 초기화 되었다. mock함수의 반환값은, 최종 반환값으로 설정된 ‘New return value’ 그대로 남아있으며 아무런 영향이 없다. 코드의 설명과 마찬가지로, clearAllMocks는 mock함수의 호출 횟수, 인스턴스 등을 초기화 한다. 하지만 mock함수의 반환값은 초기화하지 않는다. 주요 사용 시나리오: 테스트 전/후 초기화 단계에서 호출 정보를 초기화하고 싶을 때 ","date":"2023-03-31","objectID":"/posts/202303311059/:2:0","tags":["Jest"],"title":"Jest에서 jest.clearAllMocks()와 jest.resetAllMocks() 차이점","uri":"/posts/202303311059/"},{"categories":["Dev"],"content":"jest.resetAllMocks() jest.resetAllMocks() 함수는 모든 모의 함수의 호출 정보뿐만 아니라 구현 및 반환 값 설정도 초기화하는 데 사용된다. 이를 통해 각 테스트가 완전히 독립적으로 실행되도록 보장하며, 다양한 테스트 시나리오에서 유용하게 사용할 수 있다. describe('jest.resetAllMocks test', () =\u003e { const mockFn2 = jest.fn().mockImplementation(() =\u003e 'Default return value'); mockFn2.mockImplementation(() =\u003e 'New return value'); mockFn2('args1'); // 1st call mockFn2('args2'); // 2nd call test('before jest.resetAllMocks', () =\u003e { console.log( 'resetAllMocks 전, mockFn 호출 횟수 = ', mockFn2.mock.calls.length ); // 2 console.log('resetAllMocks 전, mockFn 호출 인자 = ', mockFn2.mock.calls); // [ [ 'args1' ], [ 'args2' ] ] expect(mockFn2.mock.calls.length).toBe(2); expect(mockFn2()).toBe('New return value'); }); test('after jest.resetAllMocks', () =\u003e { jest.resetAllMocks(); console.log( 'resetAllMocks 후, mockFn 호출 횟수 = ', mockFn2.mock.calls.length ); // 0 console.log('mockFn 호출 인자 = ', mockFn2.mock.calls); // [] expect(mockFn2.mock.calls.length).toBe(0); expect(mockFn2()).toBe(undefined); }); }); 위 코드를 풀어쓰자면 다음과 같다. jest.resetAllMocks() 실행 이후, 이전에 mock함수를 2번 호출한 횟수는 초기화 되었다. mock함수의 인스턴스 ‘args1’, ‘args2’ 역시 초기화 되었다. mock함수의 반환값은 초기화되어 undefined가 되었다. resetAllMocks는 clearAllMocks의 모든 기능에 더해서 반환값 역시 초기화 시킨다. 즉, 완벽한 새로운 mock함수로 되돌린다. 주요 사용 시나리오: 테스트 전/후 초기화 단계에서 호출 정보와 구현, 반환 값 설정을 모두 초기화하고 싶을 때 모의 함수의 구현을 변경하는 테스트 사이에서 각 테스트의 독립성을 보장하고 싶을 때 Differences between jest.clearAllMocks() and jest.resetAllMocks() jest.clearAllMocks() : 모든 모의 함수의 호출 정보 초기화 jest.resetAllMocks(): 모든 모의 함수의 호출 정보뿐만 아니라 구현 및 반환 값 설정도 초기화 ","date":"2023-03-31","objectID":"/posts/202303311059/:3:0","tags":["Jest"],"title":"Jest에서 jest.clearAllMocks()와 jest.resetAllMocks() 차이점","uri":"/posts/202303311059/"},{"categories":["Dev"],"content":"macOS 터미널에서 AWS EC2 ssh 접속불가 문제를 해결한 설명","date":"2023-03-17","objectID":"/posts/202303170023/","tags":["AWS","Debugging"],"title":"error log - AWS EC2의 SSH 접속이 안될 때","uri":"/posts/202303170023/"},{"categories":["Dev"],"content":" Bug 갑자기 macOS terminal에서 AWS EC2에 ssh접속이 안된다. 접속해보면 즉각적인 반응이 아닌, 오랜 시간 후 다음과 같은 메세지를 뱉었다. $ ssh -i my_key.pem ec2-user@11.111.11.111 ssh: connect to host 11.111.11.111 port 22: Operation timed out Operation timed out 이라니… 찾아보니 여러 이유가 있을 수 있었다. 보안규칙에 22포트에 대한 설정이 누락되었거나, 명령어가 잘못 입력되었거나, 방화벽 설정에서 막혔다거나… 대개 구글링했을때 나오는건 Connetion timed out 이었는데 이건 너무 포괄적인 답변들이라 난감했다. 여튼 현재상황에서, 설정은 잘못된게 없었고 다른 ssh(github)로 테스트 해보니 모두 정상이었다. 심지어 기존 PM2 서버도 잘돌아가고 있었다. 그렇다면 분명 보안그룹과 관련되어있을 것 같아서 다시 살펴보기 시작했고, 이상한 주소 하나를 찾았다. 바로 인바운드 규칙에 보이는 IP주소 하나… 유형 : SSH 소스 : IP주소 아무리 봐도 일반 공인IP 주소 같았다. 아마도 EC2를 처음 세팅하면서 넣었거나 또는 간과했던 설정같은데 이전 내 IP 주소로 보였다. 역시나 문제가 된건 최근 인터넷 업체를 교체하면서 생긴 IP 주소 변화. Success 아래 이미지와 같이 EC2 - 보안그룹 - \u003c해당 인스턴스\u003e - 인바운드 규칙 편집 으로 가서 살펴보면 내 IP가 있는데 이를 선택하고 수정된 규칙을 저장한다. 그리고 다시 ssh 접속을 해보면 이제 정상적으로 접속됨을 볼 수 있었다. ","date":"2023-03-17","objectID":"/posts/202303170023/:0:0","tags":["AWS","Debugging"],"title":"error log - AWS EC2의 SSH 접속이 안될 때","uri":"/posts/202303170023/"},{"categories":["Dev"],"content":"코드에 NODE_ENV 설정이 되어있을 경우, EC2에서 PM2 실행시 NODE_ENV 설정 명령 처리에 대한 에러 노트","date":"2023-03-16","objectID":"/posts/202303162353/","tags":["AWS","JavaScript","project_review","Debugging"],"title":"error log - EC2에서 PM2 실행시 NODE_ENV 설정 명령어","uri":"/posts/202303162353/"},{"categories":["Dev"],"content":" AWS EC2에 배포를 하려하는데, 조그만 문제가 하나 생겼다. 아래 예시와 같이 .env파일을 개발, 배포 등의 목적으로 각각 나누어 코드를 작성했는데, main.ts 파일 코드 예시 if (process.env.NODE_ENV === 'production') { dotenv.config({ path: path.join(__dirname, '/../env/.env.production') }); } else if (process.env.NODE_ENV === 'develop') { dotenv.config({ path: path.join(__dirname, '/../env/.env.dev') }); console.log('process.env.NODE_ENV is ', process.env.NODE_ENV); } else if (process.env.NODE_ENV === 'test') { dotenv.config({ path: path.join(__dirname, '/../env/.env.test') }); } else { throw new Error('process.env.NODE_ENV IS_NOT_SET!!'); } 이걸 EC2에서 pm2로 프로세스를 돌릴 때 어떻게 명령을 내려야 하는가 였다. cross-env 설명을 보면 script의 가장 서두에 설정하고자 하는 값을 두면 된다고 한다. Bug \"scripts\": { \"distribute\": \"NODE_ENV=production npm i \u0026\u0026 pm2 start dist/main.js --name review -i max\" }, NODE_ENV=production 이 코드를 처음에는 명령어의 가장 서두에 두었다. ec2에서 pm2 list로 확인해보니 계속 에러가 나길래 cross-env NODE_ENV=production 이렇게도 바꿔보고 별 시도를 다 해봤다. \"scripts\": { \"distribute\": \"cross-env NODE_ENV=production npm i \u0026\u0026 pm2 start dist/main.js --name review -i max\" }, 끝끝내 찾아낸 건 아주 간단하고도 허무했다. Success NODE_ENV=production을 pm2 바로 앞에 두면 되는 것이었다. \"scripts\": { \"distribute\": \"npm i \u0026\u0026 NODE_ENV=production pm2 start dist/main.js --name review -i max\" }, \u0026\u0026는 명령을 순차적으로 하겠다는 것이며, npm과 pm2는 다른 성격임에 해당 명령을 따로 주어야 하는데 이런 기본을 간과하고 엉뚱한 곳에서 답을 찾고 있었다. ","date":"2023-03-16","objectID":"/posts/202303162353/:0:0","tags":["AWS","JavaScript","project_review","Debugging"],"title":"error log - EC2에서 PM2 실행시 NODE_ENV 설정 명령어","uri":"/posts/202303162353/"},{"categories":["Dev"],"content":"백엔드에서 프론트엔드로 API 명세를 전달함에 효과적인 Swagger 사용방법을 고찰하였다.","date":"2023-03-08","objectID":"/posts/202303080503/","tags":["RESTful_API","project_review","Back-End"],"title":"Swagger를 이용한 백엔드의 효과적인 API명세 전달","uri":"/posts/202303080503/"},{"categories":["Dev"],"content":" ","date":"2023-03-08","objectID":"/posts/202303080503/:0:0","tags":["RESTful_API","project_review","Back-End"],"title":"Swagger를 이용한 백엔드의 효과적인 API명세 전달","uri":"/posts/202303080503/"},{"categories":["Dev"],"content":"백엔드가 효과적인 명세 전달 고민 Failure 그동안 진행했던 프로젝트에서 제대로 된 API 명세를 적시에 프론트측에 전달하지 못했고, 때문에 최종 출시를 앞두고 서로간의 코드 오류를 잡느라 시간적 비용이 꽤나 낭비됐다. 간단하게는 Request Body 값의 TYPE과 관련된 소통 혼선으로 겪었던 헤프닝도 더러 있었다. 어떻게 하면 좋을까? 우선은 백엔드의 입장에서 생각해본다. 명세는 코드가 만들어지면서 늘 수정될 수 있다. 때문에 명세는 코드와 함께 작업되어야 하고, API 테스트를 위해 즉각적으로 수정되어야 한다. 같은 백엔드 팀의 경우에는 직접 코드를 보기에 명세의 부족함을 보완할 수는 있다. 반면 프론트 팀으로 넘어갈 때는 이야기가 다르다. 최대한 문서만으로 전달이 되었을 때, 그리고 전달이 가능할 때, 불필요한 소통이 사라진다. 지금까지는 그때그때 백엔드와 프론트엔드가 코딩을 하며 API를 맞춰나갈때마다 일대일로 물어보는 방식이었다. 불필요한 대화 시간과 에너지가 크게 소모되었으리라. 이런 비효율적인 방법이 아닌, 좀 더 생산적이고 효과적인 방법을 도모하고 싶었다. 그간 사용해왔던 API 툴은 Postman API Platform | Sign Up for Free이었다. 불편함을 느낀건 명세를 전달하기 위한 문서작성을 IDE나 텍스트편집기가 아닌 웹 브라우저나 Postman의 자체 클라이언트 앱에서 진행을 해야했다는 점이 가장 컸다.1 또한 문서가 완비되기 전의 상황에서도 프론트 측으로 공유를 하고 싶은데 이 부분에 있어 Postman은 유료 플랜, 즉 비용이 발생하였다.2 때문에 새로운 툴을 찾아보다 **Swagger**를 알게되었다. Postman과 Swagger의 차이 간단하게 기술하자면, open source 여부 이로 인한 무료 커버리지 및 커스터마이징 가능 여부 정도로 볼 수 있겠다. Swagger가 open source이다. 이전에는 Swagger로부터 비롯되었다하여 현재의 국제표준인 openAPI도 Swagger의 특징으로 볼 수 있는데, 현재 시점에서는 그다지 차이가 없는 듯 보인다. ","date":"2023-03-08","objectID":"/posts/202303080503/:1:0","tags":["RESTful_API","project_review","Back-End"],"title":"Swagger를 이용한 백엔드의 효과적인 API명세 전달","uri":"/posts/202303080503/"},{"categories":["Dev"],"content":"Postman vs Swagger 사용 후기 Postman 장점 초보자도 이해하기 쉬운 UI UX 편리한 API 테스트 잘 만들어진 desktop API Client app 단점 팀원간 명세 전달시 비용 발생 명세 작성시, 코드 개발자는 IDE 또는 편집기에서 벗어나 작업 진행 Swagger Swagger의 경우 장단점을 언급하기 전, 먼저 사용방법에 대한 간략한 설명을 한다. Swagger API 명세 사용법 SwaggerHub 이용 API 개수에 제한이 있지만 무료로도 가능하다. SwaggerUI 이용 http-server를 사용하여 열람 및 API 테스트 가능 SwaggerUI를 프로젝트 코드에 import하여 사용[^ NodeJS의 경우 보통 route단에서 주석+데코레이터 형태로 사용할 수 있다. 사용방법 SwaggerHub 이용 API 개수에 제한이 있지만 무료로도 가능하다. SwaggerUI 이용 http-server를 사용하여 열람 및 API 테스트 가능 SwaggerUI를 프로젝트 코드에 import하여 사용3 NodeJS의 경우 보통 route단에서 주석+데코레이터 형태로 사용할 수 있다. 장점 2, 3번을 사용할 경우 코드 개발자는 IDE 또는 편집기 내에서 API 명세를 그대로 작성할 수 있다. 또한 2, 3번의 경우 비용이 발생하지 않는다. 3번의 경우 git으로 관리가 가능하기에 팀원간 명세 전달시 편리하다. API 명세 문서임과 동시에 API 통신 TEST 까지 가능하다. 오픈소스이기에 명세 전달시 주어지는 UI 역시 수정이 가능하다. 팀의 성격이나 전달자의 입맛에 따라 바꿀 수 있다. 단점 어려운건 아니지만, 어쨌든 Postman에 비해 러닝커브가 있다. 일단 문법이나 코드형식을 익혀야 한다. ","date":"2023-03-08","objectID":"/posts/202303080503/:1:1","tags":["RESTful_API","project_review","Back-End"],"title":"Swagger를 이용한 백엔드의 효과적인 API명세 전달","uri":"/posts/202303080503/"},{"categories":["Dev"],"content":"Swagger 적용 이야기 ","date":"2023-03-08","objectID":"/posts/202303080503/:2:0","tags":["RESTful_API","project_review","Back-End"],"title":"Swagger를 이용한 백엔드의 효과적인 API명세 전달","uri":"/posts/202303080503/"},{"categories":["Dev"],"content":"과도기 문법도 맛볼겸 사용방법 3번을 우선 도입했다. 모든 준비와 테스트를 끝내고 막상 전달을 하려고 보니, 프론트엔드 팀이 이를 보려면 백엔드의 깃을 클론하고 DBMS를 비롯한 모든 세팅을 해야만 서버가 돌아가기에 정상작동을 한다. 몹시 불편하지 싶다. 1번을 사용해볼까는데, 링크전달이라… 4 비공개 프로젝트로 진행하게될 사안도 염두에 두었다. (물론 불가능한건 아니지만, 비용문제를 간과할 수 없었다.) 어떻게든 1번만은 피하고 싶었던지라 이번에는 다시 2번으로 확정짓고 세팅을 했다. 그런데 진행간에 문득 백엔드 팀원간의 명세 교류는 어떡하지?란 생각이 들었다. (이걸 간과한 이유는 현재 진행중인 프로젝트의 백엔드가 나 혼자이기 때문이었다.) 한참을 고민하다 2번과 3번의 하이브리드 방법을 모색해보았다. 만약 2번과 3번이 참조하는 swagger.yaml을 하나로 지정을 한다면? 간단한 테스트를 통과시키고 이 방법으로 진행하기로 했다. ","date":"2023-03-08","objectID":"/posts/202303080503/:2:1","tags":["RESTful_API","project_review","Back-End"],"title":"Swagger를 이용한 백엔드의 효과적인 API명세 전달","uri":"/posts/202303080503/"},{"categories":["Dev"],"content":"SwaggerUI + 프로젝트 코드 주입 방식 미리 언급하지만 2가지 방법을 혼용한 내용임으로, 처음 Swagger를 사용해보기 위해 이 글을 참조한다면 몹시 혼란스러울 수 있다. 따라서 참조된 다른 문서에서 하나씩 실행해보며 작동을 익힌 후 참고하길 권장한다. 별도의 폴더를 route단에 주석으로 넣는 방식은 아래 예시처럼 작성하게 되는데 정말 불편하다.5 // routes/users.js /** * @swagger * /users: * get: * summary: Retrieve a list of JSONPlaceholder users * description: Retrieve a list of users from JSONPlaceholder. Can be used to populate a list of fake users when prototyping or testing an API. */ router.get('/', function(req, res) { //... }); 이런식으로 데코레이터를 비롯한 주석처리로써 swagger-ui-jsdoc가 코드를 읽을 수 있게 처리하는건데, API별 명세는 한 두개가 아닐뿐더러 자세하게 작성하기 시작하면 수십줄은 훌쩍 넘어간다. 즉, 자칫 route파일의 가독성이 몹시 불편하게 될 수 있어 보였다. 물론 해당 주석부분을 collapse all 처리해도 되지만 그때그때 expand하고 다시 collapse 하는게 역시나 불편하긴 마찬가지다. 때문에, 프로젝트 코드의 swagger.js파일의 세팅을 별도의 폴더로 지정하고 SwaggerUI를 프론트단에서도 쓸 수 있도록 js파일이 아닌 yaml파일로 작성하기로 하고, 명세 종류가 많아짐에 따라 가독성 저하로 인한 파일 분기를 모색했다.6 ","date":"2023-03-08","objectID":"/posts/202303080503/:3:0","tags":["RESTful_API","project_review","Back-End"],"title":"Swagger를 이용한 백엔드의 효과적인 API명세 전달","uri":"/posts/202303080503/"},{"categories":["Dev"],"content":"설치 SwaggerUI 세팅 SwaggerUI는 세팅이 굉장히 쉽다. swagger-api/swagger-ui 코드를 클론하고 dist폴더만 남긴채 모두 지워준다.(나머지는 모두 참고문서) 현재시점인 Swagger UI v4.17.1의 dist폴더를 살펴보면 index.html에서, 내가 사용하고자 하는 URL을 지정하는 위치는 다음과 같다. // swagger-initializer.js window.onload = function () { //\u003ceditor-fold desc=\"Changeable Configuration Block\"\u003e // the following lines will be replaced by docker/configurator, when it runs in a docker-container window.ui = SwaggerUIBundle({ url: 'http://localhost:8080/index.yaml', // \u003c= 바로 이부분을 고쳐준다. dom_id: '#swagger-ui', deepLinking: true, presets: [SwaggerUIBundle.presets.apis, SwaggerUIStandalonePreset], plugins: [SwaggerUIBundle.plugins.DownloadUrl], layout: 'StandaloneLayout', }); //\u003c/editor-fold\u003e }; 저 부분이 앞으로 사용하게 될 SwaggerUI의 html파일이 가르키게 될 나의 Swagger 메인 문서 위치이다. 나는 dist폴더의 파일들을 적용할 프로젝트 폴더 안 api-docs 라는 폴더를 만들고 그 안에 배치하였다. 그리고 api-docs폴더의 tree는 다음과 같다. . ├── README.md \u003c- 이건 후술할 git submodule때문에 생성 ├── _build │ └── index.yaml ├── favicon-16x16.png ├── favicon-32x32.png ├── index.css ├── index.html ├── oauth2-redirect.html ├── swagger \u003c- 이 폴더안에 index.yaml을 만들어서 세팅한다. │ ├── index.yaml │ └── user \u003c- 내용이 방대함으로 성격에 따라 폴더와 파일들로 분기했다. │ ├── signIn.yaml │ └── signUp.yaml ├── swagger-initializer.js ├── swagger-ui-bundle.js ├── swagger-ui-bundle.js.map ├── swagger-ui-es-bundle-core.js ├── swagger-ui-es-bundle-core.js.map ├── swagger-ui-es-bundle.js ├── swagger-ui-es-bundle.js.map ├── swagger-ui-standalone-preset.js ├── swagger-ui-standalone-preset.js.map ├── swagger-ui.css ├── swagger-ui.css.map ├── swagger-ui.js └── swagger-ui.js.map tree 확인!! (click) 나는 project 내 api-docs라는 폴더를 만들고 그 안에 SwaggerUI의 dist 폴더안 파일들을 위치시켰다. 그리고 swagger라는 폴더를 따로이 생성한 뒤 그 안에 index.yaml파일을 세팅하는데 그 이유는 앞서 언급한 파일 분기 때문이다. 그리고 다음과 같이 index.yaml을 작성해준다. # api-docs/swagger/index.yaml openapi: 3.0.0 servers: - description: project_review localhost API document url: http://localhost:8000 info: version: \"1.0.0\" title: project_review-API description: The API for project_review contact: name: Inchan Song url: https://github.com/inchanS email: song@inchan.dev paths: /users/signup: $ref: \"./user/signUp.yaml\" #코드가 길어지면서 지저분해짐에 따라 분기하고 참조했다. /users/signin: $ref: \"./user/signIn.yaml\" swagger 명세 작성 index 내용중 경로와 사용처 확인!! api-docs/swagger/index.yaml 파일은 http-server에서 돌아가는 SwaggerUI가 사용하는 파일이다. 그리고 swagger.ts에 작성된 info는 node서버에서 돌아가는 SwaggerUI가 사용하는 파일이다. API 명세의 자세한 내용은 다음과 같은 형식으로 작성하였다. API 명세 yaml 파일 보기 (클릭) # api-docs/swagger/user/signin.yaml post: tags: - User summary: 회원 로그인 description: 회원 로그인 관련 API produces: - application/json requestBody: description: 회원 로그인시 필요한 사용자 정보 객체 required: true content: application/json: schema: $ref: '#/components/schemas/User' responses: 200: description: successful operation content: apllication/json: schema: $ref: '#/components/schemas/Success' 500: description: Internal server error content: apllication/json: schema: $ref: '#/components/schemas/Error' components: schemas: User: type: object properties: email: type: string description: The email of the user example: 'black@test.com' password: type: string description: The password of the user example: 'Abcd@1234' Success: type: object properties: message: type: string description: 로그인 성공 메세지 example: 'SIGNIN_SUCCESS' result: type: object description: 로그인 성공시 토큰값 properties: token: type: string description: 토큰값 example: \"token: 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6MTYsImlhdCI6MTY3ODEyOTY0OX0.eyNOFEZt5xPSakzy4FyGqub9heSZA5FtOZGtWyTI4vg'\" Error: type: object properties: message: type: string description: 로그인 실패시 메세지 example: '(THIS_EMAIL)_IS_NOT_FOUND' Tip 일반적인 $ref \u003cpath\u003e는 대부분 코드가 잘 작동하지만, paths의 값으로 `$ref 를 넣으면 swagger가 처리를 못하는 것 같다. 때문에 다음과 같은 추가 작업이 필요하다. \"scripts\": { \"test\": \"cross-env NODE_ENV=test jest --runInBand --detectOpenHandles --forceExit\", \"build\": \"npx tsc\", \"api-docs\": \"swagger-cli bundle api-docs/swagger/index.yaml --outfile api-docs/_build/index.yaml --type yaml\", \"predev\": \"npm run api-docs\", \"start\": \"cross-env NODE_ENV=production node dist/main.js\", \"dev\": \"cross-","date":"2023-03-08","objectID":"/posts/202303080503/:3:1","tags":["RESTful_API","project_review","Back-End"],"title":"Swagger를 이용한 백엔드의 효과적인 API명세 전달","uri":"/posts/202303080503/"},{"categories":["Dev"],"content":"git submodule 로 해결 api-docs 폴더를 그대로 git submodule로 새로 만든 레포지토리에 연결을 한다. 나는 “진행중인 프로젝트명-API\"라는 이름으로 생성하였고, 다음과 같이 진행하였다. # submodule 진행할 디렉토리로 이동후, $ git init $ git remote add origin \u003c새로만든 레포지토리\u003e # 그리고 project root폴더로 이동후, $ git submodule add -b main \u003c새로만든 레포지토리\u003e \u003csubmodule 진행할 디렉토리\u003e # 예시... $ git submodule add -b main https://github.com/inchanS/project-review-API-docs.git api-docs 그리고 # project root폴더의 위치에서 $ git submodule status # 명령하면 연결된 submodule의 내역이 나온다. 이제 코딩을 진행하며 백엔드 코드는 기존과 같이 커밋과 푸쉬를 자유롭게 진행하고, 프론트엔드 측으로 전달할만큼의 API 명세가 update 됐을 때에는, 따로이 submodule 폴더만 해당 레포지토리에 commit, push를 진행하면 된다. 그리고 실수로 올라간 push로 인해 불필요한 알림을 최소화 할 수 있도록, 버전 release 태그를 활용하여 여기에만 slack에 webhook 될 수 있도록 세팅을 한다. 그러면 다음과 같이 github의 release로 필터링된 멋진 알림이 가게 된다. ","date":"2023-03-08","objectID":"/posts/202303080503/:3:2","tags":["RESTful_API","project_review","Back-End"],"title":"Swagger를 이용한 백엔드의 효과적인 API명세 전달","uri":"/posts/202303080503/"},{"categories":["Dev"],"content":"아쉬운 점 http-server나 node서버 없이도 명세만 볼수 있게끔 완전한 HTML 파일만 추출 할 수 없나 고민중이다. 어찌됐든간에 html파일을 바로 열어볼 수 있는 것과, http-server 명령어를 입력하고 열어볼 수 있는건 사용자 경험에 있어 하늘과 땅차이니까 말이다. 현재 SwaggerUI의 코드가 워낙 복잡(?)해서 건드릴 엄두가 나질 않지만, 차후 시도해볼만하지 않나 싶다. 대량으로 작성하다보면 셀 이동이나 유사코드 붙여넣기와 같은 작업을 비롯하여 키보드에서 손을 떠나게 만드는게 스트레스였다. ↩︎ Postman API Platform | plans \u0026 pricing 이 글을 작성하는 현재시점에서 1인당 매월 $12라는 꽤나 높은 비용이다.(현재 환율 1,300원을 반영해보면 대략 월 15,600원 정도 나온다.) ↩︎ [협업] 협업을 위한 swagger 설정하기 (feat node.js) 참고  ↩︎ 간편하게 사용하거나 최종 배포 시점에 작성한다면 되려 더 편할지도 모르겠지만 프로젝트 과정간 계속해서 사용하기에는 불편해보였다. 또한 링크의 공개성과 소멸시효에 따른 우려때문에… ↩︎ 첨부코드 참고문서 How to Document an Express API with Swagger UI and JSDoc - DEV Community ↩︎ 참고문서 How to split a large OpenAPI document into multiple files - David Garcia ↩︎ 파일명은 아무거나 해도 상관없다. app.ts에서 import할때 이름만 같으면 되기에 아무거나.ts라고 해도 작동한다. ↩︎ 만약 main.ts에서 app을 분기시키지 않았다면 해당 파일에서 코드를 추가한다.  ↩︎ ","date":"2023-03-08","objectID":"/posts/202303080503/:3:3","tags":["RESTful_API","project_review","Back-End"],"title":"Swagger를 이용한 백엔드의 효과적인 API명세 전달","uri":"/posts/202303080503/"},{"categories":["Misc"],"content":"github hugo 시스템으로 블로그 이전에 대한 장단점과 관련 정보 기술","date":"2023-03-02","objectID":"/posts/202303022327/","tags":["Misc"],"title":"블로깅 플랫폼 변경: Google Blogger에서 GitHub + Hugo로","uri":"/posts/202303022327/"},{"categories":["Misc"],"content":" ","date":"2023-03-02","objectID":"/posts/202303022327/:0:0","tags":["Misc"],"title":"블로깅 플랫폼 변경: Google Blogger에서 GitHub + Hugo로","uri":"/posts/202303022327/"},{"categories":["Misc"],"content":"github + hugo로 이사! ","date":"2023-03-02","objectID":"/posts/202303022327/:1:0","tags":["Misc"],"title":"블로깅 플랫폼 변경: Google Blogger에서 GitHub + Hugo로","uri":"/posts/202303022327/"},{"categories":["Misc"],"content":"이전: google blogger 이전에 사용했던 블로그의 플랫폼은 google blogger였다. tistory나 naver 블로그에 비해 내가 원하는 문법의 허용범위가 더 커서 오랫동안 사용중이었다. (예를 들면 toc, 각주, MarkDown 문법과 HTML 문법으로의 글 작성 모두 지원 등…) 기술 블로그에서 하나의 문서 양이 스크롤을 필요로 한다면 특히나 필요한 것이, 바로 TOC(table of contents) 라고 생각한다. h태그를 이용한 toc의 가장 큰 장점은 전체적인 개요를 한 눈에 살펴볼 수 있고, 또한 해당 태그 부분만의 링크를 가질 수 있다는 점!! 이로써 참고 링크를 추가할 때 문서의 스크롤 위치까지 지정할 수 있는 이점이 있다.1 문제는 바로 이 toc를, google blogger에서 문서 상단에 고정된 방식이 아니라, 스크롤 중에도 상시 표현되는 사이드 바로 빼고 싶었는데 그러자니 xml을 건드리는 수고가 여간 큰게 아니었다. 게다가 google blogger의 xml 방식 때문인지는 모르겠지만 website 최적화 체감지수가 미묘하게 불쾌하달까? 반응이 반템포 느리다. 체험해본바 주관적인 최적화 속도는 다음 순서였다. github.io = velog \u003e tistory \u003e\u003e naver blog \u003e google blogger ","date":"2023-03-02","objectID":"/posts/202303022327/:1:1","tags":["Misc"],"title":"블로깅 플랫폼 변경: Google Blogger에서 GitHub + Hugo로","uri":"/posts/202303022327/"},{"categories":["Misc"],"content":"이후: github.io 보통 github 블로그 구성시 많이들 이용하는 정적사이트 생성기로서 ruby 언어 기반의 jekyll이 있는데, hugo를 택한 이유는 다음과 같다. Jekyll 현재 가장 인기 있음(깃헙 별 수 제일 많음) 한글 레퍼런스도 제일 많음 느리다는 제보가 많음(몇 십개의 포스팅 뿐인데도 빌드 하는데 5분씩 걸린다고) 윈도우 공식 지원 안됨 Hexo 자바스크립트(Node.js) 기반 한글 레퍼런스 꽤 많음 메인 개발자가 손을 놓은 듯 개발자가 중국인? 이라 구글링하면 중국어 글이 많이 나옴 Hugo Go 기반 빌드 빠름 문서화 잘돼있음 깃헙 별 수가 헥소보다 많음 한글 레퍼런스가 거의 없음 출처: http://tadakichi.tistory.com/188 사용하며 느낀 장점 사용자 설정이 꽤 자유롭다. git으로 관리 가능 hugo server 명령어를 통해 web과 동일한 수준의 전체 site map 미리보기 가능 (댓글처럼 외부 콤포넌트를 이용한 부분은 제외) 게시물의 title, description등을 따로이 설정 가능함으로 SEO에 유리하다. 컨텐츠의 원본 문서를 언제든 HTML로 변환할 수 있는 MD파일이 곧 게시글 초안이자 발행 문서임으로, 해당 플랫폼 귀속이 아닌 내 로컬 스토리지에서 관리가 가능하다. 위의 장점때문에 플랫폼 이동시, 전체 컨텐츠 이전이 비교적 용이하다. 이미지 삽입시, 로컬+github 이중 관리로 링크소실의 염려나 원본 이미지 파일의 관리 등이 수월하다.2 hugo의 경우 shortcodes(일종의 snippet)를 지원함으로써, markdown의 부족한 부분을 보다 손쉽게 충족하며 문서작성을 돕는다. 이를테면 github의 gist 추가나 tweet, instagram, youtube, vimeo 등을 손쉽게 넣을 수 있도록 기본 built-in으로 들어가있고, 사용자가 직접 추가할수도 있다. 3 notion, obsidian의 기능중 하나인 callout 기능을 손쉽게 구현 가능 hugo에서는 이를 admonition이라고 부른다.4 hugo의 admonition의 특징은 toggle이 default 옵션이라는 점!! 단점 장점에서 언급됐었지만, 아이러니하게도 git으로 관리해야한다는 점 때문에 메인 디바이스가 아니거나 모바일인 경우, 배포가 불편하다. hugo의 경우, 비공개 게시글이나 비공개 초안 등을 관리한다면 2개의 repository 또는 branch를 운영해야한다. 단점일지 조금 애매하긴 한데, hugo에서는 파일명이 곧 url 주소가 된다. 때문에 한글보다는 영어로 파일명을 관리하는 것이 seo에서 유리하다. 이는 하나의 게시글당 2번의 commit과 push가 필요하다는 뜻 하지만, 쉘스크립트를 이용해 한번에 해결할 수 있다!! (Click!) #!/bin/bash echo -e \"\\033[0;32mDeploying updates to GitHub...\\033[0m\" # Cleaning the public folder (preventing crashes and removing trash) cd public find . ! -name '.git' ! -name 'CNAME' -delete cd .. # Build the project. hugo -t LoveIt # Go To Public folder, sub module commit cd public # git pull origin main git pull origin main # Add changes to git. git add . # Commit changes. msg=\"rebuilding site `date`\" if [ $# -eq 1 ] then msg=\"$1\" fi git commit -m \"$msg\" # Push source and build repos. git push origin main # Come Back up to the Project Root cd .. # blog remote git Commit \u0026 Push git add . msg=\"rebuilding site `date`\" if [ $# -eq 1 ] then msg=\"$1\" fi git commit -m \"$msg\" git push origin main # Create a json file to index with the algolia API npm run algolia 참고 링크 ","date":"2023-03-02","objectID":"/posts/202303022327/:1:2","tags":["Misc"],"title":"블로깅 플랫폼 변경: Google Blogger에서 GitHub + Hugo로","uri":"/posts/202303022327/"},{"categories":["Misc"],"content":"추가 작업 블로그를 이전하며 새롭게 세팅하고 더불어 추가 연결(?) 작업 등의 목록. ","date":"2023-03-02","objectID":"/posts/202303022327/:2:0","tags":["Misc"],"title":"블로깅 플랫폼 변경: Google Blogger에서 GitHub + Hugo로","uri":"/posts/202303022327/"},{"categories":["Misc"],"content":"SEO google search console, naver search advisor 처리 google analytics 처리 ","date":"2023-03-02","objectID":"/posts/202303022327/:2:1","tags":["Misc"],"title":"블로깅 플랫폼 변경: Google Blogger에서 GitHub + Hugo로","uri":"/posts/202303022327/"},{"categories":["Misc"],"content":"comment github issue를 이용한 utterances를 설치하여 댓글로 자리잡음 하였다. light, dark theme를 auto로 설정하여 적용시킬 수 있어 외부 컴포넌트임에도 꽤 이질감 없는 디자인을 보여준다. 개발자들이라면 꼭 가지고 있는 github 계정을 활용한 댓글 방식이라, 기술 블로그에 걸맞게 조금이나마 더 나은 전문성과 투명성을 업은 양질의 댓글 문화를 기대할 수 있다(?) ","date":"2023-03-02","objectID":"/posts/202303022327/:2:2","tags":["Misc"],"title":"블로깅 플랫폼 변경: Google Blogger에서 GitHub + Hugo로","uri":"/posts/202303022327/"},{"categories":["Misc"],"content":"custom domain 이전에 사용하던 도메인(codject.dev)은 프로젝트에서 쓰려고 만든 도메인이었는데, 어떻게 하다보니 블로그에서 쓰고 있었다. inchan.dev 도메인은 작년 justcode 부트캠프와 함께 코딩을 시작하면서 구매해놓았었는데 이제서야 내 이름을 걸고 만든 도메인으로 기술 블로그를 운영해볼까 하는 생각이 들었다. ","date":"2023-03-02","objectID":"/posts/202303022327/:2:3","tags":["Misc"],"title":"블로깅 플랫폼 변경: Google Blogger에서 GitHub + Hugo로","uri":"/posts/202303022327/"},{"categories":["Misc"],"content":"search service : algolia algolia에 대한 대략적인 설명이다. Algolia는 SaaS모델로서 웹 검색을 제공하는 서비스입니다. 5, Algolia는 검색 구현과 검색 분석의 두 부분으로 구성되어 있습니다. 구현 도구를 사용하면 개발자가 사용자를 위한 훌륭한 검색 환경을 더 쉽게 만들고 유지할 수 있습니다. 6 장점 json 파일을 활용한 index 방식이라 그런지 속도가 굉장히 빠르다. 검색하고자 하는 문서의 범위를 사용자가 세밀하게 조정 가능하다. 검색어 관련 세부적인 분석이 가능하다. 소규모 사이트의 경우 free plan으로도 어느정도 커버 가능하다. 단점 npm - push_algolia_json.js 파일을 실행시키는 스크립트를 만들고 새로운 문서의 발행시마다 API를 통해 indexing을 실행해줘야 하다.7 (algolia에서 자동으로 robots.txt 파싱하는 방식이 아닌듯…) ","date":"2023-03-02","objectID":"/posts/202303022327/:2:4","tags":["Misc"],"title":"블로깅 플랫폼 변경: Google Blogger에서 GitHub + Hugo로","uri":"/posts/202303022327/"},{"categories":["Misc"],"content":"obsidian 연동 obsidian내 해당 폴더를 편입시킴으로써, 모든 기술 문서 통합 관리 가능 이미지 첨부시 obsidian의 자동 링크연결 기능 활용(drag \u0026 drop으로 굉장히 편하다.) 문서의 특정 header로 바로 가기 ↩︎ 클라우드에 업로드하고 image src 방식으로 사용시 해당 클라우드 서비스가 종료하면서 상당히 머리 아픈 일이 생긴 경험이 있다. ↩︎ hugo shortcodes-add gist ↩︎ hugo documentation ↩︎ 참고문서 https://www.algolia.com/doc ↩︎ https://www.algolia.com/doc/guides/getting-started/what-is-algolia/  ↩︎ 그러나 이 역시 쉘스크립트를 이용해 새 문서 발행시마다 algolia index 과정을 자동화시킴으로써 누락과 과정의 번거로움을 간소화하였다. ↩︎ ","date":"2023-03-02","objectID":"/posts/202303022327/:2:5","tags":["Misc"],"title":"블로깅 플랫폼 변경: Google Blogger에서 GitHub + Hugo로","uri":"/posts/202303022327/"},{"categories":["Dev"],"content":"TypeORM을 사용하여 트랜잭션을 구성하는 방법에 대한 고찰. dataSource.createQueryRunner() 방식으로 트랜잭션을 설정하는 과정에서, 리포지토리 메소드 사용 시 중복된 트랜잭션 실행으로 인해 발생하는 rollback 문제를 설명. 또한, manager.withRepository 메소드를 활용해 트랜잭션 내에서 커스텀 리포지토리를 사용하는 해결 방법 제시","date":"2023-02-25","objectID":"/posts/202302250414/","tags":["typeORM","mySQL","project_review","Back-End"],"title":"typeORM transaction에서 repository 사용하기","uri":"/posts/202302250414/"},{"categories":["Dev"],"content":" ","date":"2023-02-25","objectID":"/posts/202302250414/:0:0","tags":["typeORM","mySQL","project_review","Back-End"],"title":"typeORM transaction에서 repository 사용하기","uri":"/posts/202302250414/"},{"categories":["Dev"],"content":"transaction 구성중 Entity가 아닌 Repository 사용시 typeORM에서 transaction을 사용하는 방법은 몇가지가 있지만, 복잡한 service단의 처리에서는 아래의 queryRunner 방식이 더 적합할것 같아서 dataSource.createQueryRunner() 방식을 사용했다.1 // transaction으로 묶어주기 const queryRunner = dataSource.createQueryRunner(); await queryRunner.connect(); await queryRunner.startTransaction(); try { // feed 저장 let newTempFeed: Feed = plainToInstance(FeedDto, feedInfo); const tempFeed = await FeedRepository.createFeed(newTempFeed); //\u003c- 문제!! // ... 추가 코드들 } catch (err) { await queryRunner.rollbackTransaction(); throw new Error(`createTempFeed TRANSACTION error: ${err}`); } finally { await queryRunner.release(); } ","date":"2023-02-25","objectID":"/posts/202302250414/:1:0","tags":["typeORM","mySQL","project_review","Back-End"],"title":"typeORM transaction에서 repository 사용하기","uri":"/posts/202302250414/"},{"categories":["Dev"],"content":"문제 - commit 중복으로 rollbackTransaction 실패 FeedRepository는 이미 그 자체로 코드 실행시 transaction이 따로이 실행된다. 즉, 여기서 이미 commit이 이루어진다는 뜻! 따라서 rollbackTransaction을 하는데 있어 FeedRepository.createFeed 코드는 이미 commit이 되었기에 rollback 해당 대상에서 벗어나게 된다. 위와 같이 FeedRepository에서 START TRANSACTION이 따로이 한번 더 일어나고, 아직 실행중이지만 도중에 COMMIT이 되어버림을 확인할 수 있다. ","date":"2023-02-25","objectID":"/posts/202302250414/:1:1","tags":["typeORM","mySQL","project_review","Back-End"],"title":"typeORM transaction에서 repository 사용하기","uri":"/posts/202302250414/"},{"categories":["Dev"],"content":"해결 - withRepository 메소드 고민을 해보자! transaction의 dataSource.manager는 따로 동작한다. 즉, 곰곰히 생각해보면 FeedRepository 단독실행이 아닌 아닌 저 transaction단에서 동작하는 범주 안에서 코딩이 이루어져야 한다. 2가지 방안이 떠올랐다. 첫번째는 FeedRepository안에 만들어놓은 메소드를 포기하고 따로이 다시금 transaction의 try문단 안에 코딩을 하는 것이고, 다른 하나는 저 FeedRepository를 어떻게든 transaction 동작 범주 안으로 우겨넣는 방법일 것이다. 다시금 코딩을 하자니 상당히 비효율적이고, 이건 도저히 아니라는 판단이 들었다. 그럼 우겨넣는 방법인데… 구글링을 해봐도 typeORM의 버전 업그레이드로 관련문서의 수도 적었고, 있다하더라도 대개 nestJS의 customRepository 관련 문서였다. 최후의 수단으로 IDE에서 사용가능한 메소드를 쭉 하나하나 살펴봤다. 그러다가 찾은 메소드 withRepository Note manager.withRepository 지정된 리포지토리에서 새 리포지토리 인스턴스를 생성하고 현재 EntityManager 인스턴스를 해당 리포지토리로 설정합니다. 트랜잭션에서 사용자 지정 리포지토리로 작업하는 데 사용됩니다. 아~주 적절하다! 이렇게까지 복잡했던 이유는, 만약 해당 코드가 단순히 기본 Entity라면 const tempFeed = await queryRunner.manager.save(Feed, newTempFeed); 간단하게 뭐 이렇게 나가면 된다. 하지만 단순한 Entity가 아닌, 이미 일련의 실행과정을 코딩해놓은 Repository이기에 아래와 같이 withRepository 메소드를 사용한다. 그러면 해당 레포에서 static으로 박아놓은 .createFeed라는 하위 메소드도 연결해서 사용가능하다. const tempFeed = await queryRunner.manager .withRepository(FeedRepository) .createFeed(newTempFeed); 테스트 결과 빠르게 나머지 코드들을 세팅하고 테스트를 해본다. 이제 정상적으로 START TRANSACTION이 한번 일어나고, 의도적인 에러를 터트려 롤백하게 했더니 마지막으로 무사히 ROLLBACK이 이루어진다. transaction안에 AWS S3 objectCommand까지 있어 코드가 좀 어지러웠지만 그래도 예상되는 모든 Transaction-Rollback 테스트가 모두 통과되어 되어 한시름 놓았다. getConnection() 방식은 typeORM 0.3.x에서 deprecated 되었고, dataSource 방식으로 대체하면 된다. ↩︎ ","date":"2023-02-25","objectID":"/posts/202302250414/:1:2","tags":["typeORM","mySQL","project_review","Back-End"],"title":"typeORM transaction에서 repository 사용하기","uri":"/posts/202302250414/"},{"categories":["Dev"],"content":"TypeORM을 사용하여 댓글과 대댓글의 무한 트리 구조를 구현하는 과정에서 발생한 문제와 그 해결 방법을 다룬다. 특히, Date 타입 필드에서 불필요한 시간대 정보(T, Z 등)와 밀리초 부분을 제거하고 원하는 형식으로 출력하기 위한 방법을 상세히 설명","date":"2023-02-16","objectID":"/posts/202302160417/","tags":["TypeScript","typeORM","project_review","Back-End"],"title":"Node.js 백엔드TypeScript + typeORM으로 무한 대댓글 가공하기","uri":"/posts/202302160417/"},{"categories":["Dev"],"content":" ","date":"2023-02-16","objectID":"/posts/202302160417/:0:0","tags":["TypeScript","typeORM","project_review","Back-End"],"title":"Node.js 백엔드TypeScript + typeORM으로 무한 대댓글 가공하기","uri":"/posts/202302160417/"},{"categories":["Dev"],"content":"기능 구현 목표 댓글, 대댓글… 대댓글 등의 무한대댓글 구조 typeORM의 Entity를 연계한 createQueryBuilder 등을 지향하며, 최대한 직접적인 QueryRunner 방식 지양 삭제 및 비공개 댓글 가림 json 출력시 불필요한 요소 제거 (특히 Date 타입에서 !! ) ","date":"2023-02-16","objectID":"/posts/202302160417/:1:0","tags":["TypeScript","typeORM","project_review","Back-End"],"title":"Node.js 백엔드TypeScript + typeORM으로 무한 대댓글 가공하기","uri":"/posts/202302160417/"},{"categories":["Dev"],"content":"가장 오랜시간을 지연시켰던 Blocker typeORM createQueryBuilder 메소드로 출력시 2023-02-14T08:55:24.090Z // ^ ^^^^ \u003c- 거추장스럽다. 이런 식으로 나오는 문제가 있는데 꽤나 씨름했다. 우선 Entity의 Date type은 datetime이 아닌 timestamp로 했다. mySQL에서의 출력은 DB가 설치된 서버의 시간대를 따르기에 아래와 같이 문제가 없다. select * from comments where feedId = 1 2023-02-14 17:55:24.106558 -- 이게 실제 한국 시간 문제는, typeORM에서의 repository + createQueryBuilder를 이용한 직접 출력이다. typeORM의 .find나 createQueryBuilder 메소드를 통해 출력을 하게 되면 ISO 8601 형식으로 내보내지기 때문에 먼저 보여줬던 예와 같이 문제가 생기는데 다시 정리하면 아래와 같다. 시간대 안맞음 (실제 시간은 17:55:24) 시간대 앞 뒤로 T, 090Z와 같은 부가요소가 함께 출력 현재 데이터를 출력하려는 query는 다음과 같다. export const CommentRepository = dataSource.getRepository(Comment).extend({ async getCommentList(id: number) { return await this.createQueryBuilder('comment') .leftJoinAndSelect('comment.children', 'children') .leftJoinAndSelect('comment.user', 'user') .leftJoinAndSelect('comment.feed', 'feed') .leftJoinAndSelect('children.user', 'childrenUser') .leftJoinAndSelect('children.feed', 'childrenFeed') .where('comment.feed = :id', { id }) .andWhere('comment.parentId IS NULL') .orderBy('comment.id', 'ASC') .addOrderBy('children.id', 'ASC') .setParameter('id', id) .getMany(); }, } ","date":"2023-02-16","objectID":"/posts/202302160417/:2:0","tags":["TypeScript","typeORM","project_review","Back-End"],"title":"Node.js 백엔드TypeScript + typeORM으로 무한 대댓글 가공하기","uri":"/posts/202302160417/"},{"categories":["Dev"],"content":"해결방안 2024.03.03 시간대 관련 수정글 추가 이 글을 작성할 당시에 DB와 시간대에 관한 개념이 확실하게 잡혀있지 않았다. 따라서 시간대와 관련된 최신 글의 링크를 아래와 같이 남기지만, 해당 원글은 그대로 보존하도록 한다. typeORM 시간대 설정에 관한 고찰 - typeORM의 dateStrings와 timezone 옵션에 따른 시간대 혼란 - inchan.dev 아래 글 중 잘못된 내용은 아래와 같다. local 시간대 맞추기 - timezone 옵션 내용 시간 구분자 제거 - datastrings 옵션 내용 ","date":"2023-02-16","objectID":"/posts/202302160417/:3:0","tags":["TypeScript","typeORM","project_review","Back-End"],"title":"Node.js 백엔드TypeScript + typeORM으로 무한 대댓글 가공하기","uri":"/posts/202302160417/"},{"categories":["Dev"],"content":"1. local 시간대 맞추기 수정 전 내용 보기 우선, 이를 local의 시간대로 맞추기 위해 아래와 같이 DataSource 설정에 timezone: 'Z' 를 추가하여 local 시간대로 출력되게끔 한다. const dataSource = new DataSource({ type: process.env.TYPEORM_CONNECTION, host: process.env.TYPEORM_HOST, port: process.env.TYPEORM_PORT, username: process.env.TYPEORM_USERNAME, password: process.env.TYPEORM_PASSWORD, database: process.env.TYPEORM_DATABASE, entities: [__dirname + '/../**/*.entity.{js,ts}'], timezone: 'Z', // \u003c-- 이렇게 추가 logging: Boolean(process.env.TYPEORM_LOGGING), synchronize: Boolean(process.env.TYPEORM_SYNCHRONIZE), charset: 'utf8mb4', }); // 출력시, 2023-02-14T17:55:24.106Z 자 이제 시간대는 맞춰졌다. ","date":"2023-02-16","objectID":"/posts/202302160417/:3:1","tags":["TypeScript","typeORM","project_review","Back-End"],"title":"Node.js 백엔드TypeScript + typeORM으로 무한 대댓글 가공하기","uri":"/posts/202302160417/"},{"categories":["Dev"],"content":"2. 시간 구분자 제거 수정 전 내용 보기 다음은 문제의 저 T와 Z를 날려보자. dateStrings: true, DataSouce 연결 설정에 이걸 넣어준다. const dataSource = new DataSource({ type: process.env.TYPEORM_CONNECTION, host: process.env.TYPEORM_HOST, port: process.env.TYPEORM_PORT, username: process.env.TYPEORM_USERNAME, password: process.env.TYPEORM_PASSWORD, database: process.env.TYPEORM_DATABASE, entities: [__dirname + '/../**/*.entity.{js,ts}'], timezone: 'Z', dateStrings: true, // \u003c-- 이렇게 추가 logging: Boolean(process.env.TYPEORM_LOGGING), synchronize: Boolean(process.env.TYPEORM_SYNCHRONIZE), charset: 'utf8mb4', }) 그리고 다시 출력을 해보면 2023-02-14 17:55:24.106558 드디어 불필요한 알파벳은 사라졌다…라기보단, 그런데 더 거추장스럽게도 밀리초(?)로 바뀌었다. 이제 이걸 제거해보자! ","date":"2023-02-16","objectID":"/posts/202302160417/:3:2","tags":["TypeScript","typeORM","project_review","Back-End"],"title":"Node.js 백엔드TypeScript + typeORM으로 무한 대댓글 가공하기","uri":"/posts/202302160417/"},{"categories":["Dev"],"content":"3. 정밀 초단위는 제거하고 시,분까지만 출력 여기서 정말 많이 찾아보고 공부했다. typeORM의 queryRunner를 사용하거나, createQueryBuilder에서 getRawMany()를 사용해 자료를 뽑아낸다면, mySQL의 출력방식을 따르기때문에 SQL의 query문에 직접적인 수정을 가하여 컬럼값을 조정할 수가 있다. (또는 toISOString()과 같은 메소드를 사용하여도 된다.) 예시) select SUBSTRING(created_at, 1, 19) AS created_at from comments 결과 2023-02-14 17:55:24 하지만, 위에서 쓰고 있는 Entity에서 그대로 가져오는 getMany() 메소드를 이용해 자료를 가져오게 되면, 이런 방법이 불가하다. 이것도 확실한지는 모르겠는데 며칠을 찾아보고 시도해본 결과, TypeORM의 getRawMany() 메서드는 쿼리 결과를 직접적으로 반환하는 반면, getMany() 메서드는 Entity 객체를 반환한다. 때문에 select \u003ccolumn\u003e에 바로 수정을 가하게 되면 Entity의 type을 그대로 가져다 쓰는 특성상, 이 컬럼의 type error가 난다. 즉, select 또는 addSelect로 Alias하거나 수정을 함과 동시에 출력이 되지 않고 되려 해당 컬럼이 사라진다. 이러한 상태로의 해결방법은 Entity에서 직접 Alias 해줘야 하는데, 또 이렇게 되면 CreateDateColumn의 데코레이션 default 옵션이 깨진다.(피나는 시도 결과 ;; ㅠ) 때문에 결국 양자택일을 하는 수밖에 없었다. getMany() 방식을 포기하거나, service단에서 복잡하지만 수정을 가한다. 굳이 getMany() 방식을 고집하는 이유는 다음과 같다. 현재 저 코드 Comment Entity에서 가져오며, 해당 Entity는 parent와 children\u003cComment[]\u003e의 자기참조 컬럼을 가지고 있는 Tree 구조로 이루어져 있다. 그리고 typeORM의 createQueryBuilder는 이 구조 특성을 그대로 살려서 출력이 가능하다. 반대로 query runner 방식으로 처리하여 같은 tree 구조를 출력하려면, repository단에서 굉장히 지독히도 가독성이 떨어지는 코드로 구현된다(물론 현재의 내 수준에서;;).1 typeORM의 Tree 방식으로 출력가능한 getMany() 메소드를 포기하게 된다면, repository단에서의 query가 생각보다 복잡하게 된다. 추후 유지보수의 가능성을 염두에 둔다. 대댓글 뿐만 아니라, 대대댓글 등의 무한 대댓글 구조역시 가능하도록 진행을 하는데 현재 Date type만의 수정을 원한다면, service단에서의 조정이 맞다는 판단이 들었다. 때문에 다음과 같이 service단에서 코드를 재귀함수(무한대댓글을 구현하기 위해)로 추가하며, 원하는 결과를 가져올 수 있도록 조정한다. 추가코드 // comments.service.ts // 가공영역에 대한 재귀함수 const formatComment = (comment: any, userId: number): any =\u003e { const isPrivate = comment.is_private === true \u0026\u0026 comment.user.id !== userId; const isDeleted = comment.deleted_at !== null; const formattedComment = { // 로그인 사용자의 비밀덧글 조회시 유효성 확인 및 삭제된 덧글 필터링 ...comment, comment: isDeleted ? '## DELETED_COMMENT ##' : isPrivate ? '## PRIVATE_COMMENT ##' : comment.comment, // Date 타입의 컬럼에서 불필요한 밀리초 부분 제외 created_at: comment.created_at.substring(0, 19), updated_at: comment.updated_at.substring(0, 19), deleted_at: comment.deleted_at ? comment.deleted_at.substring(0, 19) : null, // 대댓글 영역 -------------------------------- children: comment.children ? comment.children.map((child: any) =\u003e formatComment(child, userId)) : [], }; return formattedComment; }; 이후 리팩토링을 하며 만들어진 최종 대댓글 API 결과. // 현재 로그인 유저의 id는 16번 [ { \"id\": 95, // \u003c- 원댓글 \"created_at\": \"2023-02-14 21:36:43\", // \u003c-- 짜잔!! 드디어 깔끔한 마무리! \"updated_at\": \"2023-02-14 21:36:43\", \"deleted_at\": null, \"user\": { \"id\": 3, \"nickname\": \"103\", \"email\": \"serswell2@hc360.com\" }, \"feed\": { \"id\": 96, \"title\": \"vulputate luctus cum sociis\" }, \"comment\": \"nulla tellus in sagittis\", \"is_private\": false, \"children\": [ // \u003c- 1차 대댓글 배열 { \"id\": 212, // \u003c- 대댓글 \"created_at\": \"2023-02-14 21:36:43\", \"updated_at\": \"2023-02-14 21:36:43\", \"deleted_at\": null, \"user\": { \"id\": 16, \"nickname\": \"test nickname6\", \"email\": \"test6@test.com\" }, \"comment\": \"댓글 수정 테스트\", \"is_private\": false, \"children\": [ // \u003c- 2차 대댓글 배열 { \"id\": 220, // \u003c- 대댓글에 대한 대댓글 \"created_at\": \"2023-02-16 20:04:19\", // \u003c-여기까지 재귀함수가 \"updated_at\": \"2023-02-16 20:04:19\", // 잘 적용되고 있음을 확인!! \"deleted_at\": null, \"user\": { \"id\": 16, \"nickname\": \"test nickname6\", \"email\": \"test6@test.com\" }, \"comment\": \"레포지토리 커멘트 생성 테스트\", \"is_private\": true, \"children\": [] } ] }, { \"id\": 214, \"created_at\": \"2023-02-14 21:36:43\", \"updated_at\": \"2023-02-14 21:36:43\", \"deleted_at\": null, \"user\": { \"id\": 15, // 다른 유저가 작성한 비공개 대댓글 \"nickname\": \"test nickname5\", \"email\": \"test5@test.com\" }, \"comment\": \"## PRIVATE_COMMENT ##\", // \u003c- 여기서도 정상적 차단 확인 \"is_private\": true, \"children\": [] }, { \"id\": 215, \"created_at\": \"2023-02-14 21:36:43\", \"updated_at\": \"2023-02-16 20:03:42\", \"deleted_at\": \"2023-02-16 20:03:42\", // \u003c- 본인의 삭제한 댓글 \"user\": { \"id\": 16, \"nickname\": \"test nickname6\", \"email\": \"test6@test.com\" }, \"comment\": \"## DELETED_COMMENT ##\", // \u003c- 정상적 차단 확인 \"is_private\": false, \"children\": [] } ] }, ] 아주 이쁘게 잘 나왔다. typeORM 관련하여 전체적으로 내가 원하는 샘플코드는 구체적으로 검색해보기가 힘들어, 대부분 typeORM 공식문서를 참고하며 진행하였고, 생각보다 공식문서가 굉장히 잘되어 있어 놀랐다. (공식문서 속에서 원하는 정보 찾는","date":"2023-02-16","objectID":"/posts/202302160417/:3:3","tags":["TypeScript","typeORM","project_review","Back-End"],"title":"Node.js 백엔드TypeScript + typeORM으로 무한 대댓글 가공하기","uri":"/posts/202302160417/"},{"categories":["Dev"],"content":"Entity 아! Entity에서 기본적으로 들어가는 id와 created_at, updated_at 등은 Base Entity로 따로 만들어 Entity 작업시 반복적으로 기입하지 않고 Embeded Entityes 방식으로 처리하였다.2 // comment.entity.ts @Entity('comments') export class Comment extends Base { @ManyToOne(type =\u003e User, users =\u003e users.comment, { nullable: false }) @JoinColumn({ name: 'userId' }) user: User; @ManyToOne(type =\u003e Feed, feeds =\u003e feeds.comment, { nullable: false }) @JoinColumn({ name: 'feedId' }) feed: Feed; @Column({ length: 1000 }) comment: string; @Column('boolean', { default: false }) is_private: boolean; @ManyToOne(type =\u003e Comment, comment =\u003e comment.children) @JoinColumn({ name: 'parentId' }) parent: Comment; @OneToMany(type =\u003e Comment, comment =\u003e comment.parent) children: Comment[]; } service단!! // comments.service.ts // 무한 대댓글의 경우, 재귀적으로 호출되는 함수 const formatComment = (comment: any, userId: number): any =\u003e { const isPrivate = comment.is_private === true \u0026\u0026 comment.user.id !== userId; const isDeleted = comment.deleted_at !== null; const formattedComment = { ...comment, // 로그인 사용자의 비밀덧글 조회시 유효성 확인 및 삭제된 덧글 필터링 comment: isDeleted ? '## DELETED_COMMENT ##' : isPrivate ? '## PRIVATE_COMMENT ##' : comment.comment, // Date 타입의 컬럼에서 불필요한 밀리초 부분 제외 created_at: comment.created_at.substring(0, 19), updated_at: comment.updated_at.substring(0, 19), deleted_at: comment.deleted_at ? comment.deleted_at.substring(0, 19) : null, // 대댓글 영역 -------------------------------- children: comment.children ? comment.children.map((child: any) =\u003e formatComment(child, userId)) : [], }; return formattedComment; }; const getCommentList = async (id: number, userId: number) =\u003e { const result = await CommentRepository.getCommentList(id); const formattedResult = [...result].map((comment: any) =\u003e formatComment(comment, userId) ); return formattedResult; }; ","date":"2023-02-16","objectID":"/posts/202302160417/:3:4","tags":["TypeScript","typeORM","project_review","Back-End"],"title":"Node.js 백엔드TypeScript + typeORM으로 무한 대댓글 가공하기","uri":"/posts/202302160417/"},{"categories":["Dev"],"content":"Repository단!!! // comment.repository.ts export const CommentRepository = dataSource.getRepository(Comment).extend({ async getCommentList(id: number) { return await this.createQueryBuilder('comment') .withDeleted() .addSelect(['user.id', 'user.nickname', 'user.email']) .addSelect(['feed.id', 'feed.title']) .addSelect([ 'childrenUser.id', 'childrenUser.nickname', 'childrenUser.email', ]) .addSelect([ 'childrenUser2.id', 'childrenUser2.nickname', 'childrenUser2.email', ]) .leftJoin('comment.user', 'user') .leftJoin('comment.feed', 'feed') .leftJoinAndSelect('comment.children', 'children') .leftJoin('children.user', 'childrenUser') .leftJoinAndSelect('children.children', 'children2') .leftJoin('children2.user', 'childrenUser2') .where('comment.feed = :id', { id }) .andWhere('comment.parentId IS NULL') .orderBy('comment.id', 'ASC') .addOrderBy('children.id', 'ASC') .setParameter('id', id) .getMany(); }, } fastfive 기업과제에서 수행했던 자기참조방식의 대댓글 구현 여기서 query runner방식은 이미 해봤기에 이번에는 특히나 typeORM의 queryBuilder 문법으로만 진행해보고 싶었다. ↩︎ TypeORM - Amazing ORM for TypeScript and JavaScript (ES7, ES6, ES5). Supports MySQL, PostgreSQL, MariaDB, SQLite, MS SQL Server, Oracle, WebSQL databases. Works in NodeJS, Browser, Ionic, Cordova and Electron platforms. ↩︎ ","date":"2023-02-16","objectID":"/posts/202302160417/:3:5","tags":["TypeScript","typeORM","project_review","Back-End"],"title":"Node.js 백엔드TypeScript + typeORM으로 무한 대댓글 가공하기","uri":"/posts/202302160417/"},{"categories":["Misc"],"content":"ChatGPT의 답변을 분석하며, chatGPT의 검색 방식과 기존 구글링 방식의 차이 및 ChatGPT의 장점과 한계 평가. 더불어 chatGPT를 효율적으로 사용하는 방법에 대한 제안과 개선점을 함께 제시","date":"2023-02-10","objectID":"/posts/202302100411/","tags":["Misc"],"title":"chatGPT 사용기 - whereClauseBuilder 함수를 물어보았다.","uri":"/posts/202302100411/"},{"categories":["Misc"],"content":" ","date":"2023-02-10","objectID":"/posts/202302100411/:0:0","tags":["Misc"],"title":"chatGPT 사용기 - whereClauseBuilder 함수를 물어보았다.","uri":"/posts/202302100411/"},{"categories":["Misc"],"content":"chatGPT에게 whereClauseBuilder 함수를 물어보았다. 원하는 정보는 javascript에서 mySQL DB로 query문을 작성하여 사용시, 반복되는 where절에 대해 함수화하여 재사용할 수 있는 방법 또는 예시 코드였다. 질문은 한글로 했고, 영어로 답변이 오는데 속도는 꽤 빠르다. 하지만 답변을 한글로 받고자 하면 꽤 오랜 지연이 발생한다. whereClauseBuilder함수란? I’m sorry, but you haven’t provided enough context for me to understand what you’re asking about. Could you please provide more information or specify what you’re looking to learn about “whereClauseBuilder”? 앞뒤 맥락 다 생략하고 바로 물었더니 잘 못알아듣는다. 추가 설명을 덧붙여 다시 물어보았다. javascript에서 mysql을 사용하는데 whereClauseBuilder 라는 방법이 있다고 한다. 이게 뭐야? It seems like you’re referring to a custom function named “whereClauseBuilder” in JavaScript that is used to build a WHERE clause for a SQL query to be used with a MySQL database. A WHERE clause is used in a SQL query to specify a condition for filtering the data that is returned by the query. The “whereClauseBuilder” function would be used to dynamically generate the WHERE clause based on some input parameters. It’s important to note that this function is likely a custom implementation and may not be a part of any official MySQL library or API. It is always a good practice to make sure that the custom code used in your applications is secure and does not leave any vulnerabilities that can be exploited. 상황과 맥락을 짚어주니 알아듣기 시작했다. 이제 추가적인 질문이 들어간다. javascript 코드 중 mysql의 where 절을 종종 사용한다. 이 때, 이 where 절을 만들어내는 함수의 예시를 보여줘 Sure! Here’s an example of a “whereClauseBuilder” function in JavaScript that can be used to dynamically generate a WHERE clause for a SQL query to be used with a MySQL database: function whereClauseBuilder(filters) { let whereClause = \"\"; if (filters \u0026\u0026 Object.keys(filters).length \u003e 0) { whereClause = \"WHERE \"; const filterKeys = Object.keys(filters); filterKeys.forEach((key, index) =\u003e { whereClause += `${key} = '${filters[key]}'`; if (index \u003c filterKeys.length - 1) { whereClause += \" AND \"; } }); } return whereClause; } This function takes in an object “filters” that represents the conditions for the WHERE clause. It then generates a WHERE clause by concatenating the key-value pairs in the “filters” object. The conditions are joined with the “AND” operator. Here’s an example usage of the function: \u003econst filters = { column1: \"value1\", column2: \"value2\" }; const whereClause = whereClauseBuilder(filters); console.log(whereClause); // Output: WHERE column1 = 'value1' AND column2 = 'value2' Please note that this is just one example of how a “whereClauseBuilder” function could be implemented. The specific implementation will depend on the needs of your application and the way that you want to handle the input parameters. 기본적인 whereClauseBuilder 함수 코드를 보여주고 이어서 자세한 설명을 덧붙여준다. 게다가 이 함수를 사용할 수 있는 예시 코드까지 친절하게 알려주고 출력시 로그까지 보여준다. 좋아. 이걸 typescript로 바꿔줘 Sure! Here’s an equivalent implementation of the “whereClauseBuilder” function in TypeScript: function whereClauseBuilder(filters: object): string { let whereClause = \"\"; if (filters \u0026\u0026 Object.keys(filters).length \u003e 0) { whereClause = \"WHERE \"; const filterKeys = Object.keys(filters); filterKeys.forEach((key, index) =\u003e { whereClause += `${key} = '${filters[key]}'`; if (index \u003c filterKeys.length - 1) { whereClause += \" AND \"; } }); } return whereClause; } In this implementation, the function signature includes type information for the input parameters and the returned value. This makes the code easier to understand and less prone to bugs caused by incorrect data types. 좋아, 여기서 Object.keys의 예를 보여줘 Sure! Object.keys() is a method in JavaScript that returns an array of the enumerable property names of an object. Here’s an example usage of Object.keys(): const obj = { name: \"John\", age: 30, city: \"New York\" }; const keys = Object.keys(obj); console.log(keys); // Output: [ \"name\", \"age\", \"city\" ] In the example, we have an object obj with three properties, name, age, and city. By calling Object.keys(obj), we get an array of strings containing the property names of the object. In the whereClauseBuilder function, we use Object.keys(fil","date":"2023-02-10","objectID":"/posts/202302100411/:0:1","tags":["Misc"],"title":"chatGPT 사용기 - whereClauseBuilder 함수를 물어보았다.","uri":"/posts/202302100411/"},{"categories":["Misc"],"content":"정말 흥미롭지 않은가? 이렇게 내가 원했던 부분들을 잘 짚어주다니, 정말 검색에 있어 새로운 패러다임이 만들어진게 아닐까 하는 생각이 든다. 혹자는 chatGPT가 기존의 구글링과 같은 검색방식을 대체할 수 있을거라고 하는데, 현재의 이 chatGPT버전을 두고 보자면 나는 조금 다르게 생각한다. 기존 구글링의 단점은 너무 많은 정보가 검색됨으로 인해, 그 중 내가 정말로 원하는 정보를 찾기 위해 하나하나 링크를 들여다봐야 하는 수고로움과 시간소요가 생기는 반면 여기서 생각하지 못했던 다른 방법이나 아이디어를 찾곤 할수 있다. 하지만 현재의 chatGPT는 이 부분이 빠진다. 즉, 내가 원하는 정보를 AI가 추려내고 콕집어 주기때문에 그것 말고는 다른 정보를 접해볼수 있는 기회가 없다. 즉, 완벽한 주입식 교육(?)이 되며, 또한 현재 버전에서는 제안해주는 정보의 신뢰성과 근거를 찾기가 애매하다. 뿐만 아니라 이러한 검색방식에 익숙해진다면, 자료의 교차검증이나 수많은 자료로부터 스스로 분석하고 결과를 도출해내는 사고가 사라진다. 물론, 사용하기에 따라 더 창의적이고 생산적으로 사용될 수 있겠지만 그 외의 수많은 이들은 편리함에만 익숙해질 것이다. 아마 개선? 추가되었으면 하는 부분을 말해본다면, chatGPT의 제안과 함께 현재 자료의 근거, 그리고 “함께 읽어볼만한 링크\"와 같은 추천 컨텐츠가 추가되면 어떨까 하는 생각을 해본다. 이 알고리즘 자체가 쉽진 않겠지만 (정치적, 경제적 이해관계를 비롯해서 다양한 이슈를 일으킬 수 있으니…) 본의 아니게 chatGPT의 단점이나 아쉬운 우려를 위주로 글이 쓰였는데, 사실 정말 엄청난 기술이다. 장점이야 워낙 많은 이들이 입이 닳도록 말하고 있고, 누구나 그저 한번만 질문을 해보면 알수 있을테니… 사용 팁이라면, 단순질문보다는 능동적이며 구체적인 연결된 질문이 본인이 원했던 답을 빠르게 찾을 수 있도록 도와준다. ","date":"2023-02-10","objectID":"/posts/202302100411/:0:2","tags":["Misc"],"title":"chatGPT 사용기 - whereClauseBuilder 함수를 물어보았다.","uri":"/posts/202302100411/"},{"categories":["Env"],"content":"DevonThink에서 MarkDown 웹클리핑 시 발생하는 이미지 링크 주석 처리 문제를 해결하기 위한 스크립트 작성과 적용 방법을 설명","date":"2023-02-02","objectID":"/posts/202302020410/","tags":["devonthink","markdown"],"title":"devonthink에서 markdown으로 웹클리핑시 이미지 주소 문제","uri":"/posts/202302020410/"},{"categories":["Env"],"content":" 코딩을 공부하면서 정말 많은 웹 문서들을 참고하며 정리하게 된다. 그 중, 보관하고자 하는 문서는 주로 웹클리핑을 통해 정보를 수집한다. 이때, 클리핑 포맷은 보통 PDF로 하지만 가끔 code block내 스크롤이 있는 경우에는 PDF에서 짤린채로 수집되는 문제가 있어 MarkDown 포맷으로 클리핑을 한다. 나는 주로 아래 두 도구를 사용하여 자료를 정리한다. Obsidian(note taking app) DevonThink(note database) 이때, 웹클리핑은 Obsidian 노트내로 바로 삽입할 수 있는 방법도 있지만 1 파일자체에 Tag 및 Annotation 관리를 위해 DevonThink의 웹클리핑 tool을 사용한다. ","date":"2023-02-02","objectID":"/posts/202302020410/:0:0","tags":["devonthink","markdown"],"title":"devonthink에서 markdown으로 웹클리핑시 이미지 주소 문제","uri":"/posts/202302020410/"},{"categories":["Env"],"content":"그런데 DevonThink의 MarkDown 웹클리핑 조금 이상한 부분이 있다. 일반적인 MarkDown 문법에서의 이미지 삽입 문법은 다음과 같다. ![image 설명](image 주소) 그런데 DevonThink의 웹클리퍼는 클리핑시 모든 부가적인 웹링크의 주소를 주석처리하는 경향이 보인다. 그래서 실제로 웹클리핑을 해보면 다음과 같이 MarkDown 변환 처리가 되어있음을 볼 수 있다. ![][1] \u003c!-- [1]은 주석번호 --\u003e 사실 이는 standard markdown 문법에서도 문제가 없기때문에 일반적인 다른 markdown 편집기에서도 이상없이 출력은 된다. 하지만… Obsidian 노트앱을 사용중이라면 Obsidian만의 변형 markdown 문법에서2 다른 문제를 야기한다. Obsidian에서는 이미지를 넣을때 이미지의 사이즈를 편하게 조절할 수 있는 문법이 있다.3 ![image 설명 |500](image 주소) \u003c!-- |숫자 는 이미지의 width를 설정해준다. |500x400 이렇게 넣으면 마크업 시, width 500, height 400을 설정해준다 --\u003e 이 때, 이미지의 주소가 주석으로 입력되면 위 문법이 먹히지 않게 된다. (변형문법의 단점 ;; ㅠ ) 이를 위해 이미지 주소 자리에 있는 주석번호를, 주석에 있는 원래의 이미지 링크로 바꿔주는 스크립트를 만들어 해당 파일에 스크립트를 적용하면 편하게 변환할 수 있다. 우선 스크립트는 다음과 같다. (() =\u003e { const app = Application(\"DEVONthink 3\") app.includeStandardAdditions = true; const records = app.selectedRecords(); records.forEach(r =\u003e { const links = getReferenceLinks(r.plainText()) r.plainText = replaceLinks(r.plainText(), links); }) })() function getReferenceLinks(txt) { const allLinks = [...txt.matchAll(/^\\[(\\d+)\\]:\\s+(.*)$/gm)]; const linkArray = {}; allLinks.forEach(l =\u003e { const index = l[1]; const URL = l[2]; linkArray[index] = URL; }) return linkArray; } function replaceLinks(txt, links) { console.log(links); Object.keys(links).forEach(k =\u003e { const URL = links[k]; const regEx = new RegExp(`]\\\\[${k}\\\\]`); txt = txt.replace(regEx, `](${URL})`); }) return txt; } 이를 적용하면 다음과 같이 주석으로 처리되었던 이미지 주소가 원 링크로 대체된다. ","date":"2023-02-02","objectID":"/posts/202302020410/:0:1","tags":["devonthink","markdown"],"title":"devonthink에서 markdown으로 웹클리핑시 이미지 주소 문제","uri":"/posts/202302020410/"},{"categories":["Env"],"content":"DevonThink의 상단 툴바에 스크립트 넣기 macOS에 기본으로 설치되어 있는 스크립트 편집기를 열어서 위 코드를 넣는다. 이때 옵션에서 applescript가 아닌, javascript로 설정해줘야 한다. 그리고 devonthink에서 이 스크립트를 상단 툴바에 넣고 싶다면, devonthink의 상단 메뉴 중 script(아이콘) - open scripts folder 를 눌러 아래와 같이 파인더로 진입한다. Toolbar 폴더 내에 방금 만든 스크립트를 넣는다. 그럼 이제 toolbar 커스터마이징을 통해 아래와 같이 상단에 위치 시킬 수 있다. 물론 이 스크립트는 DevonThink의 smart rules에 등록시켜 자동으로 파일을 감지하여 사용하게끔 할 수도 있다. Obsidian Web Clipper Bookmarklet to save articles and pages from the web (for Safari, Chrome, Firefox, and mobile browsers) ↩︎ 나의 경우 변형 markdown 문법의 경우 웬만해서는 기피한다. 왜냐하면 MD파일이 해당 어플리케이션에 종속되는 문제가 생기기 때문이다. 하지만 그 사용이 너무나도 편해서 종종 상황에 따라 사용하기도 한다. ↩︎ Support image resize · Issue #2 · zoni/obsidian-export ↩︎ ","date":"2023-02-02","objectID":"/posts/202302020410/:1:0","tags":["devonthink","markdown"],"title":"devonthink에서 markdown으로 웹클리핑시 이미지 주소 문제","uri":"/posts/202302020410/"},{"categories":["Env"],"content":"WebStorm의 Live Templates 기능을 활용해 console.log()를 간편하게 입력하는 방법을 설명","date":"2023-02-02","objectID":"/posts/202302020409/","tags":["webstorm"],"title":"WebStorm - console.log(변수) 자동완성 라이브 템플릿 만들기","uri":"/posts/202302020409/"},{"categories":["Env"],"content":" JavaScript로 코딩을 하다보면 적잖이 console.log()를 사용하게 된다. 보통 나는 간단하게 현재 흐름상 위치를 잡거나 객체를 확인할 때 자주 쓰는데, 많은 글자는 아니지만 그래도 같은걸 계속해서 타이핑하는게 귀찮아서 만들었다. 축약어는 console.log의 앞글자를 따서 cl로 단축 명령어를 만들었고, cl 단축어 입력 후 tab을 누르면 해당 WebStorm의 live template이 실행된다. 간단하게 다음과 같은 역할을 수행된다. console.log()를 입력 괄호 안에 변수명을 타이핑할 수 있도록 커서 위치 이동 변수명을 쓰면 따옴표 안에 동일한 변수가 들어가도록 설정 즉 console.log(\"변수 = \", 변수) 이러한 결과물을 만들어내고 터미널에는 아래와 같이 출력된다.(version 1) 변수 = 123 변수명을 타이핑한 후 tab키를 한번 더 누르면 따옴표 안의 왼쪽 설명쪽 변수로 커서가 이동한다. 그럼 상황에 맞게 console.log(\"변수의 위치는 service.ts의 signUp 함수 중' = \", 변수) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ // 추가설명 입력 뭐 이런식의 추가입력으로 사용할 수 있다. 만약 위에서 언급된 추가입력 부분을 자주 쓴다면 이 역시 WebStorm의 기능으로 템플릿 처리할 수가 있다.(version 2) 처음에는 변수를 셀렉트하고 감싸진 부분을 감지해서 사용하도록 surround templet으로 만들었었다. 그런데 어차피 몇 글자만 넣어도 자동으로 나머지 변수명이 팝업으로 뜨는 IDE 특성상, 그냥 입력하는게 더 편해서 다시 간단하게 만들었고 써보니 너무 편하다. ","date":"2023-02-02","objectID":"/posts/202302020409/:0:0","tags":["webstorm"],"title":"WebStorm - console.log(변수) 자동완성 라이브 템플릿 만들기","uri":"/posts/202302020409/"},{"categories":["Env"],"content":"결과물 예시 version 1 version 2 ","date":"2023-02-02","objectID":"/posts/202302020409/:0:1","tags":["webstorm"],"title":"WebStorm - console.log(변수) 자동완성 라이브 템플릿 만들기","uri":"/posts/202302020409/"},{"categories":["Env"],"content":"WebStrom Live Templates 추가 방법 ","date":"2023-02-02","objectID":"/posts/202302020409/:1:0","tags":["webstorm"],"title":"WebStorm - console.log(변수) 자동완성 라이브 템플릿 만들기","uri":"/posts/202302020409/"},{"categories":["Env"],"content":"console.log Version 1 우선 WebStorm / Settings… (환경설정)에 진입한다. 그리고 메뉴 중 Live Templates 선택 (스샷 1번) 이어서 템플릿 추가 버튼(스샷 2번)을 누르면 위 스샷과 동일한 화면을 볼 수 있다. 스샷 4번에서 사용하기 편한 단축어를 설정한다. 이 단축어를 타이핑하면 팝업창에서 나타나게 된다. 그리고 스샷의 붉은 박스안과 같이 입력한다. (selector, variable과 같은 변수명은 아무거나 다른걸로 써도 무방) 이후, 스샷3번의 Edit Variables…를 클릭하면 아래와 같은 화면이 나타난다. 여기서 변수의 순서를 설정할 수 있는데, 이는 live template 실행 후 커서의 이동 순서를 나타낸다. 두번째 변수인 selector안에는 첫번째 변수의 내용이 그대로 들어감으로 첫번째 변수명을 써주면 복사효과(?)처럼 사용된다. 끝으로 스샷 5번 자리를 보면 아마 처음에 define... 와 같은 이름으로 되어있다. 클릭시 아래와 같은 창이 뜨는데, 이 단축어를 실행할 파일 종류를 선택할 수 있다. console.log니까 javascript/typescript를 선택해주면 된다. ","date":"2023-02-02","objectID":"/posts/202302020409/:1:1","tags":["webstorm"],"title":"WebStorm - console.log(변수) 자동완성 라이브 템플릿 만들기","uri":"/posts/202302020409/"},{"categories":["Env"],"content":"console.log Version 2 서두에 언급했던 변수의 설명부분을 템플릿화 하고 싶다면 아래와 같이 진행. settings의 live templates에서 template test 박스안에 아래와 같이 입력 console.log('🔥🔥$FILE$/$FUNCTION$ :$LINE$ - $SELECTOR$ = ', $VARIABLE$) 그리고 위와 같이 변수를 설정한다. ","date":"2023-02-02","objectID":"/posts/202302020409/:1:2","tags":["webstorm"],"title":"WebStorm - console.log(변수) 자동완성 라이브 템플릿 만들기","uri":"/posts/202302020409/"},{"categories":["Dev"],"content":"TypeScript에서 any 대신 unknown 타입을 사용하는 것이 왜 더 좋은까에 대해 설명 및 고찰","date":"2023-01-28","objectID":"/posts/202301280423/","tags":["TypeScript"],"title":"typescript - any와 unknown의 차이","uri":"/posts/202301280423/"},{"categories":["Dev"],"content":" typescript로 코딩을 하다가 어떤 타입인지 정의 내리기 힘든 상황에서 주로 any 를 사용한다. 물론 typescript의 성격상 any는 지양하는 것이 좋다. 오류를 검증하고 확인하려는 특성으로 엄격한 타입을 지정하려는 typescript에서 어떤 타입이나 가능하다는 성격의 any는 어쩌면 존재 자체가 모순인 타입이기도 하다. 하지만코딩을 하다보면 아직은 타입을 정하기 전인 상황이거나, 도저히 타입을 알 수 없거나 추론할 수 없을 때, (또는 typescript 초급자일 때 ;;) 가끔 사용하게 되는 것이 또 any 타입이다. 하지만 이제부터 이럴때 any보다는 unknown을 사용하자. 이 둘의 정의는 마치 undefined와 null의 관계와도 비슷하다. 얼핏 비슷해보이지만 확실하게 다른 정의이다. any는 어떤 타입도 상관 없으며 어떤 식으로 내보내져도, 그리고 내보내진 타입을 어떤 곳에서 받아들이게 되더라도 상관이 없는… typescript의 엄격모드로부터 해방시켜주는 타입의 역할을 한다. unknown은 any와는 달리 어떤 타입인지 모르겠다는 의미이다. 즉, 단어 해석 그대로 any가 ‘아무거나’라면, unknown은 ‘아직 모름’이라 뜻! 차이는 다음과 같다. let a: unknown = 3; let b: any = 3; let c: number = 3; const unknownTest = a + c; // ERROR - TS2365: Operator '+' cannot be applied to types 'unknown' and '3'. const anyTest = b + c; // success!! any로 지정한 변수가 다른 타입인 숫자와 연산이 가능한 반면, unknown으로 지정한 변수는 숫자와 연산이 불가능하다. 즉, unknown은 어떤 타입이라도 적용할 수 있지만, 타입이 정의내려진 다른 변수 c와 함께 적용시 c의 타입에서 타입 엄격모드가 발동하며 에러를 낸다. any의 경우에는 이러한 타입 엄격모드를 마치 꺼놓은듯 typescript의 무장을 해제시키고 코드 진행이 가능하다. 따라서 디버깅을 하며 파일의 오류나 코드 흐름을 쫓을때, any보다는 unknown을 사용하는 것이 타입흐름의 통제에 있어 훨씬 낫다는 판단이 든다. ","date":"2023-01-28","objectID":"/posts/202301280423/:0:0","tags":["TypeScript"],"title":"typescript - any와 unknown의 차이","uri":"/posts/202301280423/"},{"categories":["Dev"],"content":"TypeScript 4.9에서 새로 도입된 satisfies 연산자를 소개하고, 이를 사용하여 객체의 유니온 타입을 보다 정확하게 처리하는 방법을 설명","date":"2023-01-28","objectID":"/posts/202301280422/","tags":["TypeScript"],"title":"typescript - satisfies operator 탐구","uri":"/posts/202301280422/"},{"categories":["Dev"],"content":" typescript 4.9에서 새로 나온 기능으로 satisfies 라는 연산자가 있다. 공식문서를 정말 한참을 봤는데도 잘 이해가 가지 않았는데 직접 코드를 가지고 만져보니 조금 이해가 되었다. ","date":"2023-01-28","objectID":"/posts/202301280422/:0:0","tags":["TypeScript"],"title":"typescript - satisfies operator 탐구","uri":"/posts/202301280422/"},{"categories":["Dev"],"content":"satisfies 적용 전 Record라는 객체의 key, value를 지정해주는 연산자를 사용하여 객체의 타입을 지정할 때, 아래 코드를 보면 value의 타입으로 string | RGB2 라는 union type으로 인해 속성에 대한 오류가 잡힌다. toUpperCase에서는 only string타입이 들어와야 하는데 union type으로 할당되어 생긴 에러로서 당연한 결과이다. type Colors = 'red' | 'green' | 'blue'; type RGB = [red: number, green: number, blue: number]; const palette: Record\u003cColors, string | RGB2\u003e = { red: [255, 0, 0], green: '#00ff00', blue: [0, 0, 255], }; const redComponent = palette.red.at(0); const greenNormalized = palette.green.toUpperCase(); // ^^^^^^^^^^^^^^ // error // \u003chtml\u003eTS2339: Property 'toUpperCase' does not exist on type 'string | RGB2'.\u003cbr/\u003eProperty 'toUpperCase' does not exist on type 'RGB2'. 마지막 코드의 palette.green.toUpperCase();에서 green부분을 호버해보면 다음과 같이 유니온 타입으 뜬다. 이런 경우를 위해 새로이 나온게 바로 satisfies 연산자인듯 싶다. ","date":"2023-01-28","objectID":"/posts/202301280422/:0:1","tags":["TypeScript"],"title":"typescript - satisfies operator 탐구","uri":"/posts/202301280422/"},{"categories":["Dev"],"content":"satisfies 적용 type을 똑같이 지정하고 객체에 대한 타입할당시 새 연산자를 사용하면 아래 예시와 같이 속성 연산에 있어서도 에러를 통과하게 된다. 즉, 유니온타입에서도 일부 유형을 만족한다면 해당 타입으로 추론하여 통과하게 되는듯 하다. type Colors = 'red' | 'green' | 'blue'; type RGB = [red: number, green: number, blue: number]; const palette = { red: [255, 0, 0], green: '#00ff00', blue: [0, 0, 255], } satisfies Record\u003cColors, string | RGB\u003e; const redComponent = palette.red.at(0); const greenNormalized = palette.green.toUpperCase(); // OK!! 위와 마찬가지로 palette.green.toUpperCase()에서 green부분을 호버해보면 유니온 타입이 아닌 string으로 할당되어 있음을 볼 수 있다. 객체가 아닌 경우의 유니온 타입에서도 될까 하고 몇가지 실험을 해봤는데 기본적으로 일단은 객체에서만 작동하는 듯 하다. 객체를 주로 사용하는 entities 모델에서도 잘 작동되는지 확인 후, 이상 없다면 굉장히 유용하게 쓰일 듯 싶다. ","date":"2023-01-28","objectID":"/posts/202301280422/:0:2","tags":["TypeScript"],"title":"typescript - satisfies operator 탐구","uri":"/posts/202301280422/"},{"categories":["Env"],"content":"터미널에서 폴더트리 구조를 확인할 수 있는 유틸리티 'tree'의 설치부터 사용까지 설명","date":"2023-01-23","objectID":"/posts/202301230429/","tags":["terminal"],"title":"mac 터미널에서 폴더트리 구조 확인","uri":"/posts/202301230429/"},{"categories":["Env"],"content":" windows에서는 cmd에서 폴더트리가 바로 나오는것 같던데 macOS의 터미널에서는 따로 tree를 설치해야 볼 수 있다. 아래 명령어로 brew를 통해 설치 $ brew install tree ","date":"2023-01-23","objectID":"/posts/202301230429/:0:0","tags":["terminal"],"title":"mac 터미널에서 폴더트리 구조 확인","uri":"/posts/202301230429/"},{"categories":["Env"],"content":"사용법 기본적인 사용법은 작업중인 폴더로 가서 tree 라고만 입력하면 된다. $ tree 폴더 제외 그런데 node project와 같이 node_module이라던가, dist 폴더가 있는 구조에서는 불필요하게 많은 폴더가 출력됨을 볼 수 있다. 폴더를 제외하고 싶을 땐, tree -I \u003cdiretory\u003e로 명령하면 해당 폴더를 제외하고 출력 가능하다. 예시\u003e 하나의 폴더 제외 $ tree -I node_module 예시\u003e 두 개의 폴더 제외 $ tree -I node_module -I dist // 또는 아래와 같이 $ tree -I 'node_modules|dist' 폴더만 출력 파일은 제외하고 폴더만 출력하고 싶다면 -d 를 옵션으로 사용할 수 있다. $ tree -d 파일은 제외하고 모든 폴더를 출력한다. 숨겨진 파일 출력 .env와 같은 파일처럼 .으로 시작하는 파일이나 숨겨진 파일의 경우 -a를 입력하여 출력할 수 있다. $ tree -a 실사용 예시 $ tree -a -I 'node_modules|dist|coverage|.idea|.github|.git|http-client' 이렇게 입력하면 아래와 같이 볼 수 있는데 막상 적고보니 좀… 번거롭긴 하다. . ├── .env_sample ├── .eslintrc ├── .gitignore ├── .prettierrc ├── babel.config.js ├── db │ ├── erd │ │ └── erd.dbml │ └── migrations │ └── 20230117191810_table_set_up.sql ├── env │ ├── .env.dev │ ├── .env.production │ └── .env.test ├── jest.config.js ├── package-lock.json ├── package.json ├── src │ ├── app.ts │ ├── controllers │ │ └── users.controller.ts │ ├── entities │ │ ├── category.entity.ts │ │ ├── comment.entity.ts │ │ ├── estimation.entity.ts │ │ ├── feed.entity.ts │ │ ├── feedStatus.entity.ts │ │ ├── feedSymbol.entity.ts │ │ ├── feedUploadFiles.entity.ts │ │ ├── symbol.entity.ts │ │ ├── uploadFiles.entity.ts │ │ └── users.entity.ts │ ├── main.ts │ ├── middleware │ │ └── jwt.strategy.ts │ ├── models │ │ ├── index.ts │ │ ├── repositories.ts │ │ └── users.dao.ts │ ├── routes │ │ ├── index.route.ts │ │ └── users.route.ts │ ├── services │ │ └── users.service.ts │ ├── tests │ │ ├── setup-tests.ts │ │ └── users.test.ts │ ├── types │ │ └── global.d.ts │ └── utils │ └── util.ts └── tsconfig.json ","date":"2023-01-23","objectID":"/posts/202301230429/:0:1","tags":["terminal"],"title":"mac 터미널에서 폴더트리 구조 확인","uri":"/posts/202301230429/"},{"categories":["Env"],"content":"tree options usage: tree [-acdfghilnpqrstuvxACDFJQNSUX] [-L level [-R]] [-H baseHREF] [-T title] [-o filename] [-P pattern] [-I pattern] [--gitignore] [--gitfile[=]file] [--matchdirs] [--metafirst] [--ignore-case] [--nolinks] [--hintro[=]file] [--houtro[=]file] [--inodes] [--device] [--sort[=]\u003cname\u003e] [--dirsfirst] [--filesfirst] [--filelimit #] [--si] [--du] [--prune] [--charset[=]X] [--timefmt[=]format] [--fromfile] [--fflinks] [--info] [--infofile[=]file] [--noreport] [--version] [--help] [--] [directory ...] ------- Listing options ------- -a All files are listed. -d List directories only. -l Follow symbolic links like directories. -f Print the full path prefix for each file. -x Stay on current filesystem only. -L level Descend only level directories deep. -R Rerun tree when max dir level reached. -P pattern List only those files that match the pattern given. -I pattern Do not list files that match the given pattern. --gitignore Filter by using .gitignore files. --gitfile X Explicitly read gitignore file. --ignore-case Ignore case when pattern matching. --matchdirs Include directory names in -P pattern matching. --metafirst Print meta-data at the beginning of each line. --prune Prune empty directories from the output. --info Print information about files found in .info files. --infofile X Explicitly read info file. --noreport Turn off file/directory count at end of tree listing. --charset X Use charset X for terminal/HTML and indentation line output. --filelimit # Do not descend dirs with more than # files in them. -o filename Output to file instead of stdout. ------- File options ------- -q Print non-printable characters as '?'. -N Print non-printable characters as is. -Q Quote filenames with double quotes. -p Print the protections for each file. -u Displays file owner or UID number. -g Displays file group owner or GID number. -s Print the size in bytes of each file. -h Print the size in a more human readable way. --si Like -h, but use in SI units (powers of 1000). --du Compute size of directories by their contents. -D Print the date of last modification or (-c) status change. --timefmt \u003cf\u003e Print and format time according to the format \u003cf\u003e. -F Appends '/', '=', '*', '@', '|' or '\u003e' as per ls -F. --inodes Print inode number of each file. --device Print device ID number to which each file belongs. ------- Sorting options ------- -v Sort files alphanumerically by version. -t Sort files by last modification time. -c Sort files by last status change time. -U Leave files unsorted. -r Reverse the order of the sort. --dirsfirst List directories before files (-U disables). --filesfirst List files before directories (-U disables). --sort X Select sort: name,version,size,mtime,ctime. ------- Graphics options ------- -i Do not print indentation lines. -A Print ANSI lines graphic indentation lines. -S Print with CP437 (console) graphics indentation lines. -n Turn colorization off always (-C overrides). -C Turn colorization on always. ------- XML/HTML/JSON options ------- -X Prints out an XML representation of the tree. -J Prints out an JSON representation of the tree. -H baseHREF Prints out HTML format with baseHREF as top directory. -T string Replace the default HTML title and H1 header with string. --nolinks Turn off hyperlinks in HTML output. --hintro X Use file X as the HTML intro. --houtro X Use file X as the HTML outro. ------- Input options ------- --fromfile Reads paths from files (.=stdin) --fflinks Process link informtion when using --fromfile. ------- Miscellaneous options ------- --version Print version and exit. --help Print usage and this help message and exit. -- Options processing terminator. ","date":"2023-01-23","objectID":"/posts/202301230429/:0:2","tags":["terminal"],"title":"mac 터미널에서 폴더트리 구조 확인","uri":"/posts/202301230429/"},{"categories":["Dev"],"content":"Jest를 이용한 테스트 환경에서 개발 DB와 테스트 DB를 분리하고, 환경별로 dotenv 파일을 유동적으로 적용하는 방법을 설명. cross-env 패키지를 사용하여 npm 스크립트에서 NODE_ENV 값을 설정하고, 그에 따라 각기 다른 dotenv 파일을 적용하는 과정과 TypeORM 초기화 문제를 해결하는 방법을 코드 예시와 함께 제시","date":"2023-01-23","objectID":"/posts/202301230424/","tags":["TypeScript","Express","Back-End"],"title":"백엔드 - typescript-express 환경에서 jest 및 node 환경에 따라 dotenv 설정 분리","uri":"/posts/202301230424/"},{"categories":["Dev"],"content":" ","date":"2023-01-23","objectID":"/posts/202301230424/:0:0","tags":["TypeScript","Express","Back-End"],"title":"백엔드 - typescript-express 환경에서 jest 및 node 환경에 따라 dotenv 설정 분리","uri":"/posts/202301230424/"},{"categories":["Dev"],"content":"문제상황 TDD 방법론을 따라 프로젝트를 진행하려는데 Jest를 이용하여 test 코드를 작성하던 중, 한가지 문제를 마주한다. test를 위한 DB는 그 조건에 따라 생성되고 삭제되어지는 특성상, 실제 개발환경에서의 DB와 test DB를 분리하기 위해서 local DB는 2가지로 나누어 세팅을 해야한다. 그런데 test를 실행할때마다 typeORM 세팅의 연결값을 매번 변경해줘야하는 문제가 상당히 번거로웠다. 고민하던 와중, cross-env라는 npm package를 알게되었고 이를 이용하여 상황에 따라 dotenv 설정값을 유동적으로 활용할 수 있도록 세팅을 구상해보았다. 요약. JEST의 test파일 실행시 기존 DB를 사용함으로 인한 DB내부 데이터 오염문제 test용 DB 분리로 해결 test용 DB 분리 이후, npm run 실행 환경(dev, test, production)에 따른 DB 설정문제 개발간 서버를 돌릴때, test코드를 돌릴때마다 dev.DB와 test.DB를 따로이 설정해줘야 하는 번거로움 main.ts파일에서 cross-env 패키지를 사용하더라도 JEST- test 코드는 해당 파일을 사용하지 않기에 어떻게 환경설정을 분리시킬 수 있는가에 따른 고민 ","date":"2023-01-23","objectID":"/posts/202301230424/:1:0","tags":["TypeScript","Express","Back-End"],"title":"백엔드 - typescript-express 환경에서 jest 및 node 환경에 따라 dotenv 설정 분리","uri":"/posts/202301230424/"},{"categories":["Dev"],"content":"Project Tree 우선 현재 상황에서의 project folder Tree는 다음과 같다. // project Tree ├── .env ├── jest.config.js ├── package.json ├── src │ ├── app.ts │ ├── controllers │ │ └── users.controller.ts │ ├── entities │ │ └── users.entity.ts │ ├── main.ts │ ├── middleware │ │ └── jwt.strategy.ts │ ├── models │ │ ├── index.ts │ │ ├── repositories.ts │ │ └── users.dao.ts │ ├── routes │ │ ├── index.route.ts │ │ └── users.route.ts │ ├── services │ │ └── users.service.ts │ ├── tests │ │ ├── setup-tests.ts │ │ └── users.test.ts │ ├── types │ │ └── global.d.ts │ └── utils │ └── util.ts └── tsconfig.json ","date":"2023-01-23","objectID":"/posts/202301230424/:1:1","tags":["TypeScript","Express","Back-End"],"title":"백엔드 - typescript-express 환경에서 jest 및 node 환경에 따라 dotenv 설정 분리","uri":"/posts/202301230424/"},{"categories":["Dev"],"content":"설정 고민하며 생각했던 해결방안은 다음과 같다. 일반적인 개발환경과 test를 위한 환경을 구분할 수 있는 특정값을 조건으로 할당 위에서 할당한 조건에 따라 node는 각기 다른 dotenv 파일을 참조하여 실행할 수 있도록 작성 우선 1번을 구현하기 위해 cross-env 패키지를 이용하여 조건을 할당한다. ","date":"2023-01-23","objectID":"/posts/202301230424/:2:0","tags":["TypeScript","Express","Back-End"],"title":"백엔드 - typescript-express 환경에서 jest 및 node 환경에 따라 dotenv 설정 분리","uri":"/posts/202301230424/"},{"categories":["Dev"],"content":"cross-env package 설치 $ npm i -D dotenv cross-env Tip dev [^2] 개발용을 위해 response에 따라 색상이 입혀진 축약된 로그를 출력. status값이 빨간색이면 서버 에러코드, 노란색이면 클라이언트 에러 코드, 청록색은 리다이렉션 코드, 그외 코드는 컬러가 없다. combined 배포환경에서 사용 불특정 다수가 접속하기 때문에 IP를 로그에 남겨줌 cli에서 명령어 입력시 정확한 입력에 주의한다. Warning crossenv 사건 [^1] 노드와 npm 생태계를 떠들썩하게 만든 사건이 있었다. 이름하여 ‘crossenv 사건’ 이다. 사람들이 cross-env를 설치할 때 실수로 cross-env 대신 crossenv를 설치해서 발생했는데, crossenv는 사용자의 .env 파일에 들어 있는 키들을 해커에게 전송하는 악성 패키지였던 것이었다. 다행히 문제를 발견한 즉시 패키지가 차단되어 피해가 크게 확산되지는 않았지만, 유명한 패키지를 설치하는 과정에 혼동을 야기해 해킹하려는 시도가 있었다는 점에서 충격적인 사건이었다. 따라서 패키지를 설치할 때는 항상 주의를 기울여야 한다. ","date":"2023-01-23","objectID":"/posts/202301230424/:2:1","tags":["TypeScript","Express","Back-End"],"title":"백엔드 - typescript-express 환경에서 jest 및 node 환경에 따라 dotenv 설정 분리","uri":"/posts/202301230424/"},{"categories":["Dev"],"content":"node setting package.json 파일에서 명령어를 이용해 앞서 설치하였던 cross-env 패키지에서 사용할 NODE_ENV라는 이름으로 설정값을 세팅한다. (이름은 다른걸로 해도 무방하다.) // package.json \"scripts\": { \"test\": \"cross-env NODE_ENV=test jest --runInBand --detectOpenHandles --forceExit\", \"start\": \"cross-env NODE_ENV=production node dist/main.js\", \"dev\": \"cross-env NODE_ENV=develop concurrently \\\"npx tsc --watch\\\" \\\"nodemon -q dist/main.js\\\"\", npm 명령 스크립트의 서두에 cross-env NODE_ENV=\u003cKEY\u003e 라는 방식으로 세팅을 해두면, 해당 명령이 실행될때 NODE_ENV 의 값이 앞서 언급한 조건으로서 process.env.NODE_ENV라는 dotenv의 내부 값으로 할당된다. 예시\u003e $ npm test # NODE_ENV=test 라는 값으로 세팅되고 jest가 실행된다. $ npm run dev # NODE_ENV=dev 라는 값으로 세팅되고 ts파일의 컴파일과 nodemon이 실행된다. 이렇게 스크립트단에서의 명령어에 세팅을 해두고 이제 해당 값에 따라 dotenv가 적용될 수 있게끔, 설정파일의 분리를 진행한다. ","date":"2023-01-23","objectID":"/posts/202301230424/:2:2","tags":["TypeScript","Express","Back-End"],"title":"백엔드 - typescript-express 환경에서 jest 및 node 환경에 따라 dotenv 설정 분리","uri":"/posts/202301230424/"},{"categories":["Dev"],"content":"dotenv 분리 우선 상황에 따른 dotenv 설정값을 별도로 작성한 파일을 만들어주는데, root 폴더에서 파일을 그대로 두자니 지저분한것 같아 따로이 폴더를 만들어 그 안에 정리하였다. ├── env ├── .env.dev ├── .env.production └── .env.test typrORM의 dotenv 설정을 아래 예시로 들어본다. 차이점은, TYPEORM_DATABASE명에서 test용과 개발용을 구분하였고, TYPEORM_LOGGING의 경우 test환경에서는 필요가 없기에 false로 처리하였다. 그리고 배포시에 쓰는 .env.production 파일에서는 연결시마다 scheme 자동생성을 막기 위해 TYPEORM_SYNCHRONIZE=FALSE 처리를 해둔다. 예시\u003e // env/.env.dev #typeorm TYPEORM_CONNECTION=mysql TYPEORM_HOST=127.0.0.1 TYPEORM_USERNAME=root TYPEORM_PASSWORD=1234\" TYPEORM_DATABASE=project_abc // \u003c- 여기 TYPEORM_PORT=3306 TYPEORM_LOGGING=TRUE // \u003c- 여기 TYPEORM_SYNCHRONIZE=TRUE // env/.env.test TYPEORM_CONNECTION=mysql TYPEORM_HOST=127.0.0.1 TYPEORM_USERNAME=root TYPEORM_PASSWORD=1234 TYPEORM_DATABASE=test_project_abc // \u003c- 여기 TYPEORM_PORT=3306 TYPEORM_LOGGING=FALSE // \u003c- 여기 TYPEORM_SYNCHRONIZE=TRUE 그리고 끝으로 이제 앞에서 특정한 값을 실행시킬 수 있도록 express를 실행하는 파일(대개 main.ts)에서 아래와 같이 세팅을 한다. 1 // src/main.ts import dotenv from 'dotenv'; import path from 'path'; if (process.env.NODE_ENV === 'production') { dotenv.config({ path: path.join(__dirname, '/../env/.env.production') }); } else if (process.env.NODE_ENV === 'develop') { dotenv.config({ path: path.join(__dirname, '/../env/.env.dev') }); console.log('process.env.NODE_ENV is ', process.env.NODE_ENV); } else if (process.env.NODE_ENV === 'test') { dotenv.config({ path: path.join(__dirname, '/../env/.env.test') }); } else { throw new Error('process.env.NODE_ENV IS_NOT_SET!!'); } console.log('process.env.NODE_ENV is ', process.env.NODE_ENV); dev 설정시 따로 console.log를 찍어준 이유는 개발환경에서 dev설정이 제대로 먹히고 있는지 확인차 넣어줬다. 2 ","date":"2023-01-23","objectID":"/posts/202301230424/:2:3","tags":["TypeScript","Express","Back-End"],"title":"백엔드 - typescript-express 환경에서 jest 및 node 환경에 따라 dotenv 설정 분리","uri":"/posts/202301230424/"},{"categories":["Dev"],"content":"jest 세팅 exress를 돌릴때, 만약 app.ts와 main.ts(or index.ts)등으로 나누어 놓았다면, process.env.NODE_ENV 는 main.ts에 작성되기에 jest에서는 해당 설정이 적용되지 않는다. 때문에 test 파일 실행시 test용 dotenv가 작동될 수 있도록 따로 세팅을 해주어야 한다.(파일명 확인) // 최상단 root폴더에 jest.config.js 또는 jest.config.json 파일 setupFiles: [ '\u003croot\u003e/src/tests/setup-tests.ts', ], // setup-tests.ts (파일명은 jest.config에서 설정했던 값과 동일하기만 하면 되기에 임의로 정해도 된다.) import dotenv from 'dotenv'; dotenv.config({ path: '\u003croot\u003e/env/.env.test', }); 위에서 설명한 바와 같이 세팅을 하면, 이제 jest / test파일 실행시 해당 dotenv 설정이 작동한다. typeORM.intialize()를 사용한다면 여기에 더해 실행하고자 하는 test파일 내부에서 한가지 더 코드가 필요하다. // users.test.ts const dataSource = new DataSource({ type: process.env.TYPEORM_CONNECTION, host: process.env.TYPEORM_HOST, port: process.env.TYPEORM_PORT, username: process.env.TYPEORM_USERNAME, password: process.env.TYPEORM_PASSWORD, database: process.env.TYPEORM_DATABASE, entities: [__dirname + '/../**/*.entity.{js,ts}'], logging: process.env.TYPEORM_LOGGING, synchronize: process.env.TYPEORM_SYNCHRONIZE, }); jest에서는 코드가 돌아갈때, main.ts파일을 거치지 않기때문에 typeorm의 DataSource 코드가 돌기 전 이니셜라이징할 수 있도록 따로 준비가 필요하다. 위와 같이 설정시, 앞서 설정했던 jest.config에 따라 intialize() 코드가 돌게되고 이후부터는 앞서 설명했던대로 함수와 명령들이 실행되게 된다. ","date":"2023-01-23","objectID":"/posts/202301230424/:2:4","tags":["TypeScript","Express","Back-End"],"title":"백엔드 - typescript-express 환경에서 jest 및 node 환경에 따라 dotenv 설정 분리","uri":"/posts/202301230424/"},{"categories":["Dev"],"content":"추가 : dotenv 분리로 morgan설정 역시 따로 분리 가능 //app.ts const createApp = () =\u003e { const app: Express = express(); app.use(cors(corsOptions)); if (process.env.NODE_ENV === 'develop') { app.use(morgan('dev')); // \u003c- 이렇게 } else { app.use(morgan('combined')); // \u003c- 이렇게 } app.use(express.json()); app.use(express.urlencoded({ extended: false })); app.use(router); return app; }; express에서 log를 남겨주는 morgan의 옵션중 dev와 combined 가 있는데 이를 상황에 맞게 나눌 수 있다. Tip dev [^2] 개발용을 위해 response에 따라 색상이 입혀진 축약된 로그를 출력. status값이 빨간색이면 서버 에러코드, 노란색이면 클라이언트 에러 코드, 청록색은 리다이렉션 코드, 그외 코드는 컬러가 없다. combined 배포환경에서 사용 불특정 다수가 접속하기 때문에 IP를 로그에 남겨줌 Nest.js - Deprecated - dotenv 와 cross-env 를 활용한 환경 별 환경 변수 지정하기 ↩︎ 그리고 끝까지 본문을 읽으면 알겠지만 test용 dotenv설정은 따로이 jest세팅에서 진행하기에 사실, else if (process.env.NODE_ENV === 'test') { dotenv.config({ path: path.join(__dirname, '/../env/.env.test') }); 이 부분은 필요가 없다. 하지만 추후 다른 용도에서 응용을 위한 예시로써 남겨둔다.  ↩︎ ","date":"2023-01-23","objectID":"/posts/202301230424/:3:0","tags":["TypeScript","Express","Back-End"],"title":"백엔드 - typescript-express 환경에서 jest 및 node 환경에 따라 dotenv 설정 분리","uri":"/posts/202301230424/"},{"categories":["Dev"],"content":"Node.js에서 에러 핸들링 중 return과 throw의 차이를 설명.","date":"2023-01-18","objectID":"/posts/202301180430/","tags":["JavaScript","Debugging"],"title":"error log - throw와 return","uri":"/posts/202301180430/"},{"categories":["Dev"],"content":" ","date":"2023-01-18","objectID":"/posts/202301180430/:0:0","tags":["JavaScript","Debugging"],"title":"error log - throw와 return","uri":"/posts/202301180430/"},{"categories":["Dev"],"content":"문제상황 Failure 에러핸들링 중, 에러메세지를 response로 보내려고 한다. 이에 return을 사용 에러가 나와야 할 상황에서 정상적으로 에러메세지를 response로 보낸다. 하지만 에러코드를 알려야 할 status code는 정상을 알리며 이후 모든 함수와 명령이 진행됨 ","date":"2023-01-18","objectID":"/posts/202301180430/:1:0","tags":["JavaScript","Debugging"],"title":"error log - throw와 return","uri":"/posts/202301180430/"},{"categories":["Dev"],"content":"해결 Success return이 아닌 throw를 사용하여야 한다. 특히 node.js에서 서버를 돌릴때, 단순히 return만으로는 이후의 모든 과정, 즉 controller - service - dao 등에서 return을 한 함수 이외 모든 명령어가 진행 된다. 하지만 throw의 경우, 에러를 내보내는 것 뿐만 아니라 이후의 모든 명령어를 중지시킨다. 단순히 response창만 봤다가는 정상적으로 에러를 잡는것 같지만 실제 잘못된 데이터들이 계속해서 DB에 쌓이는걸 볼 수 있다. 문법의 기본 의미를 더 살펴보게 된다. ","date":"2023-01-18","objectID":"/posts/202301180430/:2:0","tags":["JavaScript","Debugging"],"title":"error log - throw와 return","uri":"/posts/202301180430/"},{"categories":["Dev"],"content":"return1 return 명령문 은 함수 실행을 종료하고, 주어진 값을 함수 호출 지점으로 반환합니다. ","date":"2023-01-18","objectID":"/posts/202301180430/:3:0","tags":["JavaScript","Debugging"],"title":"error log - throw와 return","uri":"/posts/202301180430/"},{"categories":["Dev"],"content":"throw2 ** throw ** 문은 사용자 정의 예외를 발생(throw)할 수 있습니다. 예외가 발생하면 현재 함수의 실행이 중지되고 (throw 이후의 명령문은 실행되지 않습니다.), 제어 흐름은 콜스택의 첫 번째 catch 블록으로 전달됩니다. 호출자 함수 사이에 catch 블록이 없으면 프로그램이 종료됩니다. 얼핏 보면 둘 다 현재 함수의 실행을 중지한다는 부분에서 유사하게 보일 수 있다. 하지만 MDN 문서 중 throw 설명의 두번째 줄. Note ** throw 이후의 명령문은 실행되지 않습니다. ** 이 부분을 다시 살펴보게 된다. return - JavaScript | MDN ↩︎ throw - JavaScript | MDN ↩︎ ","date":"2023-01-18","objectID":"/posts/202301180430/:4:0","tags":["JavaScript","Debugging"],"title":"error log - throw와 return","uri":"/posts/202301180430/"},{"categories":["Dev"],"content":"TypeScript로 TypeORM을 설정할 때, dotenv에서 불러온 설정값이 TypeORM의 타입과 일치하지 않아 발생하는 타입 에러를 해결하는 방법을 설명","date":"2023-01-18","objectID":"/posts/202301180428/","tags":["TypeScript","Debugging"],"title":"error log - typeORM을 dotEnv로 연결하여 설정시 TypeScript에서의 type Error","uri":"/posts/202301180428/"},{"categories":["Dev"],"content":" ","date":"2023-01-18","objectID":"/posts/202301180428/:0:0","tags":["TypeScript","Debugging"],"title":"error log - typeORM을 dotEnv로 연결하여 설정시 TypeScript에서의 type Error","uri":"/posts/202301180428/"},{"categories":["Dev"],"content":"에러 상황 TypeScript로 typeORM을 세팅할때, dotEnv로 설정값을 주고 연결을 해보면 다음과 같은 오류를 볼 수 있다. 위 3개의 해당 error message를 살펴보면 차례로 다음과 같다. TS2322: Type 'string' is not assignable to type '\"mysql\" | \"mariadb\" | \"postgres\" | \"cockroachdb\" | \"sqlite\" | \"mssql\" | \"sap\" | \"oracle\" | \"cordova\" | \"nativescript\" | \"react-native\" | \"sqljs\" | \"mongodb\" | \"aurora-mysql\" | ... 4 more ... | \"spanner\"'. TS2322: Type 'string' is not assignable to type 'number'. TS2322: Type 'string' is not assignable to type 'LoggerOptions'. 이는 typeORM의 설정 type이 정해져있어서 그렇다. 첫번째, type의 경우 mysql' | 'mariadb' | 'postgres'... 의 경우로 type이 정해져있음을 볼 수 있다. 즉 이를 지정해주면 되는데, 조금 난해하다. ","date":"2023-01-18","objectID":"/posts/202301180428/:1:0","tags":["TypeScript","Debugging"],"title":"error log - typeORM을 dotEnv로 연결하여 설정시 TypeScript에서의 type Error","uri":"/posts/202301180428/"},{"categories":["Dev"],"content":"해결 global.d.ts라는 이름의 파일을 만들고 1 다음과 같이 세팅을 한다. declare global { namespace NodeJS { interface ProcessEnv { TYPEORM_CONNECTION: 'mysql' | 'mariadb'; TYPEORM_PORT: number; SECRET_KEY: string; TYPEORM_LOGGING: boolean; } } } export {} // \u003c- 이거 꼭 넣어줘야 한다. global로 선언하고, NodeJS에서의 dotenv 명은 ProcessEnv인가보다. 문제는 이렇게 해도 타입에러가 해결되지 않는다. 이는 밖으로 내보내기가 되지 않아서 생긴 문제로, 아무 의미 없는것 같지만 export {} 를 가장 아랫단에 꼭 넣어줘야 한다. Note 다른 파일에서 타입 정의시, declare - namespace 후, export {} 넣기!! 그러면 정상적으로 에러가 해결됨을 볼 수 있다. 중괄호 안에 아무것도 없지만 정확하게 어떻게 작동하는지는 조금 더 공부를 해봐야겠다. 파일명은 상관없다. ↩︎ ","date":"2023-01-18","objectID":"/posts/202301180428/:2:0","tags":["TypeScript","Debugging"],"title":"error log - typeORM을 dotEnv로 연결하여 설정시 TypeScript에서의 type Error","uri":"/posts/202301180428/"},{"categories":["Dev"],"content":"NestJS 프로젝트에서 CLI 설치부터 기본적인 프로젝트 설정, 모듈 생성, 컨트롤러 및 서비스 작성 방법 등을 설명. 또한 DTO, 파이프, 커스텀 파이프, 에러 핸들링, 인증 및 인가를 포함한 다양한 기능들을 세부적으로 기술","date":"2023-01-09","objectID":"/posts/202301090435/","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":" ","date":"2023-01-09","objectID":"/posts/202301090435/:0:0","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"nestJS - cli 설치 사전 조건 : node lts버전 설치 npm i @nestjs/cli 또는 npm i -g @nestjs/cli 이렇게 package.json 파일에서 정상적으로 설치되었음을 확인할 수 있다. ","date":"2023-01-09","objectID":"/posts/202301090435/:1:0","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"nestJS - new Project 시작하기 nest new \u003cnew project name\u003e // 예시 nest new nestjs-test 위와 같이 입력하면 현재 터미널이 위치한 폴더 내에 새로운 “nestjs-test\"폴더가 만들어지면서 그 안에 세팅이 된다. 만약 $ mkdir nestjs-test 로 폴더를 만들고 이 위치에서 진행중이라면, nest new ./ 위와 같이 입력시 현재 폴더명 그대로 새로운 프로젝트가 진행된다. 계속해서 진행시 아래와 같은 선택을 묻는다. npm 또는 yarn 등, 편한 패키지로 선택하면 nestJS 기본 세팅이 이루어진다. 위 이미지와 같이 prettier부터 controller-service-model, main.ts, jest 등 기본적인 구성이 모두 세팅된다. (시간이 조금 걸릴 수 있다.) ","date":"2023-01-09","objectID":"/posts/202301090435/:2:0","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"모듈 생성 nestJS에서는 nest 명령어로 모듈을 생성할 수 있다. nest g module boards 설명 nest : nest cli 사용 g : generate module : schematic that I want to create boards : name of schematic ","date":"2023-01-09","objectID":"/posts/202301090435/:3:0","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"controller, service 생성하기 nest g controller boards --no-spec nest g service boards --no-spec 한줄씩 차례로 입력하면 된다. 설명 nest : using nest cli g : generate controller : controller schematic service : service schematic boards: name of the schematic –no-spec: 테스트를 위한 소스코드 생성을 하지 않겠다! Tip 접근제한자를 이용해서 소스 간단하게하기 접근제한자(public,protected,private)을 생성자(constructor) 파라미터에 선언하면 접근제한자가 사용된 생성자파라미터는 암묵적으로 클래스 프로퍼티로 선언된다. ","date":"2023-01-09","objectID":"/posts/202301090435/:3:1","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"NestJS에서 request의 body값은 어떻게 가져올까? NodeJS에서는 req.body로 가져올 수 있다. app.post(`/`, (req,res) =\u003e { console.log(req.body); }) 하지만 NestJS에서는 @Body body를 이용해서 가져온다. 이렇게 하면 모든 request에서 보내온 값을 가져올 수 있으며, 만약, 하나씩 가져오려면 @Body('title') title 혹은 @Body('description') description 이런식으로 가져올 수 있다. @Post() createBoard(@Body() body) { console.log('body', body); } @Post() createBoard(@Body('title') title: string, @Body('description'), description: string, ) { console.log('title', title); console.log('description', description); } ","date":"2023-01-09","objectID":"/posts/202301090435/:4:0","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"NestJS에서 Param은 어떻게 가져올까? localhost:3000/:id 와 같은 params 값의 경우, body와 유사하다. 위의 경우에는 findOne(@Param('id') id: string) 과 같이 가져오면 된다. 만약 여러개의 Param을 가져와야 한다면, findOne(@Param() params: string[]) 으로 가져올 수 있다. Note @는 데코레이터라고 부른다. 타입스크립트의 데코레이터는 파이썬의 데코레이터나 자바의 어노테이션과 유사한 기능을 한다. 클래스, 메서드, 접근자, 프로퍼티, 매개변수에 적용 가능 참고 - 2.6 데코레이터 - NestJS로 배우는 백엔드 프로그래밍 ","date":"2023-01-09","objectID":"/posts/202301090435/:4:1","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"DTO (Data Transfer Object)란? 계층간 데이터 교환을 위한 객체 DB에서 데이터를 얻어 Service나 Controller 등으로 보낼 때 사용하는 객체를 말한다. DTO는 데이터가 네트워크를 통해 전송되는 방법을 정의하는 객체이다. NestJS 공식문서에서는 interface보다는 class를 이용해서 정의하는 것을 추천하고 있다. DTO(Data Transfer Object)를 쓰는 이유는? 데이터 유효성을 체크하는데 효율적 더 안정적인 코드로 만들어준다. 타입스크립트의 타입으로도 사용된다. 예시) controller, service에서 인자로 title, description이 있는 상황에서 만약 title을 다른 인자로 바꾼다면? 인자가 하나인 경우에는 크게 문제가 없겠지만 이것들이 여러개라면 모든 파일에서 인자를 다 수정해줘야 한다. 하지만 DTO를 이용한다면 이 모든 것을 하나의 class로 처리하고 해당 class만 수정하면 된다. ","date":"2023-01-09","objectID":"/posts/202301090435/:5:0","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"Pipe란?? 파이프는 @Injectable() 데코레이터로 주석이 달린 클래스이다. 파이프는 data transformation (데이터 변형)과 data validation (데이터 유효성)을 위해서 사용된다. 파이프는 컨트롤러 경로 처리기에 의해 처리되는 인수에 대해 작동한다. Nest는 메소드가 호출되기 직전에 파이프를 삽입하고 파이트는 메소드로 향하는 인수를 수신하고 이에 대해 작동한다. Data Transformation? 입력 데이터를 원하는 형식으로 변환 (예 : 문자열을 정수로) PIPE 사용하는 법 (Binding Pipes) Handler-level Pipes Parameter-level pipes Golbal-level Pipes Built-in PIpes NestJS에서 기본적으로 사용할 수 있는 6가지 파이프 ValidationPipe ParseIntPipe ParseBoolPipe ParseArrayPipe ParseUUIDPipe DefaultValuesPipe ","date":"2023-01-09","objectID":"/posts/202301090435/:5:1","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"파이프를 이용한 유효성 체크 검사 필요한 모듈 class-validator, class-transformer npm install class-validator class-transformer --save Documentation page 예시 create-board.dto.ts 파일에서 import { IsNotEmpty } from 'class-validator'; export class CreateBoardDto { @IsNotEmpty() // \u003c-- 이렇게 title: string; @IsNotEmpty() // \u003c-- 이렇게 description: string; } 먼저 위와 같이 각 인자별로 반드시 있어야 하는 곳에 @IsNotEmpty를 적용시킨다. controller 파일에서 @Post() @UsePipes(ValidationPipe) // \u003c-- 이렇게 createBoard(@Body() createBoardDto: CreateBoardDto): Board { return this.boardsService.createBoard(createBoardDto); } 다음으로 위와 같이 해당 부분에 @UsePipes(ValidationPipe)를 적용시킨다. 위 방법은 앞서 언급했던 3가지 Binding Pipes 중, Handler-level Pipes이다. @IsNotEmpty를 적용하지 않았을 경우 실제 Postman을 돌려보면 UsePipes에서 해당 함수를 찾을 수 없다는 오류가 뜨며, @UsePipes를 적용하지 않았을 경우에는 해당 유효성 검사 자체가 이루어지지 않는다. ","date":"2023-01-09","objectID":"/posts/202301090435/:5:2","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"에러를 표출해주기 위해서는 예를 들어 찾는 게시물이 없는 경우, 예외 인스턴스를 생성해서 이용할 수 있다. getBoardById(id: string): Board { const found = this.boards.find(board =\u003e board.id === id); if(!found) { throw new NotFoundException() } return found; } NotFoundException은 nestjs에 있는 인스턴스이다. (import로 사용가능) 사용시 다음과 같은 에러메세지를 볼 수 있다. { \"statusCode\": 404, \"message\": \"Not Found\" } 여기서 만약 에러메세지를 따로 넣어주고 싶다면, getBoardById(id: string): Board { const found = this.boards.find(board =\u003e board.id === id); if(!found) { throw new NotFoundException(`Can't find Board with id ${id}`) } return found; } 이렇게 하면, 아래와 같이 response 된다. { \"statusCode\": 404, \"message\": \"Can't find Board with id wefewwefewf\", \"error\": \"Not Found\" } ","date":"2023-01-09","objectID":"/posts/202301090435/:5:3","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"커스텀 파이프 구현방법 먼저 Pipe Transform이란 인터페이스를 새롭게 만들 커스텀 파이프에 구현해줘야 한다. 이 Pipe Transform 인터페이스는 모든 파이프에서 구현해줘야 하는 인터페이스이다. 그리고 이것과 함께 모든 파이프는 transform()메소드를 필요로 한다. 이 메소드는 NestJS가 인자(arjuments)를 처리하기 위해서 사용된다. import { ArgumentMetadata, PipeTransform } from \"@nestjs/common\" export class BoardStatusValidationPipe implements PipeTransform { transfomr(value: any, metadata: ArgumentMetadata) { console.log('value', value) console.log('metadata', metadata) return value; } } transform() 메소드 이 메소드는 2 개의 파라미터를 가진다. 첫번째 파라미터는 처리가 된 인자의 값(value)이며, 두번째 파라미터는 인자에 대한 메타 데이터를 포함한 객체이다. transform()메소드에서 return된 값은 Route 핸들러로 전해진다. 만약 예외(Exception)가 발생하면 클라이언트에 바로 전해진다. ","date":"2023-01-09","objectID":"/posts/202301090435/:5:4","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"mySQL 및 TypeORM ","date":"2023-01-09","objectID":"/posts/202301090435/:6:0","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"설치 typeorm mysql @nestjs/typeorm 이렇게 3가지 모듈을 설치한다. npm install --save @nestjs/typeorm typeorm mysql 참고문서 documentation ","date":"2023-01-09","objectID":"/posts/202301090435/:6:1","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"설정 src 폴더 아래 configs 폴더 생성 후 그 안에 typeorm.config.ts 파일 생성 DAO단 분리 기존 내가 알던 model폴더에서 000.dao.ts 라고 파일명을 붙였던 것과 달리, 소스 강의에서는 000.repository.ts 파일명을 사용하고 있다. 그 외에는 동일하다. ","date":"2023-01-09","objectID":"/posts/202301090435/:6:2","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"게시물 삭제 remove()? vs delete()? remove 무조건 존재하는 아이템을 remove 메소드를 이용해서 지워야 한다. 그렇지 않으면 에러 발생(404 error) delete 만약 아이템이 존재하면 지우고 존재하지 않으면 아무런 영향이 없다. 이러한 차이로 remove를 이용하면 하나의 아이템을 지울 때 두번 데이터베이스를 이용해야 하기 때문에 (아이템 유무 + 지우기) 소스강의에서는 데이터베이스에 한번만 접근해도 되는 delete 메소드를 사용한다. ","date":"2023-01-09","objectID":"/posts/202301090435/:6:3","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"Error Handling 에러를 잡아내고 싶다면 우선, try { await this.save(user) // \u003c- 이건 사용하고자 하는 코드 } catch(error) { console.log('error', error); } 를 이용하여 에러를 확인할 수 있다. http에서 해당 API를 돌려보면, 아래와 같은 로그를 터미널에서 확인할 수 있다. code: 'ER_DUP_ENTRY', errno: 1062, sqlMessage: \"Duplicate entry 'test' for key 'user.IDX_78a916df40e02a9deb1c4b75ed'\", sqlState: '23000', index: 0, sql: \"INSERT INTO `user`(`id`, `username`, `password`) VALUES (DEFAULT, 'test', 'te171')\" } 해당 error code 확인 후, 다시 아래와 같은 방식으로 수정하면 정상적으로 잡고자 하는 에러를 잡아낼 수 있다. try { await this.save(user); } catch (error) { if (error.code === 'ER_DUP_ENTRY') { throw new ConflictException('Existing username'); } else { throw new InternalServerErrorException(); } } 코드 수정 후, http Request 실행시 HTTP/1.1 409 Conflict X-Powered-By: Express Content-Type: application/json; charset=utf-8 Content-Length: 67 ETag: W/\"43-bvU5hJ7vN5ptmGJpSCIhkVNclIU\" Date: Mon, 16 Jan 2023 06:19:56 GMT Connection: keep-alive Keep-Alive: timeout=5 { \"statusCode\": 409, \"message\": \"Existing username\", \"error\": \"Conflict\" } statusCode 409부터 정상적으로 송출하고자 하는 에러 메세지까지 모두 이상없음을 확인가능하다. ","date":"2023-01-09","objectID":"/posts/202301090435/:7:0","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"인증 및 인가 - bycriptjs ","date":"2023-01-09","objectID":"/posts/202301090435/:8:0","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"hashedPassword 생성 및 검증 우선 인증 인가 부분에 필요한 bcrypt를 설치한다. npm install bcryptjs --save import는 다음과 같다. import * as bcrypt from 'bcryptjs'; 회원가입시, bcrypt로 hashedPassword 생성 //user.repository.ts @CustomRepository(User) export class UserRepository extends Repository\u003cUser\u003e { async createUser(authCredentialsDto: AuthCredentialsDto): Promise\u003cvoid\u003e { const { username, password } = authCredentialsDto; const salt = await bcrypt.genSalt(); // bcrypt가 들어간 코드 const hashedPassword = await bcrypt.hash(password, salt); // bcrypt가 들어간 코드 const user = this.create({ username, password: hashedPassword }); // bcrypt가 들어간 코드 try { await this.save(user); } catch (error) { if (error.code === 'ER_DUP_ENTRY') { throw new ConflictException('Existing username'); } else { throw new InternalServerErrorException(); } } } } 로그인시, hashedPassword를 비교하여 검증 ( bcrypt.compare) // user.service.ts async signIn(authCredentialsDto: AuthCredentialsDto): Promise\u003cstring\u003e { const { username, password } = authCredentialsDto; const user = await this.userRepository.findOneBy({ username }); if (user \u0026\u0026 (await bcrypt.compare(password, user.password))) { return 'login success'; } else { throw new UnauthorizedException('login failed'); } } ","date":"2023-01-09","objectID":"/posts/202301090435/:8:1","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"jwt token 생성 필요 모듈 @nestjs/jwt : nestjs에서 jwt를 사용하기 위해 필요한 모듈 @nestjs/passport : JWT를 이용하여 인증 처리하는 등의 과정을 훨씬 쉽게 만들어주는 모듈 npm install @nestjs/jwt @nestjs/passport passport passport-jwt --save 위 터미널 명령어와 같이 총 4개의 모듈을 설치한다. @nestjs/jwt @nestjs/passport passport passport-jwt passport 사용 Passport 모듈이란? 토큰이 유효한 토큰인지 서버에서 secret text를 이용해서 알아내면 payload 안에 유저 이름을 이용해서, 데이터베이스 안에 있는 유저 이름에 해당하는 유저 정보를 모두 가져올 수 있다. 이러한 처리를 쉽게 해주는게 Passport 모듈이다. –\u003e 쉽게 말해 로그인 이후, 토큰 확인 과정을 말한다!!! npm install @types/passport-jwt --save Note nestJS의 미들웨어 Pipes : 파이프는 요청 유효성 검사 및 페이로드 변환을 위해 만들어진다. 데이터를 예상한 대로 직렬화 한다. Filters : 필터는 오류 처리 미들웨어이다. 특정 오류 처리기를 사용할 경로와 각 경로 주변의 복잡성을 관리하는 방법을 알 수 있다. Guards : 가드는 인증 미들웨어. 지정된 경로로 통과할 수 있는 사람과 허용되지 않는 사람을 서버에 알려준다. Interceptors : 인터셉터는 응답 매핑 및 캐시 관리와 함께 요청 로깅과 같은 전후 미들웨어. 각 요청 전후에 이를 실행하는 기능은 매우 강력하고 유용하다. Tip 각각의 미들웨어가 불러지는(called) 순서 middleware -\u003e guard -\u003e interceptor (before) -\u003e pipe -\u003e controller -\u003e service -\u003e controller -\u003e interceptor (after) -\u003e filter (if applicable) -\u003e client UseGuards UseGuards 안에 @nestjs/passport에서 가져온 AuthGuard()를 이용하면 요청안에 유저 정보를 넣어준 수 있다. @Post('/authTest') @UseGuards(AuthGuard()) authTest(@Req() req) { console.log(req) } UseGuards가 아닌 바로 객체에 접근하려면? 커스텀 데코레이터를 생성하여 접근할 수 있다. import { createParamDecorator, ExcutionContext } from '@nestjs/common' import { User } from './user.entity' export const GetUser = createParamDecorator((data, ctx: ExcutionContext): User =\u003e { const req = ctx.switchToHttp().getRequest() return req.user }) 위 예시는, UseGuards를 사용하고 그 안에 있는 정보를 사용하는 방법이기에 UseGuards가 필요하다. ","date":"2023-01-09","objectID":"/posts/202301090435/:8:2","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"인증된 유저만 게시물을 사용할 수 있게 하기 우선 auth 모듈의 인증인가 부분을 boards 모듈에서도 사용할 수 있게 한다. // boards.module.ts @Module({ imports: [ TypeOrmExModule.forCustomRepository([BoardRepository]), AuthModule // \u003c- 추가 ], controllers: [BoardsController], providers: [BoardsService], }) //boards.controller.ts @Controller('boards') @UseGuards(AuthGuard()) // \u003c- 추가 컨트롤러 boards 아래 넣게되면 미들웨어처럼 작동하여, 모든 boards API는 토큰을 필요로 하게 된다. ","date":"2023-01-09","objectID":"/posts/202301090435/:8:3","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"유저와 게시물 데이터의 관계 형성 사용자 1명은 여러개의 게시물을 작성할 수 있다. 때문에 OneToMany Relationship, ManyToOne Relationship의 성격을 동시에 가지는데, User Entity, Board Entity에서 각각 설정할 수 있다. //user.entity.ts @OneToMany(type =\u003e Board, board =\u003e board.user, { eager: true }) boards: Board[] //board.entity.ts @ManyToOne((type) =\u003e User, (user) =\u003e user.boards, { eager: false }) user: User; Tip option: eager true 일때는 user정보를 가져올 때, board도 같이 가져옴 ","date":"2023-01-09","objectID":"/posts/202301090435/:8:4","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"Log 로그의 종류 Log - 중요한 정보의 범용 로깅 Warning - 치명적이거나 파괴적이지 않은 처리되지 않은 문제 Error - 치명적이거나 파괴적인 처리되지 않은 문제 Debug - 개발자용 / 오류 발생시 로직을 디버그하는 데 도움이되는 유용한 정보입니다. Verbose - 응용자용 / 응용 프로그램의 동작에 대한 통찰력을 제공하는 정보입니다. 예시 // main.ts import { NestFactory } from '@nestjs/core'; // \u003c- this import { AppModule } from './app.module'; import { Logger } from '@nestjs/common'; async function bootstrap() { const app = await NestFactory.create(AppModule); const port = 3000; await app.listen(port); Logger.log(`Application running on port ${port}`); // \u003c- this } bootstrap(); // board.controller.ts @Get() getAllBoard(@GetUser() user: User): Promise\u003cBoard[]\u003e { this.logger.verbose(`User ${user.username} trying to get all boards`); return this.boardsService.getAllBoards(user); } ","date":"2023-01-09","objectID":"/posts/202301090435/:9:0","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Dev"],"content":"설정 .env 파일 사용 관련 필요 모듈 설치 npm install config --save // 또는 npm install dotenv --save ","date":"2023-01-09","objectID":"/posts/202301090435/:10:0","tags":["NestJS","Back-End"],"title":"nestJS 시작하기","uri":"/posts/202301090435/"},{"categories":["Misc"],"content":"유튜브 영상 \"부트캠프 가셔도 실패하는 분 특징\" 시청후 느낀 점 정리","date":"2023-01-02","objectID":"/posts/202301020441/","tags":["Misc"],"title":"부트캠프 가도 실패하는 사람 특징","uri":"/posts/202301020441/"},{"categories":["Misc"],"content":" youtube 서핑을 하다 우연히 보게 된 영상. “부트캠프 가셔도 실패하는 분 특징” 문구가 굉장히 도발적이었다. 마침 부트캠프를 막 수료한지라 더욱 내 시선을 끌었었다.   youtube 내용 요약 - 펼치기 ","date":"2023-01-02","objectID":"/posts/202301020441/:0:0","tags":["Misc"],"title":"부트캠프 가도 실패하는 사람 특징","uri":"/posts/202301020441/"},{"categories":["Misc"],"content":"youtube 내용 요약 글쓰기를 잘하는 사람이 개발자로서도 잘한다. 본인의 메세지를 글에 녹일 수 있어야 한다. 컴퓨터가 이해할 수 있는 코드는 어느 바보라도 작성할 수 있다. 좋은 프로그래머는 사람이 이해할 수 있는 코드를 작성한다. 마틴 파울러, 리팩토링의 저자 코딩 자체가 문서를 정리하는 것이다. 때문에 중요한건 사고력과 논리력이다. 글을 잘 쓰면 기본적으로 이 2가지가 탑재되어 있다. 따라서 글쓰기를 잘한다면, 코드를 잘 작성할 수 있게 될 뿐만 아니라, 다른 사람과의 원활한 커뮤니케이션, 개발자 커리어에서 좋은 선택을 하는 것까지 가능하게 된다. ","date":"2023-01-02","objectID":"/posts/202301020441/:1:0","tags":["Misc"],"title":"부트캠프 가도 실패하는 사람 특징","uri":"/posts/202301020441/"},{"categories":["Misc"],"content":"이것부터 시작하면 반드시 개발자로 롱런 가능하다. 만약 당신이 이미 코드에서 프로그래밍적 기술에 대한 이해도가 높은 편이다? 코드에서 목적성을 갖고 짜는 연습을 하는 것이 베스트 기술적인 이해도가 낮다면? 프로그래밍적 기술보다 자연어!, 즉 일반적인 글로써 목적성이 있는 글 쓰기를 연습! 그렇다면, 목적성이 있는 글쓰기는 우선 무엇부터 쓸 수 있을까? 단순히 배운 것을 정리한다? =\u003e X 내가 다음에 00을 까먹었을때 나에게 다시 00을 가르칠 수 있도록 글을 써보겠다. =\u003e O 2번과 같은 구체적인 목적성이 있는 글이 좋다. 때문에 혼자 메모장에 쓰기보다는, 이 글을 나 이외에도 다른 누군가가 볼 수 있도록 블로그에 쓰는 것을 추천한다. 사실 이 영상과는 별개로 늘 WIL 블로깅을 하면서, 이 글이 과연 무슨 이점이 있을까? 나중에 내가 이 글을 볼때 나조차도 알아볼 수 없는 단순 메모에 불과하다면 이 글의 목적은 과연 무엇이었을까? 목적이 없는 글을 쓰는건 시간낭비가 아닐까? 라는 생각을 종종 하곤 했다. 부트캠프에서는 꾸준히 TIL(Today I leaned)을 쓰라고 했다. 일단 배운걸 그저 메모하듯이라도 쓰라고 했다. 아마 코딩을 처음 배우는 입장에서 목적성을 가진 글까지 쓰라고 하는건 무리였을까? 내가 배운 것을 정리한다. 그리고 시간이 지난 후, 이 글을 또 다른 누군가 보더라도 유의미한 글이 되게끔 글을 쓴다. 생각보다 1번은 쉽지만, 2번은 그렇지 않다. 대개 늘 2번에서 멈춘다. 마지막 탈고까지 들어가는 시간과 노력이 굉장히 많이 들기 때문이다. 나 역시도 현재, 초안으로 작성해놓고 탈고하지 못한 글만 해도 20여개가 넘는다. 하지만, 비록 시간과 품이 많이 들더라도 무의미한 글을 쓰는 것보다 훨씬 생산적이고, 글을 쓰는 동안 글의 목적, 효율적인 전달을 위한 개요, 구조, 문법, 어휘 등등을 끊임없이 생각하며 작성해나가는 것, 바로 그 자체가 또 다른 코딩 훈련이 아닐까? 사실 생각해보면 글쓰기와 코딩은 굉장히 비슷한 점이 많다. 하나의 API를 설계하기 위해 전체적인 그림을 그리고(주제 선정), 세부적인 기능을 배치하고(개요짜기), 기능을 개발하며 코드를 짜나간다(살 붙이기). 그리고 오류나 버그를 수정한다. (최종 탈고). 코딩 글쓰기 API 선정 주제 선정 기능 배치 및 설계 개요 짜기 기능 개발 살 불이기 디버깅 및 오류 수정 최종 탈고 유튜브 저자는 바로 이러한 점에서, 논리력과 사고력을 강조하며 글쓰기와 코딩을 함께 말하지 않았나 생각한다. ","date":"2023-01-02","objectID":"/posts/202301020441/:1:1","tags":["Misc"],"title":"부트캠프 가도 실패하는 사람 특징","uri":"/posts/202301020441/"},{"categories":["Dev"],"content":"AWS EC2에서 발생하는 퍼블릭 IP 변경 문제와 이를 해결하기 위한 탄력적 IP(EIP) 설정 방법에 대해 설명. EC2 인스턴스 재부팅 시 퍼블릭 IP가 변경되는 이유 및 문제점, 그리고 퍼블릭 IP를 고정하기 위해 탄력적 IP를 사용하는 방법과 비용 관련 사항 안내","date":"2022-12-28","objectID":"/posts/202212280458/","tags":["AWS","Back-End"],"title":"AWS 퍼블릭 IP와 탄력적 IP","uri":"/posts/202212280458/"},{"categories":["Dev"],"content":" AWS EC2에서 백엔드 서버와 프론트엔드 서버를 돌리는 와중, 백엔드 서버의 PM2 리스트 전체가 다운이 되면서 멈추는 일이 발생했다. 이래저래 복구를 하다가, 프리티어 t2의 메모리 부족 현상때문이라는 결론에 다달았고 ec2를 재부팅하였다. ","date":"2022-12-28","objectID":"/posts/202212280458/:0:0","tags":["AWS","Back-End"],"title":"AWS 퍼블릭 IP와 탄력적 IP","uri":"/posts/202212280458/"},{"categories":["Dev"],"content":"AWS 퍼블릭 IP 주소가 바뀌었다? EC2 재부팅 결과, 서버의 퍼블릭 IPv4 주소가 바뀌는 일이 발생했다. 왜 그런가 찾아보니 AWS 기술문서에 다음과 같은 안내가 있었다. ","date":"2022-12-28","objectID":"/posts/202212280458/:1:0","tags":["AWS","Back-End"],"title":"AWS 퍼블릭 IP와 탄력적 IP","uri":"/posts/202212280458/"},{"categories":["Dev"],"content":"퍼블릭 IP 주소1 다음과 같은 경우 인스턴스의 퍼블릭 IP 주소가 변경됩니다. 중지되었거나 최대 절전 모드로 전환된 인스턴스는 다시 시작될 때 새 퍼블릭 IP 주소를 받습니다. 인스턴스의 연결된 엘라스틱 IP 주소가 인스턴스에서 연결 해제되는 경우, 새 퍼블릭 IP 주소를 받습니다. 퍼블릭 IP 주소가 더 이상 인스턴스와 연결되지 않으면 해당 주소는 퍼블릭 IP 풀로 돌아갑니다. 중지된 인스턴스에 대해 동일한 퍼블릭 IP 주소를 복구할 수 없습니다. 문제는 바뀐 주소를 프론트엔드 코드에서 다시 설정 해줘야 한다는 부분이었다. 다행히 프론트엔드 코드에서 백엔드 서버 주소는 .env 또는 config 파일로 변수처리를 해줬기때문에 간단한 작업만으로 가능했지만, 절대주소처럼 돌아가야 할 서버 주소가 앞으로도 언제든 바뀔 수 있다는 점은 그 때마다 프론트의 코드를 만져줘야 한다는 부분에서 큰 불편요소로 다가왔다. ","date":"2022-12-28","objectID":"/posts/202212280458/:1:1","tags":["AWS","Back-End"],"title":"AWS 퍼블릭 IP와 탄력적 IP","uri":"/posts/202212280458/"},{"categories":["Dev"],"content":"AWS 퍼블릭 IP주소를 고정값으로 쓰려면?! ","date":"2022-12-28","objectID":"/posts/202212280458/:2:0","tags":["AWS","Back-End"],"title":"AWS 퍼블릭 IP와 탄력적 IP","uri":"/posts/202212280458/"},{"categories":["Dev"],"content":"탄력적 IP 주소1 탄력적 IP 주소를 사용하는 것이 가장 좋습니다. 탄력적 IP 주소는 인스턴스가 아닌 계정에 할당됩니다. 필요에 따라 탄력적 IP 주소를 인스턴스와 연결할 수 있습니다. 계정에 할당된 탄력적 IP 주소는 해제 시 복구할 수 있습니다. 자세한 내용은 탄력적 IP 주소 복구를 참조하십시오. 모든 인스턴스는 중지될 때 연결된 탄력적 IP 주소를 유지합니다. AWS는 중지된 인스턴스와 연결된 탄력적 IP 주소에 대한 요금을 계속 청구합니다. 참고: 자동 할당된 동적 퍼블릭 IP 주소를 탄력적 IP 주소로 변환할 수 없습니다. ","date":"2022-12-28","objectID":"/posts/202212280458/:2:1","tags":["AWS","Back-End"],"title":"AWS 퍼블릭 IP와 탄력적 IP","uri":"/posts/202212280458/"},{"categories":["Dev"],"content":"탄력적 IP의 비용은 어떻게 될까? ","date":"2022-12-28","objectID":"/posts/202212280458/:3:0","tags":["AWS","Back-End"],"title":"AWS 퍼블릭 IP와 탄력적 IP","uri":"/posts/202212280458/"},{"categories":["Dev"],"content":"탄력적 IP 주소 요금 부과 2 실행 중인 인스턴스에 연결된 엘라스틱 IP 주소(EIP) 한 개는 무료로 사용할 수 있습니다. 해당 인스턴스에 추가 EIP를 연결하면 인스턴스와 연결된 추가 EIP당 시간에 비례하여 요금이 부과됩니다. 추가 EIP는 Amazon VPC에서만 사용할 수 있습니다. 탄력적 IP 주소의 효율적인 사용을 보장하기 위해 IP 주소가 실행 중인 인스턴스와 연결되어 있지 않거나, 중지된 인스턴스 또는 분리된 네트워크 인터페이스와 연결되어 있는 경우 소액의 시간당 요금이 부과됩니다. Bring Your Own IP를 사용하여 AWS로 가져온 IP 주소 접두사에서 생성하는 탄력적 IP 주소에 대해서는 요금이 부과되지 않습니다. AWS를 사용중이라면 1개까지는 무료로 사용가능하지만, 주의할 점은 탄력적 IP 주소를 생성 후, **사용하지 않는다면!! 요금이 부과된다는 점이다. ** (AWS 문서를 찾아보면 ip주소 점유 방지를 위함이라고 한다.) 탄력적 IP 생성과 사용 적용 및 삭제는 아래 블로그에서 아주 잘 설명되어있다. AWS - EIP(탄력적 IP) 개념 \u0026 사용 세팅 정리 EC2 인스턴스와 연결된 퍼블릭 또는 탄력적 IP 주소 복구 ↩︎ ↩︎ EC2 온디맨드 인스턴스 요금 – Amazon Web Services ↩︎ ","date":"2022-12-28","objectID":"/posts/202212280458/:3:1","tags":["AWS","Back-End"],"title":"AWS 퍼블릭 IP와 탄력적 IP","uri":"/posts/202212280458/"},{"categories":["Projects"],"content":"FastFive 프리온보딩 기업과제 참여 및 진행간 회고 기록","date":"2022-12-27","objectID":"/posts/202212270448/","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Projects"],"content":" ","date":"2022-12-27","objectID":"/posts/202212270448/:0:0","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Projects"],"content":"FastFive 프리온보딩 기업과제 ","date":"2022-12-27","objectID":"/posts/202212270448/:1:0","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Projects"],"content":"과제 진행간 개요 ","date":"2022-12-27","objectID":"/posts/202212270448/:2:0","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Projects"],"content":"기간 프로젝트 수행 기간 : 2022.12.12. Mon ~ 2022. 12.22. Thu (11일간) 최초 정식 코드 배포일 : 2022.12.22. Thu 프로젝트 보수 기간 : 2022.12.23. Fri ~ 2022.12.27. Tue (5일간) ","date":"2022-12-27","objectID":"/posts/202212270448/:2:1","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Projects"],"content":"인원 프론트엔드 3명, 백엔드 2명 ","date":"2022-12-27","objectID":"/posts/202212270448/:2:2","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Projects"],"content":"백엔드 기술스택 JavaScript, TypeScript, TYPEORM, mySQL, Node, Express ","date":"2022-12-27","objectID":"/posts/202212270448/:2:3","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Projects"],"content":"개발환경 OS : macOS 개발 tool : WebStrom, DBeaver, Postman 형상관리 : GitHub 문서관리 및 티켓관리 : Notion 협업 툴 : Slack ","date":"2022-12-27","objectID":"/posts/202212270448/:2:4","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Projects"],"content":"백엔드 기능분담 ERD 분석 및 작성 : 송인찬, 김**님 DBmate 세팅 : 송인찬 User API : 송인찬 category API : 송인찬 feedList API : 송인찬 reply API : 송인찬 posting API : 김**님 임시저장 posting: 김**님 임시저장 수정 posting: 김**님 정식저장 posting: 김**님 delete posting API : 송인찬 ","date":"2022-12-27","objectID":"/posts/202212270448/:2:5","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Projects"],"content":"결과 github 주소 ERD 구성 백엔드 API Documentation 배포 사이트 ","date":"2022-12-27","objectID":"/posts/202212270448/:2:6","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Projects"],"content":"최초 문서 분석 이번 프로젝트는 typescript라는 새로 배운 언어를 도입하여 진행하기로 하면서, 처음엔 아니었으나 프로젝트 종료시점에서 어쩌다 보니 내가 맡은 파트가 꽤… 많아졌다. 패스트파이브에서 준 개발 의뢰문서를 처음 봤을 때, 상세등록과 덧글 부분이 가장 난이도가 높아보였다. 난이도가 높다고 느껴진 부분은 다음과 같다. 상세등록 임시저장 : 1분마다 자동저장 이미지 업로드 (작성시점에 이미지 등록시 미리보기가 가능해야 함) 파일 업로드 (파일명과 파일 링크를 반환해야 함) 덧글 대댓글 구조 비공개 댓글의 경우 로그인 사용자별 내용 공개처리 ","date":"2022-12-27","objectID":"/posts/202212270448/:3:0","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Projects"],"content":"User API 작업을 시작하다 사실 개발문서에는 직접적인 user API 관련 언급은 없었다. 하지만 로그인한 사용자별 등급이 존재했었고, 이에 따른 커뮤니티 이용별 권한이 따로이 나눠져 있었다. 또한 최종적으로 website 링크를 배포 했을때 이러한 권한을 보여주려면 user API가 무조건 있어야 한다고 판단했다. 따라서 회원가입과 백엔드측에서 서버 세팅시 필요하게끔 약식으로 작성을 하였고, 로그인은 프론트엔드가 API 통신 테스트를 하는데 크게 문제되지 않도록 실제와는 달리 의도적으로 에러핸들링을 최소한으로 하였다. 회원권한조회 API user API 중 유일하게 가장 유용하면서도 의미있으며 필수적인 API로써, 로그인한 회원의 정보를 실시간으로 클라이언트(프론트엔드)에서 확인할 수 있도록 설계를 했다. \"user_id\": 9, \"write_permission\": true, \"is_admin\": true, \"member_type\": \"입주예정자\", \"company_name\": \"세탁 상태\", \"group_id\": 8, \"nickname\": \"reservation1\", \"email\": \"reservation1@test.com\", \"position_name\": \"대표\", \"start_date\": \"2023년 03월 01일\", \"end_date\": \"2023년 12월 31일\", \"period\": \"총 306일 계약\", \"group_feed_exist\": true, \"feed_id\": 8 해당 API는 위와 같은 응답 json을 반환한다. 로그인시, 위 정보를 한번에 줄수도 있겠지만 다음과 같은 이유로 따로 API를 분리시켰다. localStorage에는 최소한의 정보만이 암호화 된채 담겨야 한다. 회원의 권한과 관련된 정보는 localStorage에 저장되지 않는 것이 보안상 유리하다. API를 분리하여 사용함으로써 실시간으로 변경되는 권한 내역까지 잡아낼 수 있다. 실제 mySQL의 테이블에는 group의 start_date와 end_date만이 담기지만, 이를 바탕으로 model단에서 member_type(멤버 등급)과, write_permission(글쓰기 가능 조건, is_admin 권한이 있더라도 퇴주기업은 글쓰기 불가하기 때문), 그리고 period(해당 업체의 계약기간)를 담아 보낼 수 있도록 하였다. 또한, 회사의 멤버가 “우리회사 소개하기\"라는 버튼을 누를 때, 클라이언트가 회사의 글이 있으면 해당 게시글로 이동하고, 없다면 글쓰기 API로 이동시킬 수 있도록 group_feed_exist, feed_id를 찾아 반환할 수 있도록 하였다. sql where 절의 변수에서는 중간 탈취를 막기 위해 query injection처리를 하였다. mySQL query 펼치기 SELECT u.id AS user_id, (CASE WHEN date (ug.end_date) \u003e= date (now()) AND u.is_admin = TRUE THEN TRUE ELSE FALSE END ) AS write_permission, u.is_admin, (CASE WHEN date (ug.start_date) \u003c= date (now()) AND date (ug.end_date) \u003e= date (now()) THEN '입주자' WHEN date (ug.end_date) \u003c date (now()) THEN '퇴주자' WHEN date (ug.start_date) \u003e date (now()) THEN '입주예정자' ELSE '일반가입자' END ) AS member_type, ug.company_name, u.group_id, u.nickname, u.email, u.position_name, DATE_FORMAT(ug.start_date, '%Y년 %m월 %d일') AS start_date, DATE_FORMAT(ug.end_date, '%Y년 %m월 %d일') AS end_date, concat(\"총 \", TIMESTAMPDIFF(DAY, ug.start_date, ug.end_date) + 1, \"일 계약\") AS period FROM users u INNER JOIN user_group ug ON ug.id = u.group_id WHERE u.id = ? `, [userId] ","date":"2022-12-27","objectID":"/posts/202212270448/:4:0","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Projects"],"content":"가장 많이 고민한 reply API 댓글 테이블은 다음과 같은 구조를 가졌다. ","date":"2022-12-27","objectID":"/posts/202212270448/:5:0","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Projects"],"content":"대댓글은 자기참조 구조 활용 여기서 난 대댓글의 구조에서 parent_reply_id를 같은 테이블의 id값으로 잡는, 즉 자기참조 구조를 사용하였다. 최초에는 parent_reply_id에다 같은 테이블의 id값을 foreign key로 잡았었는데, 이 fk를 해제한 이유는 아이러니하게도 검색효율성 때문이었다. 1 isNull 함수 검색보다 const 상수검색으로 검색효율성 증대 이에 null일때, 0으로 입력되도록 default 값을 세팅 대댓글 그룹핑시 불필요한 함수 사용 제거 가능 read replies by feed API - mySQL query 펼치기 SELECT t2.id AS reply_id, t2.feed_id, u2.id AS feed_user_id, t2.is_private, t2.is_deleted, t2.comment, t2.parent_reply_id, u3.id AS parent_user_id, t2.reply_group, t2.rnk, u.id AS reply_user_id, ug.company_name, u.nickname, u.email, u.position_name, u.is_admin, CASE WHEN instr(DATE_FORMAT(t2.created_at, '%Y년 %m월 %d일 %p %h:%i'), 'PM') \u003e 0 THEN REPLACE(DATE_FORMAT(t2.created_at, '%Y년 %m월 %d일 %p %h:%i'), 'PM', '오후') ELSE REPLACE(DATE_FORMAT(t2.created_at, '%Y년 %m월 %d일 %p %h:%i'), 'AM', '오전') END AS created_at FROM ( SELECT * FROM ( SELECT r.id, r.is_private, r.is_deleted, r.user_id, r.feed_id, r.comment, r.parent_reply_id, r.created_at, RANK() OVER (PARTITION BY parent_reply_id ORDER BY id ASC ) AS rnk, r.parent_reply_id AS reply_group FROM replies r WHERE feed_id = 1 AND parent_reply_id) AS t1 UNION ALL ( SELECT r2.id, r2.is_private, r2.is_deleted, r2.user_id, r2.feed_id, r2.comment, r2.parent_reply_id, r2.created_at, r2.parent_reply_id AS rnk, r2.id AS reply_group FROM replies r2 WHERE r2.feed_id = 1 AND r2.parent_reply_id = 0 ORDER BY r2.id ASC ) ) AS t2 INNER JOIN users u ON t2.user_id = u.id LEFT JOIN replies r3 ON r3.id = t2.parent_reply_id LEFT JOIN users u3 ON u3.id = r3.user_id LEFT JOIN user_group ug ON ug.id = u.group_id LEFT JOIN feeds f ON f.id = t2.feed_id LEFT JOIN users u2 ON f.user_id = u2.id ORDER BY reply_group, rnk sql의 결과를 보기좋게 재가공!! read replies by feed API - typescript code 펼치기 .then(value =\u003e { value.map((e: any) =\u003e { if (value[0].parent_reply_id !== 0) { let temporary = { reply_id: e.parent_reply_id, parent_reply_id: 0, is_fake: true, }; value.unshift(temporary); } return value; }); value = [...value].map(item =\u003e { return { ...item, is_private: item.is_private === 1, is_deleted: item.is_deleted === 1, comment: item.comment === '0' ? false : item.comment, reply: [], }; }); value .filter( (e: any) =\u003e e.is_private === true \u0026\u0026 (e.reply_user_id || e.parent_user_id || e.feed_user_id) !== userId ) .map((e: any) =\u003e { e.comment = false; return e; }); value .filter((e: any) =\u003e e.parent_reply_id !== 0) .forEach((e: any) =\u003e value .find((re: any) =\u003e re.reply_id === e.parent_reply_id) .reply.push(e) ); return value.filter((e: any) =\u003e e.parent_reply_id === 0); }); 이리하여 나온 멋진 res.json은 아래 이미지와 같다. 풀이 댓글과 대댓글의 depth를 나눈 테이블구조가 아닌 자기참조 구조는 무한대댓글 구조를 구성할 수 있는 장점이 있다. 페이지네이션에서 댓글과 대댓글을 포함한 개수를 실행할 수 있도록 db에서는 대댓글을 분리하지 않은채 limit에 따라 댓글을 가져온다 비공개 댓글이나 삭제된 댓글은 로그인 유저의 정보에 따라 여기서 바로 내용을 false로 전환시킨다. (애시당초 클라이언트 쪽으로 데이터가 나가지 않도록 원천 차단) 페이지네이션으로 인한 n페이지의 최초 댓글이 대댓글일 경우, 원댓글 속에 들어가는 json 구조를 유지하기 위해 가상의 원댓글을 만들어주었다. 끝으로 대댓글은 댓글 속 reply라는 배열 안으로 모두 집어넣고, 최초의 json depth 단계에서는 원댓글들만 남게 함으로써, 클라이언트에서 구분하기 쉽게 response를 재가공하였다. 뿐만 아니라 이러한 구조에서 대댓글이 소속되는 원댓글에 대한 정보를 클라이언트에서 보다쉽게 구분할 수 있도록 reply_group이라는 컬럼을 추가생성되게 함으로써 원댓글 속으로 다시 한번 묶어주는 조건값을 생성해주고, 이 안에서 대댓글의 순서 정렬을 위해 rnk라는 컬럼을 추가로 만들었다. 프론트엔드에서는 왠만한 상황에서도 컴포넌트를 유지하는데 최대한 불편함 없이, 조건의 오류 없이 json데이터를 가져올 수 있도록 고민하고 설계하였다. 이 로직을 고민하는데 완전히 구현하는데 대략 이틀이나 걸렸다. ","date":"2022-12-27","objectID":"/posts/202212270448/:5:1","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Projects"],"content":"그외 기타 고민의 의사결정 과정들 하위 카테고리 설정 처음 카테고리를 설정할땐 직무 위주로 찾고 있었는데 문득, 패스트파이브는 B2B 업종이라는 걸 간과했다는 사실을 발견했다. 즉, 카테고리는 직무의 나열이 아니, 업종의 나열이 되어야 한다고 판단했다. 즉각 한국표준산업분류를 찾아봤고, 이에 따라 직무가 아닌 회사의 업종으로 나눌 수 있게끔 다시 세팅을 하였다. 퇴주자 글 단순 삭제시 - 데이터 유실의 문제점 삭제는 아니고 가림 =\u003e 2번으로 선택한 이유 기록! 왜?!! 퇴주기업의 게시글이라는 멘트로 공개 - 입주자 우선의 고객경험상 게시글 리스트가 오염될 수 있고, 데이터를 보호함으로써 패스트파이브에서는 추후 전체 데이터 통계 유의미한 멤버혜택 정보를 되려 게시글 작성자에게 추천할 수도 있다고 생각했다. ","date":"2022-12-27","objectID":"/posts/202212270448/:6:0","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Projects"],"content":"개선과정 반복코드들의 middleware 처리 controller단에서 무한히 반복되는 try-catch문을 따로 뽑아내어 미들웨어 처리함으로써 코드를 더 간결히 하였다. 대부분의 API에서 권한 검증을 함에 따라, 유저 권한 API를 jsonwebtoken 다음 단계로 하여 역시 미들웨어 처리하였다. router.post( '', catchMiddleware(authMiddleware), catchMiddleware(checkPermission), catchMiddleware(replyController.createReply) ); 배포 사이트에서 보는 이가 실제로 로그인해서 살펴볼 수 있도록 로그인 페이지와 해당 페이지에 시연용 ID와 PW를 남겨놓자고 건의하였고, 프론트엔드분들의 수고로 다음과 같이 남겨졌다. ","date":"2022-12-27","objectID":"/posts/202212270448/:7:0","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Projects"],"content":"회고 저스트코드의 과정중, 내게 이번 프로젝트는 따로 진행한 미니프로젝트를 포함한다면 4번째 프로젝트였다. 이전의 세 프로젝트와 다른 점은 이전까지의 클론프로젝트가 아닌, 즉, 모체가 없는 단순 문서만으로 진행해야하는 부분이었다. 때문에 ERD에서 고민이 조금 더 들어갔고, 이전의 프로젝트와는 달리, 프로젝트 진행 중간에 ERD 수정이 빈번히 이루어졌다. 프로젝트 단위로 코딩을 했을때 가장 좋은 점은 전체 사이클을 오롯이 한바퀴 돌려볼 수 있다는 점이었다. 때문에 기본적인 기능구현만을 위한 작업은 생각보다 금방 끝났고, 미들웨어 처리라던가, service단에서의 활용성 등 이전 프로젝트에 비해 코드를 더욱 조직화, 세분화함으로써 보다 복잡한 코드들을 나름 정리할 수 있었다. 어쩌다 보니 백엔드에서 많은 파트를 담당하게 되었는데 이는 내게 더 많은 도움과 공부가 되었다. 이제 어느정도 클론사이트의 경우 혼자서도 2주정도면 할수 있을것 같은 근거없는(?) 자신감도 생겼다. 새로운 길을 걷는 자에게 이런 자신감이 없다면 시작부터 흔들리거나 불안할 수 있다는 점에서 이는 큰 장점이라 생각한다. 이번 프로젝트에서 내가 목표로하였던 백엔드 지향점은 다음과 같다. 기획의도를 파악하여 타 부서와의 불필요한 조율 및 미팅을 최소화하고, 클라이언트 즉, 프론트엔드와의 비가시적인 동기화를 최대한 고려할 수 있는 조화로운 백엔드 엔지니어 이러한 점에서 이번 프로젝트에서는 전보다 더 많은 고민과 발전을 이루었음에, 아직 채워지지 않은 나의 포트폴리오 시작점에서 또하나의 유의미한 “한 자리\"를 기록하게 되었다. 아쉬웠던 점은, 클론사이트가 아니다보니 처음부터 프론트엔드와의 합을 맞추기까지 꽤나 오랜 시간이 걸렸고, 이로인해 API 통신 테스트까지의 일정이 다소 지연되었었다. 이번 경험은 실무에서 많은 도움이 될 것이다. 끝으로, 저스트코드의 과정이 끝남으로써 정기적으로 주어지는 프로젝트가 없다는 부분에서 오는 무료함과 상실감을 느끼게 되었다. 목표가 없다면 그것은 죽은 영혼이다!라는 글을 본적이 있다. 프로젝트를 진행할때의 그 긴장감과 일정을 맞추기 위한 적절한 압박감이 존재함을 느끼게 하였고, 코딩을 하는 내내 나 스스로가 몹시 즐거워한다는 점을 느꼈다. 이제 또 다른 목표를 잡으러 가봐야겠다. 참고 [블라인드 따라하기] 8. 대댓글 구현하기 - 디비 성능 개선기 : 네이버 블로그](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true\u0026blogId=1ilsang\u0026logNo=221569040532) ↩︎ ","date":"2022-12-27","objectID":"/posts/202212270448/:8:0","tags":["project_fastfive"],"title":"패스트파이브 기업과제 회고록","uri":"/posts/202212270448/"},{"categories":["Dev"],"content":" 덧글 삭제 API 관련 코드 리팩토링 중, status code를 맞추다가 알게 된 사실! statusCode 204는 response를 전송하지 않는다!! 다만 status code “204” 만 전달할 뿐. 이걸 몰라서 코드의 오류인줄 알고 한참을 헤맸다. status code를 200으로 바꿨더니 다시 원래대로 response가 전송되었다. status code 문서의 개요만 어설프게 보고서는, 삭제니까 no content인 204가 맞겠지 하고 내 멋대로 이해한 실수였다. 204에서 다시 200으로 처리한 이유는, 삭제된 이후의 덧글 리스트와 페이지네이션 위치를 json으로 전달해야할 필요성이 있어서였다. ","date":"2022-12-25","objectID":"/posts/202212250509/:0:0","tags":["RESTful_API","Back-End"],"title":"status code 204","uri":"/posts/202212250509/"},{"categories":["Dev"],"content":"204 No Content 1 HTTP 204 No Content 성공 상태 응답 코드는 요청이 성공했으나 클라이언트가 현재 페이지에서 벗어나지 않아도 된다는 것을 나타냅니다. 기본값에서 204 응답은 캐시에 저장할 수 있습니다. 캐시에서 가져온 응답인 경우 ETag 헤더를 포함합니다. 흔히 204를 반환하는 경우는 PUT 요청에 대한 응답으로, 사용자에게 보여지는 페이지를 바꾸지 않고 리소스를 업데이트할 때 쓰입니다. 리소스를 생성한 경우엔 201 Created를 대신 반환합니다. 새롭게 업데이트한 페이지를 보여줘야 할 경우 200을 사용해야 합니다. 204 No Content - HTTP | MDN ↩︎ ","date":"2022-12-25","objectID":"/posts/202212250509/:0:1","tags":["RESTful_API","Back-End"],"title":"status code 204","uri":"/posts/202212250509/"},{"categories":["Dev"],"content":" ","date":"2022-12-13","objectID":"/posts/202212130459/:0:0","tags":["TypeScript","typeORM","Debugging"],"title":"typescript - typeorm 연결 오류 해결","uri":"/posts/202212130459/"},{"categories":["Dev"],"content":"typescript에서의 typeorm 연결 오류 대략 14시부터 22시까지 무려 8시간을 끼니도 거른채 뚫어져라 모니터만 쳐다보며 찾았다. type 지정이 잘못되었나? 아니면 새롭게 설치해줘야하는 npm이 있는건가? 눈이 빠져라 찾았는데 결과는 Success server.ts에서 dotenv.config() 바로 이 코드의 순서 문제였다. 기존 import createApp from './app.ts' 보다 아래에 있었는데 위로 올렸더니 바로 서버가 열렸다. dotenv.config() import createApp from './app.ts' // 윗줄과 순서 바꿈 동기/비동기 세션을 진행한게 오늘인데 바로 이러한 문제를 찾지 못하고 엉뚱한 곳에서 하루종일 헤매고 있었다니… 참고사이트 오류 해결1 Layered Pattern: test 목적으로 만들던 서버에서 발생한 에러 - 에러 종류 : throw new MissingDriverError_1.MissingDriverError - 에러의 원인 : 정확히 알수는 없으나 대개 app.js 에서 database 까지 연결이 되지 않을 때 나타난다. 처음에는 driver 에러라고 생각했으나 아니었다. 스펠링을 틀려서 연결이 안되거나 router 를 빠뜨려서 연결이 안되는 경우도 드라이버 문제로 나타났다. 정확히 하자면 typeorm 문제로 등장했다. - 디버깅 : 콘솔로 일일이 찍으면서 에러의 위치를 찾아야 한다. ⇒해결함 : 원인은 dotenv.config() 가 const routes = require(”….”) 보다 아래에 있어서 생겼다. 위치를 바꿔주니 서버가 잘 열렸다. ","date":"2022-12-13","objectID":"/posts/202212130459/:0:1","tags":["TypeScript","typeORM","Debugging"],"title":"typescript - typeorm 연결 오류 해결","uri":"/posts/202212130459/"},{"categories":["Dev"],"content":" mySQL을 사용하여 Database를 쌓아 나갈때, DBmate 를 사용하면 초기 세팅값을 마치 git을 저장하듯 이력을 남겨 언제든 migration 할 수 있다. 지금부터의 설명은 macOS 기준으로 기술한다. (windows가 없음 ;; ) ","date":"2022-10-18","objectID":"/posts/202210180502/:0:0","tags":["DBmate","Back-End"],"title":"DBmate 사용법","uri":"/posts/202210180502/"},{"categories":["Dev"],"content":"DBmate 설치 Homebrew 를 사용하여 설치한다. 1 터미널 아무 위치에서나 무관하다. 2 $brew install dbmate 이렇게 입력 후, 엔터키를 누르면 자동으로 설치가 된다. 그리고, 하나의 파일이 필요하다. 바로 .env 파일. IDE 또는 터미널 vim으로 .env 파일을 생성한 후, 다음과 같이 입력한다. \" cat.env \" 이런식으로 해도 되고, 바로 확장자부터 시작하는 파일명도 된다. 작성해야할 코드는 단 한줄! 아래와 같다. DATABASE URL=\"mysql://username: password@127.0.0.1:3306/database이름\" 주의사항 username : mySQL 서버 실행시 설정했던 이름을 넣으면 되는데 보통 root이다. database 위치 반드시 확인!! (localhost라면 127.0.0.1:3306 이 default 값이다.)3 기존의 database가 있다면 database 이름을 똑같이 적어야 한다. (아니면 또다른 이름의 DB생성됨) 주소 잘못 입력하면 아래 이미지와 같은 error 메세지를 무한히 받아볼 수 있다. ;; ","date":"2022-10-18","objectID":"/posts/202210180502/:0:1","tags":["DBmate","Back-End"],"title":"DBmate 사용법","uri":"/posts/202210180502/"},{"categories":["Dev"],"content":"DBmate 세팅 dbmate new create_users2_table 위와 같이 이라고 터미널에서 입력하면 migrations 폴더가 자동 생성되면서 users2라는 테이블이 생성할 수 있는sql 파일이 생성된다. 그리고 IDE나 vim에서 해당 sql 파일에 테이블 생성 규칙을 작성해주면 된다. ","date":"2022-10-18","objectID":"/posts/202210180502/:0:2","tags":["DBmate","Back-End"],"title":"DBmate 사용법","uri":"/posts/202210180502/"},{"categories":["Dev"],"content":"DBmate sql 문서 작성 위와 같이 제대로 파일이 생성되면 파일 편집시 이런 양식이 들어가 있음을 볼 수 있다. -- migrate:up -- migrate:down -- migrate:up =\u003e table 생성시 넣고자 하는 코드 -- migrate:down =\u003e 보통 테이블 제거시 실행할 명령문 -- migrate:up create table users ( id integer, name varchar(255), email varchar(255) not null ); -- migrate:down drop table users2; 위와 같은 형식으로 작성된다. 제대로 했다면, 이게 끝이다. ","date":"2022-10-18","objectID":"/posts/202210180502/:0:3","tags":["DBmate","Back-End"],"title":"DBmate 사용법","uri":"/posts/202210180502/"},{"categories":["Dev"],"content":"migration 실행 $dbmate up 문서를 모두 작성 한 후, 터미널에서 명령어를 실행하면 이런 아름답고 꿈에 그리던 메세지를 받아볼 수 있다. 그리고 mySQL 서버로 가서 show tables; 를 입력해보자. 무사히 테이블이 생성되어 있음을 확인할 수 있다. dbmate down 위 명령어도 실행해보며 DBmate가 어떻게 돌아가는지 알면 좋다. 추가 보완해야 할 사항 위 문서에서는 하나의 테이블을 가지고 예를 들었는데, 명령어는 단순히 dbmate up이 끝이다. 만약 여러 테이블을 생성한다면?? 어떤 테이블을 up 하는 건지 table에 대한 지정이 없다. 즉, 아마도 테이블의 생성 순서에 따라 순차적으로 실행되지 싶은데, 이부분은 mock table을 여러개 만들어 직접 실험을 해봐야겠다. 참고문서 amacneil/dbmate: A lightweight, framework-agnostic database migration tool. dbmate 설치 / 사용 하기 Database migrations with Dbmate - M1lt0n on software java MySQL URL, 호스트, 포트 및 사용자 이름을 어떻게 알 수 있습니까? - 리뷰나라 참고로 맥에서 개발 관련 어플리케이션이나 cask 소프트웨어는 brew를 통해 설치 및 관리하는 것이 편하다… 라고 해서 그렇게 하고 있다. ↩︎ $ 몰랐는데 이게 그런 뜻인듯. ↩︎ 참고로 저 주소를 localhost:3306이라고 했다가 DBmate가 제대로 먹히질 않아서 1시간이나 헤맸다. 정확하게 써야 함! ↩︎ ","date":"2022-10-18","objectID":"/posts/202210180502/:0:4","tags":["DBmate","Back-End"],"title":"DBmate 사용법","uri":"/posts/202210180502/"},{"categories":["Env"],"content":" 터미널을 처음 실행하면 꼭 사용자 폴더에서 시작한다. cd ...로 찾아가는게 너무 귀찮다. 그렇다고 finder에서 폴더를 drag \u0026 drop해서 직접경로를 떨어뜨리는 방법도 있지만, 굳이 키보드에서 또 마우스나 트랙패드로 손을 움직이고 싶지 않다. 때문에 알게 된 것이 바로 alias를 활용한 터미널 단축키 방법! 우선 zsh라는 쉘 을 한번 가볍게 알아본다. 힘들면 Pass하고, 바로 적용할 수 있는 방법으로 가자! ","date":"2022-10-13","objectID":"/posts/202210130407/:0:0","tags":["terminal"],"title":"터미널에서 '바로가기' 단축키 등록하기","uri":"/posts/202210130407/"},{"categories":["Env"],"content":"1. zshrc 편집 터미널에서 다음을 실행 vim ~/.zshrc 이미 파일이 존재할수도 있고, 없으면 파일이 생성되며 바로 편집모드로 들어가게 된다. 파일 위치는 ~/ ","date":"2022-10-13","objectID":"/posts/202210130407/:0:1","tags":["terminal"],"title":"터미널에서 '바로가기' 단축키 등록하기","uri":"/posts/202210130407/"},{"categories":["Env"],"content":"2. 단축키 입력 vim 모드에서 작성하기에 가장 기본적인 vim 사용법 2개만 알면 된다.1 i 또는 a키를 눌러 편집모드 진입 편집 편집이 끝나면 esc키를 눌러 명령어 모드 진입후, :wq 누르고 엔터! 이것이 기본적인 방법이고, 편집시 단축키 등록은 아래와 같이 입력하면 된다. alias 단축키=\"실행명령어\" 예를 들면, alias gocode=\"cd Users/codject/Documents/Code\" 예와 같이 등록했을 때, 터미널에서 gocode만 치면 바로 저 위치로 이동한다. 입력이 끝나면 :wq로 무사히 빠져나와 주면 된다. ","date":"2022-10-13","objectID":"/posts/202210130407/:0:2","tags":["terminal"],"title":"터미널에서 '바로가기' 단축키 등록하기","uri":"/posts/202210130407/"},{"categories":["Env"],"content":"3. 단축키 등록 입력만 한다고 끝이 아니다. 입력한 zshrc를 터미널에 등록해줘야 한다. source ~/.zshrc 터미널에서 위와 같이 입력후 엔터를 치면, 단축키가 등록되었다며 리스트로 바로 뜬다. 이후 잘 쓰면 끝. 만약 단축키가 바로 동작하지 않는다면, 터미널을 완전종료 후, 재실행해주면 된다. 전체적인 참고 사이트 자주쓰는 명령어 .zshrc에 단축명령어 등록하기 더 자세히 알고 싶다면 Vim 단축키 정리 :: Outsider’s Dev Story 참고 ↩︎ ","date":"2022-10-13","objectID":"/posts/202210130407/:0:3","tags":["terminal"],"title":"터미널에서 '바로가기' 단축키 등록하기","uri":"/posts/202210130407/"}]